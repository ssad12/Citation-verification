[
  {
    "citation_marker": "[10]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: YES. The statement lists the authors \"Sanidhya Kashyap, Changwoo Min, and Taesoo Kim,\" which exactly matches the authors listed in the cited document's header (\"Sanidhya Kashyap; Changwoo Min; Taesoo Kim\"). No misrepresentation, distortion, or fabrication is present."
    ],
    "evidence": [
      {
        "text_fragment": "Scalable NUMA-aware Blocking Synchronization Primitives\nSanidhya Kashyap\nChangwoo Min\nTaesoo Kim\nGeorgia Institute of Technology\nAbstract\nApplication scalability is a critical aspect to efficiently\nuse NUMA machines with many cores. To achieve that,\nvarious techniques ranging from task placement to data\nsharding are used in practice. However, from an operat-\ning system‚Äôs perspective, these techniques often do not\nwork as expected because various subsystems in the OS\ninteract and share data structures among themselves, re-\nsulting in scalability bottlenecks. Although current OSes\nattempt to tackle this problem by introducing a wide range\nof synchronization primitives such as spinlock and mu-\ntex, the widely-used synchronization mechanisms are not\ndesigned to handle both under- and over-subscribed sce-\nnarios in a scalable manner. In particular, the current\nblocking synchronization primitives that are designed to\naddress both scenarios are NUMA oblivious, meaning\nthat they suffer from cache line contention in an under-\nsubscribed situation, and even worse, inherently spur long\nscheduler intervention, which leads to sub-optimal perfor-\nmance in an over-subscribed situation.\nIn this work, we present several design choices to im-\nplement scalable blocking synchronization primitives that\ncan address both under- and over-subscribed scenarios.\nSuch design decisions include memory-efficient NUMA-\naware locks (favorable for deployment) and scheduling-\naware, scalable parking and wake-up strategies. To vali-\ndate our design choices, we implement two new blocking\nsynchronization primitives, which are variants of mutex\nand reader-writer semaphore in the Linux kernel. Our\nevaluation results show that the new locks can improve\nthe application performance by 1.2‚Äì1.6√ó, and some of\nthe file system operations by as much as 4.7√ó, in both\nunder- and over-subscribed scenarios. These new locks\nuse 1.5‚Äì10√ó less memory than state-of-the-art NUMA-\naware locks on 120-core machine.\n1\nIntroduction\nOver the last decade, microprocessor vendors have been\npursuing the direction of bigger multi-core and multi-\nsocket machines [19, 33]. For example, a single sys-\ntem can have up to 4096 hardware threads that are or-\nganized into sockets, known as NUMA (Non-Uniform\nMemory Access) domains [33]. They address a key prob-\nlem of removing the memory access latency bottleneck\nby directly attaching multiple CPUs to a large chunk of\nmemory (DRAM). Furthermore, these machines have be-\ncome a norm to further scale applications such as large\nin-memory databases (Microsoft SQL server [28]) and\nprocessing engines [34, 41].\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\n120.0\n140.0\n0\n20\n40\n60\n80 100 120\nM ops/sec\n#thread\n(a) Directory read\n0.0\n5.0\n10.0\n15.0\n20.0\n25.0\n0\n20\n40\n60\n80 100 120\nGB\n#thread\n(b) Memory usage\nVanilla\nCohort\nCST\nFigure 1: Impact of NUMA-aware locks on a file-system micro-\nbenchmark that spawns threads to enumerate files in a shared\ndirectory [29], which stresses the reader side of the reader-writer\nsemaphore (rwsem). Figure (a) shows the impact of locks on\nthe throughput till 120 threads on a 120 core machine, where\nVanilla is Linux‚Äôs native version, Cohort is an in-kernel ported\nversion of NUMA-aware lock [17], and our NUMA-aware lock\nimplementation (CST). Figure (b) shows memory usage that\nuse these locks before and after the experiments.\nA NUMA machine consists of multiple sockets, where\neach node has a locally attached memory, a last-level\ncache and multiple CPUs. It only exposes a flat cache-\ncoherent architecture to software by hiding the underlying\nhardware topology from the applications. Unfortunately,\nthis flat architecture hinders the scalability of applications\nas the applications may either suffer remote memory ac-\ncess or from the contended memory access from multiple\nCPUs, thereby degrading their performance [2, 4].\nTo achieve scalability in NUMA machines, various\napplications such as databases, and OS rely on NUMA\npartitioning to mitigate the cost of remote memory access\neither by data placement or via task placement to achieve\nhigh performance. However, this approach does not solve\nthe problem of how to efficiently modify shared data struc-\ntures such as inodes, dentry cache or even the structures of\nmemory allocator that are shared among multiple threads.\nAs a result, synchronization primitives become the basic\nbuilding blocks of such multi-threaded applications, and\nare critical in determining their scalability [4]. Hence, the\nstate-of-the-art NUMA-aware locks [6, 7, 12, 13, 17, 25]\nare the apt choice to efficiently exploit the NUMA be-\nhavior, and also achieve scalability on these architectures.\nUnfortunately, they are difficult to adopt in practice due\nto their inherent memory overhead.\nFor non-blocking locks, Wickizier et al. [4] showed that\na Ticket lock suffers from cache-line contention with in-\ncreasing core count. They replace it with the MCS lock to\nmitigate such effect, improving the system performance.\nHowever, its adoption faced several challenges due to\nthe memory constraint of the spinlock [23]. Similarly,\nfor blocking synchronization primitives, there are various\n1",
        "semantic_similarity": 0.08546900749206543,
        "keyword_overlap": 0.005573248407643312,
        "combined_score": 0.06150027976673879,
        "paragraph_id": 0,
        "source_title": "Scalable NUMA-aware Blocking Synchronization Primitives"
      }
    ],
    "resolved_title": "Scalable NUMA-aware Blocking Synchronization Primitives"
  },
  {
    "citation_marker": "[11]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.67, overlap: 0.02): NO. The original article cites [11] (the Nest paper) to support the definition of \"work conservation\" as \"resources should be occupied unless no waiter exists.\" However, the provided excerpt from the Nest paper does not mention \"work conservation\" or discuss this concept. The Nest paper focuses on core scheduling to keep tasks on warm cores, not work conservation. Thus, the statement misrepresents the cited content by attributing an unmentioned concept to it.",
      "DeepSeek LLM confirms support for this match: YES. The original statement defines work conservation as \"resources should be occupied unless no waiter exists,\" which aligns with the cited paragraph's definition: \"if a task is placed on a core that is not idle, then no idle core should be available.\" Both convey the principle of avoiding idle resources when tasks are waiting."
    ],
    "evidence": [
      {
        "text_fragment": "OS Scheduling with Nest: Keeping Tasks Close Together on Warm Cores\nEuroSys ‚Äô22, April 5‚Äì8, 2022, RENNES, France\n2.2\nThe ùëÜmove scheduler, targeting core frequency\nGouicem et al. [7] identified the problem of frequency inver-\nsion, in the common case where a task ùëáparent forks or wakes\nanother task ùëáchild and then immediately sleeps to wait for\nùëáchild‚Äôs results. ùëáparent‚Äôs core is likely running at a high fre-\nquency, while CFS will place ùëáchild on an idle core if one is\navailable.ùëáparent will thus be delayed untilùëáchild completes on\nan initially low-frequency core, while ùëáparent‚Äôs former high\nfrequency core is available. Gouicem et al. showed that this\nproblem causes a slowdown on a variety of applications.\nGouicem et al. proposed the scheduler ùëÜmove that places\nùëáchild on the core ofùëáparent, allowingùëáchild to benefit from that\ncore‚Äôs high frequency. If ùëáparent does not immediately block\nor if other tasks are waiting, such a strategy may cause ùëáchild\nto incur high latency. Thus, ùëÜmove only makes this placement\nwhen the core chosen by CFS has a low frequency and sets\na timer for ùëáchild, to move ùëáchild to the core chosen by CFS\nif ùëáchild is not scheduled on ùëáparent‚Äôs core within a brief de-\nlay. When the timer expires, however, ùëÜmove does nothing to\nensure that ùëáchild ends up on a core with a high frequency.\n2.3\nLinux‚Äôs power governors\nThe scheduler has no control over core frequencies. Instead,\ncore frequency results from an interplay between the Linux\npower governor and the hardware. The governor sets the\nbounds in which the frequency should vary and can make\nsuggestions about what frequency should be used. The hard-\nware combines the information from the governor with its\nobservations about the current activity on the core and the\ncore‚Äôs socket, and chooses a frequency for the core accord-\ningly. The power governor has a significant impact on per-\nformance for many applications. Thus, when we refer to a\nscheduler, we refer to the used governor as well. We consider\nthe performance and schedutil governors, which are available\non most Linux systems and represent distinct strategies.\nPerformance requests that the hardware use the nomi-\nnal frequency of the machine. The hardware can still freely\nchoose between the nominal frequency and the turbo fre-\nquencies. Performance gives tasks high performance, but\nmisses the potential energy savings that can be obtained by\nrunning non-demanding tasks at lower frequencies.\nSchedutil takes into account information from the sched-\nuler about recent task activity, to attempt to reconcile perfor-\nmance and energy usage. It allows the machine to use its full\nrange of frequencies. When schedutil observes that the tasks\non a core have a high recent CPU utilization, it suggests to\nthe hardware to increase the frequency.\n3\nThe Nest Approach\nThe key idea behind Nest is the use of a nest, defining a\nlimited set of recently used cores to consider in high priority\nwhen placing a task. By limiting activity to a small number of\ncores, Nest encourages the hardware to choose a high core\nprimary\nnest\nSearch\nstrategy\nNest management\non core selection\nnothing idle\n)\ntimeout\n5\nreserve\nnest\nselection\nj\nnothing idle\n&\nCFS\nnormal\nselection\ni\nimpatient selection\nO\nFigure 1. Core-search path through the nests (top) and core\nmovement between nests (bottom).\nfrequency. The challenge in creating a scheduler around this\nidea is to design heuristics that properly dimension the nest\naccording to applications‚Äô current needs. A nest that is too\nlarge will result in task dispersal, replicating the dispersal\nproblem of CFS. A nest that is too small will result in tasks\ncompeting for the same cores, inducing overloads.\n3.1\nBuilding the nest\nAs shown in Figure 1, Nest keeps track of two sets of cores\n(nests), to consider in high priority for task placement. Cores\nin the primary nest are currently in use or have been used\nrecently and are expected to be useful in the near future.\nCores in the reserve nest were previously in the primary nest\nbut have not been used recently and thus are considered\nto be less likely to be used in the near future, or they have\nrecently been selected by CFS and have not yet proved their\nnecessity for the current set of tasks.\nThe top of Figure 1 (red arrows) describes the core-search\nheuristic. For a forking or waking task, Nest first searches\nfor an idle core in the primary nest, then if none is found\nit searches for an idle core in the reserve nest. If that also\nfails, then it falls back on CFS. The search in the primary\nnest starts at the task‚Äôs previous core (or the parent‚Äôs core,\nfor a fork), to reduce the risk of collision with concurrent\nforks and wakeups on other cores. The search in the reserve\nnest, which is expected to be accessed less often, starts from\na fixed core, chosen arbitrarily as the core on which the\nsystem call that started Nest was executed, to reduce task\ndispersal. In both cases, the search first considers cores on\nthe same die as the task‚Äôs previous core (or the parent‚Äôs core,\nfor a fork), before considering cores on the other dies. This\nheuristic reduces the number of used dies, thus increasing\nthe chance of leaving some dies completely idle and saving\nenergy. Unlike CFS, Nest selects any core that is found to be\ncurrently idle, independent of recent load, in order to favor\ncore reuse. Also unlike CFS, Nest does not take into account\nactivity on hyperthreads. Nevertheless, all cores that newly\nenter the nests are initially chosen by CFS when there are no\nidle cores in the nests, and thus they inherit CFS‚Äôs strategy\nof selecting cores where the hyperthread is idle.\nThe bottom of Figure 1 (blue arrows) indicates how cores\nmove between the nests, to allow the nest size to adapt to\n3",
        "semantic_similarity": 0.6711508333683014,
        "keyword_overlap": 0.02436795613767895,
        "combined_score": 0.4771159701991146,
        "paragraph_id": 2,
        "source_title": "OS Scheduling with Nest: Keeping Tasks Close Together on Warm Cores"
      },
      {
        "text_fragment": "OS Scheduling with Nest: Keeping Tasks Close\nTogether on Warm Cores\nJulia Lawall\nInria\nParis, France\nHimadri Chhaya-Shailesh\nInria\nParis, France\nJean-Pierre Lozi\nOracle Labs\nZurich, Switzerland\nBaptiste Lepers\nUniversity of Sydney\nSydney, Australia\nWilly Zwaenepoel\nUniversity of Sydney\nSydney, Australia\nGilles Muller\nInria\nParis, France\nAbstract\nTo best support highly parallel applications, Linux‚Äôs CFS\nscheduler tends to spread tasks across the machine on task\ncreation and wakeup. It has been observed, however, that in\na server environment, such a strategy leads to tasks being\nunnecessarily placed on long-idle cores that are running at\nlower frequencies, reducing performance, and to tasks being\nunnecessarily distributed across sockets, consuming more\nenergy. In this paper, we propose to exploit the principle of\ncore reuse, by constructing a nest of cores to be used in pri-\nority for task scheduling, thus obtaining higher frequencies\nand using fewer sockets. We implement the Nest scheduler\nin the Linux kernel. While performance and energy usage\nare comparable to CFS for highly parallel applications, for a\nrange of applications using fewer tasks than cores, Nest im-\nproves performance 10%‚Äì2√ó and can reduce energy usage.\nCCS Concepts: ‚Ä¢ Computer systems organization;\nKeywords: Scheduling, Linux kernel\nACM Reference Format:\nJulia Lawall, Himadri Chhaya-Shailesh, Jean-Pierre Lozi, Baptiste\nLepers, Willy Zwaenepoel, and Gilles Muller. 2022. OS Schedul-\ning with Nest: Keeping Tasks Close Together on Warm Cores. In\nSeventeenth European Conference on Computer Systems (EuroSys\n‚Äô22), April 5‚Äì8, 2022, RENNES, France. ACM, New York, NY, USA,\n16 pages. https://doi.org/10.1145/3492321.3519585\n1\nIntroduction\nThe primary goal of an operating system (OS) task scheduler\nis to allocate tasks to cores in a way that maximizes applica-\ntion performance. A well-known desirable property is work\nconservation, i.e., if a task is placed on a core that is not idle,\nthen no idle core should be available [10, 11]. However, in\nchoosing a core for a task, it is also important to consider\nwhether the chosen core will allow the task to access needed\nEuroSys ‚Äô22, April 5‚Äì8, 2022, RENNES, France\n¬© 2022 Association for Computing Machinery.\nThis is the author‚Äôs version of the work. It is posted here for your personal\nuse. Not for redistribution. The definitive Version of Record was published\nin Seventeenth European Conference on Computer Systems (EuroSys ‚Äô22), April\n5‚Äì8, 2022, RENNES, France, https://doi.org/10.1145/3492321.3519585.\n(hardware) resources efficiently. The performance that a task\ncan achieve is determined in part by the frequency of the cho-\nsen core [7]. On modern CPUs, core frequencies may vary\nsignificantly, as individual cores can adjust their frequency\nindependently. Nevertheless, the Linux kernel‚Äôs default CFS\nscheduler does not take core frequency into account. Placing\ntasks on cores in a way that causes higher frequencies to be\nused can improve performance.\nWe consider scheduling on large multicore servers. Such\nservers are today becoming more accessible and affordable.\nThey can be used for traditional high-performance comput-\ning, where applications are often designed to decompose to\nthe number of cores available, so that tasks can be pinned to\ncores, making scheduling irrelevant. But multicore servers\ncan also be used as computing resources for applications\nthat are demanding in terms of compute cycles, memory,\nor disk requirements. Such applications rely on the OS task\nscheduler for task placement. The number of cores required\nmay vary from few to many, and back, across the course of\nthe application. To get the best performance, the OS task\nscheduler must optimally adapt to all of these situations.\nModern servers offer ‚Äúturbo‚Äù frequencies [1, 8] allowing\ncores to run at a frequency higher than the nominal fre-\nquency. Various turbo frequencies are available, depending\non the number of active cores on the socket, to respect ther-\nmal constraints. The frequency is determined jointly by the\nsoftware and the hardware. The software, typically an OS\nkernel-level power governor, suggests boundaries, and then\nthe hardware chooses a frequency for a core within these\nboundaries according to the number of cores on the same\nsocket and their current degree of activity. To obtain the\nhighest possible frequencies, it is necessary to minimize the\nnumber of cores used (‚Äúkeeping tasks close together‚Äù) and\nensure a sustained activity (‚Äúkeeping cores warm‚Äù).\nIn this paper, we propose the task scheduler Nest, de-\nsigned according to the principles reuse cores and keep cores\nwarm. To increase core reuse, Nest tries to place tasks within\na set of recently used cores (the nest). To keep cores warm,\nwith Nest, the idle process spins on a newly idle core for\na short period, to encourage the hardware to keep the fre-\nquency high. Finally, when a task must be placed outside the\n1",
        "semantic_similarity": 0.6453209519386292,
        "keyword_overlap": 0.053804765564950036,
        "combined_score": 0.4678660960265254,
        "paragraph_id": 0,
        "source_title": "OS Scheduling with Nest: Keeping Tasks Close Together on Warm Cores"
      }
    ],
    "resolved_title": "OS Scheduling with Nest: Keeping Tasks Close Together on Warm Cores"
  },
  {
    "citation_marker": "[12]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.11, overlap: 0.04): NO. The cited paragraph from [12] (SMURF-THP) focuses on a Transformer Hawkes Process model for event prediction, discussing metrics like accuracy and uncertainty quantification. It contains no mention of locks, NUMA architecture, lock handover, or scheduler-cooperative mutexes (SCL), which the original article claims [12] addresses. This misrepresents the content of [12].",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.10, overlap: 0.03): NO. The original article cites [12] regarding lock mechanisms (SCL) and NUMA architecture, but the provided content from [12] (SMURF-THP) focuses on performance metrics of a Transformer Hawkes Process model, with no mention of locks, NUMA, or SCL. The cited paragraph does not support the lock-related claims, indicating misrepresentation.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.09, overlap: 0.03): NO. The cited paragraph from [12] focuses on SMURF-THP, a score matching method for Transformer Hawkes Process, discussing event sequences, intensity functions, and score matching objectives. It contains no mention of SCL (scheduler-cooperative mutex), lock usage monitoring, CPU time fairness, NUMA sockets, or lock handover to minimize data movement. The article misrepresents [12] by citing it for unrelated lock/NUMA content.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process\nTable 5. Event type prediction accuracy of SMURF-THP given\nground truth event time as the inputs.\nSMURF-THP\nDataset\nAcc(%)\nStackOverflow\n48.38\nRetweet\n61.65\nMIMIC-II\n83.84\nFinancial\n61.62\nWe also train SMURF-THP with different volumes of train-\ning data to study its generalization ability. We train the\nmodel on different ratios of the dataset and present the per-\nformance in Figure 6 and Figure 7. As shown, all metrics\ngo better as we feed more training data. Compared to the\nRetweet dataset, SMURF-THP is more sensitive to training\nratio on the StackOverflow dataset. This is due to that the\nStackOverflow dataset contains less events than the Retweet,\nthereby SMURF-THP requires larger proportion to learn the\ndistribution.\nSMURF-THP(CS)\nSMURF-THP(CRPS)\nCalibration Score (%)\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\nCRPS\n0.440\n0.445\n0.450\n0.455\nHyperparameter Œ±\n1\n10\n(a) Calibration Score and CRPS\nSMURF-THP(Acc)\nAccuracy (%)\n46.15\n46.20\n46.25\n46.30\nHyperparameter Œ±\n1\n10\n(b) Accuracy\nFigure 5. Sensitivity of the hyperparameter Œ± in the training objec-\ntive to prediction performance.\nSMURF-THP(CS)\nSMURF-THP(CRPS)\nCalibration Score (%)\n1\n2\n3\nCRPS\n0.44\n0.45\n0.46\n0.47\nTraining data ratio\n0\n0.5\n1.0\n(a) StackOverflow\nCalibration Score (%)\n0.7\n0.8\n0.9\nCRPS\n0.85\n0.90\n0.95\n1.00\n1.05\nTraining data ratio\n0\n0.5\n1.0\n(b) Retweet\nFigure 6. Calibration Score and CRPS of SMURF-THP trained\nwith different ratios of the StackOverflow and Retweet datasets.\n5. Discussion and Conclusion\nWe acknowledge the existence of several studies that\nadopt non-likelihood-based estimators to circumvent the\nintractable integral within the log-likelihood computation.\nWe present our discussions on these works below. Xiao et al.\nAcc(SO)\nAcc(RT)\nAccuracy-SO (%)\n45.0\n45.5\n46.0\nAccuracy-RT (%)\n59.6\n59.8\n60.0\n60.2\n60.4\nTraining data ratio\n0\n0.5\n1.0\nFigure 7. Prediction Accuracy of SMURF-THP trained with differ-\nent ratio of the StackOverflow and Retweet datasets.\n(2017) trains the model to directly predict the next event‚Äôs\ntime and type through a summation of Mean Squared Error\n(MSE) and cross-entropy loss. However, their model does\nnot construct an explicit intensity function and hence doesn‚Äôt\nsupport flexible sampling and uncertainty quantification. TP-\nPRL (Li et al., 2018) employs reinforcement learning (RL)\nfor learning an events generation policy, but they concentrate\non the temporal point process (TPP) rather than the marked\ntemporal point process (MTPP), which is the focus of our\nwork. Furthermore, they presume the intensity function to\nbe constant between timestamps, a limitation that hampers\nthe accurate capture of the point process‚Äôs temporal dynam-\nics. Upadhyay et al. (2018) applies RL for MTPP which\ntrains a policy to maximize feedback from the environment.\nSimilar to TPPRL, it assumes a very stringent intensity func-\ntion, e.g., in exponential forms, which is oversimplified to\ncapture the complex point process in real-world applications.\nINITIATOR (Guo et al., 2018) and NCE-TPP (Mei et al.,\n2020) are both based on noise-contrastive estimations for\nMTPP. However, they utilize the likelihood objective for\ntraining the noise generation network, which consequently\nreintroduces the intractable integral. In our experiments,\nwe include NCE-TPP‚Äôs performance, as its authors have\ndemonstrated it outperforms INITIATOR.\nSeveral other works (Wang et al., 2020; Fox et al., 2016)\nexplore different scopes of uncertainty quantification for the\nHawkes process. That is, they provide uncertainty quantifi-\ncation for the parameters in conventional Hawkes process\nmodels, whereas we focus on uncertainty quantification for\nthe predicted arrival time.\nIn this work, we present SMURF-THP, a score-based\nmethod for training Transformer Hawkes process models\nand quantifying prediction uncertainty. Our proposed model\nadopts score matching as the training objective to avoid\nintractable computations in conventional Hawkes process\nmodels. Moreover, with the learnt score function, we can\nsample arrival time of events using the Langevin Dynam-\nics. This enables uncertainty quantification by calculating\nthe associated confidence interval. Experiments on various\nreal-world datasets demonstrate that SMURF-THP achieves\nstate-of-the-art performance in terms of Calibration Score,\nCRPS and Interval Length.\n9",
        "semantic_similarity": 0.10659301280975342,
        "keyword_overlap": 0.04172560113154172,
        "combined_score": 0.0871327893062899,
        "paragraph_id": 8,
        "source_title": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process"
      },
      {
        "text_fragment": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process\nTable 2. Comparison of different methods‚Äô performance on four real-world datasets in terms of Calibration Score (CS), CRPS, Coverage\nError (CER) and Interval Length (IL). CER and IL are both calculated at confidence level 0.5.\nStackOverflow\nRetweet\nMethods\nCS(%)(‚Üì)\nCRPS(‚Üì)\nCER(%)(‚Üì)\nIL(‚Üì)\nAcc(%)(‚Üë)\nCS(%)(‚Üì)\nCRPS(‚Üì)\nCER(%)(‚Üì)\nIL(‚Üì)\nAcc(%)(‚Üë)\nNHP\n6.64¬±0.55\n0.69¬±0.01\n5.78¬±0.26\n0.67¬±0.04\n46.20¬±0.13\n14.35¬±0.22\n1.65¬±0.05\n16.98¬±0.24\n0.077¬±0.005\n60.28¬±0.08\nNCE-TPP\n5.64¬±0.46\n0.62¬±0.01\n5.99¬±0.39\n0.63¬±0.04\n46.20¬±0.15\n13.55¬±0.10\n1.59¬±0.05\n10.30¬±0.16\n0.055¬±0.002\n60.30¬±0.06\nSAHP\n4.48¬±0.24\n0.48¬±0.02\n7.73¬±0.47\n0.55¬±0.01\n46.22¬±0.06\n10.06¬±0.35\n1.12¬±0.02\n15.65¬±0.03\n0.061¬±0.001\n60.32¬±0.08\nTHP\n4.13¬±0.25\n0.46¬±0.01\n5.90¬±0.33\n0.56¬±0.02\n46.48¬±0.05\n4.12¬±0.13\n1.08¬±0.05\n3.00¬±0.02\n0.059¬±0.002\n60.63¬±0.13\nSMURF-THP\n0.65¬±0.12\n0.44¬±0.01\n0.48¬±0.09\n0.52¬±0.01\n46.26¬±0.08\n0.71¬±0.16\n0.86¬±0.04\n0.76¬±0.03\n0.031¬±0.001\n60.34¬±0.12\nMIMIC-II\nFinancial\nMethods\nCS(%)(‚Üì)\nCRPS(‚Üì)\nCER(%)(‚Üì)\nIL(‚Üì)\nAcc(%)(‚Üë)\nCS(%)(‚Üì)\nCRPS(‚Üì)\nCER(%)(‚Üì)\nIL(‚Üì)\nAcc(%)(‚Üë)\nNHP\n9.87¬±0.35\n0.74¬±0.03\n8.85¬±0.29\n0.69¬±0.04\n83.25¬±0.27\n4.64¬±0.42\n1.03¬±0.01\n3.85¬±0.30\n0.053¬±0.002\n60.23¬±0.05\nNCE-TPP\n6.52¬±0.24\n0.65¬±0.01\n9.23¬±0.38\n0.77¬±0.04\n83.60¬±0.32\n3.55¬±0.12\n1.35¬±0.05\n4.09¬±0.04\n0.055¬±0.04\n60.28¬±0.09\nSAHP\n7.49¬±0.32\n0.61¬±0.01\n15.70¬±0.52\n0.35¬±0.01\n83.76¬±0.13\n3.50¬±0.33\n1.09¬±0.05\n3.19¬±0.06\n0.048¬±0.003\n60.37¬±0.08\nTHP\n3.89¬±0.14\n0.75¬±0.05\n5.81¬±0.12\n0.84¬±0.03\n84.78¬±0.13\n3.53¬±0.19\n1.57¬±0.02\n3.19¬±0.13\n0.042¬±0.006\n60.51¬±0.06\nSMURF-THP\n2.87¬±0.11\n0.55¬±0.04\n2.33¬±0.09\n0.43¬±0.03\n84.02¬±0.31\n2.49¬±0.13\n0.84¬±0.01\n2.61¬±0.06\n0.042¬±0.003\n61.02¬±0.09\nnecessary for modeling scores on low-density regions.\nSAHP\nTHP\nSMURF-THP\nCoverage\n0\n0.5\n1.0\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(a) StackOverflow\nCoverage\n0\n0.5\n1.0\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(b) Retweet\nSAHP\nTHP\nSMURF-THP\nCoverage\n0\n0.5\n1.0\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(c) MIMIC-II\nCoverage\n0\n0.5\n1.0\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(d) Financial\nFigure 1. Coverage of different confidence levels on four datasets.\nDistribution of Samples. We visualize the distribution of\nsamples generated by SMURF-THP for several events to\nstudy the predicted intensity and present them in Figure 3.\nA large proportion of the samples stay close to value 0,\nwhich is reasonable since most of the events occur within\na short time after the previous one. Distributions vary as\ntime goes further. Figure 3(c) and Figure 3(d) exhibit that\nSMURF-THP can still present tiny peaks around the ground\ntruth value, indicating that our model can still capture the\neffect of historical events. Yet, the generated samples may\nbe inaccurate due to the huge randomness around the large\nvalues of arrival time.\nSAHP\nTHP\nSMURF-THP\nInterval LengthŒî\n‚àí0.4\n‚àí0.2\n0\n0.2\n0.4\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(a) StackOverflow\nInterval LengthŒî\n0\n1\n2\n3\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(b) Retweet\nSAHP\nTHP\nSMURF-THP\nInterval LengthŒî\n0\n0.5\n1.0\n1.5\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(c) MIMIC-II\nInterval LengthŒî\n‚àí2\n0\n2\n4\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(d) Financial\nFigure 2. Interval Length of different confidence levels on four\ndatasets. We minus the interval length of all methods with the\nlength of SMURF-THP (denoted as Interval Length‚àÜ) to better\nshow the differences.\n4.3. Ablation Study\nParametrization. In addition to parametrizing the inten-\nsity function, we can also parametrize the score function\ndirectly as was done in conventional score-based methods.\nThat is, we can mask the intensity function and directly\nparametrize the corresponding score function with the neu-\nral network. Table 3 summarizes the results on the Stack-\nOverflow and Retweet datasets, where SMURF-THPs in-\ndicates SMURF-THP with the score function parametrized.\nResults imply that parametrizing the intensity function fits\nthe sequence better than parametrizing the score function.\n7",
        "semantic_similarity": 0.10252058506011963,
        "keyword_overlap": 0.029420594416091263,
        "combined_score": 0.08059058786691112,
        "paragraph_id": 6,
        "source_title": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process"
      },
      {
        "text_fragment": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process\ndomains. Early work (Sahani et al., 2016) has adopted score\nmatching in modeling Poisson point process, but it fails\nto model complicated event dependencies induced in the\nmodern event data due to the simplified assumption that the\nintensity function is independent of the historical events.\nA suitable sampling algorithm for score matching-based\nmodels is Langevin Dynamics (LD), which can produce\nsamples from a probability density using only its score func-\ntion. Hsieh et al. (2018) propose Mirror Langevin Dynamics\n(MLD) as a variant of Langevin Dynamics that focuses on\nsampling from a constrained domain. We employs both LD\nand MLD for generating event samples in our experiments.\n3. Method\n3.1. Score Matching Objective of Hawkes Process\nLet S = {(ti, ki)}L\ni=1 denote an event sequence of length\nL, where each pair (ti, ki) corresponds to an event of type\nki ‚àà{1, ¬∑ ¬∑ ¬∑ , M} happened at time ti ‚àà[0, tmax]. Also, we\ndenote the history events up to time t as Ht = {(tj, kj) :\ntj < t}. Our goal is to learn pT,K(t, k | Ht), the joint\nconditional probability of the event proceeding time t given\nthe history Ht.\nTo employ score matching, we decompose the joint con-\nditional pdf pT,K by conditioning on the event time. By\ndoing so, the partial likelihood of the discrete event types\ncan be maximized by minimizing the cross entropy loss;\nand the marginal likelihood of the continuous event time\ncan be substituted by a score-matching objective. Such a\nsubstitution avoids the intractable integral of the intensity.\nSpecifically, we condition pT,K on the event time and we\nhave:\npT,K(t, k | Ht) = pT (t | Ht) ¬∑ pK | T (k | t, Ht).\nCorrespondingly, we have the log-likelihood:\n‚Ñì(S)=\nL\nX\ni=1\nlog pT (ti|Hti)+\nL\nX\ni=1\nlog pK|T (ki|ti, Hti). (3)\nWe can use a neural network to parameterize the intensity\nfunction and directly train the model using Eq. (3). How-\never, such an approach inevitably faces computational chal-\nlenges, i.e., exact computation of the intensity‚Äôs integral is\nintractable. Therefore, we derive a score-matching objective\nto substitute the first term in Eq. (3).\nIn SMURF-THP, we use a Transformer model with parame-\nters Œ∏ to parameterize the intensity function. More details\nare presented in Section 3.3. A sample‚Äôs score is defined as\nthe gradient of its log-density. Then, using Eq. (1), we can\nwrite the score of the i-th event given its history Hti and the\nmodel parameters Œ∏ as:\nœà(ti | Hti; Œ∏) = ‚àÇt log pT (ti | Hti; Œ∏)\n= ‚àÇt log Œª(ti | Hti; Œ∏) ‚àíŒª(ti | Hti; Œ∏).\nThe original objective of score matching is to minimize the\nexpected squared distance between the score of the model\nœà(¬∑; Œ∏) and the score of the ground truth œà‚àó(¬∑). However,\nminimizing such an objective is infeasible since it relies\non the unknown score œà‚àó(¬∑). We can resolve this issue by\nfollowing the general derivation in Hyv¬®arinen (2005) and\narrive at an empirical score-matching objective for Hawkes\nprocess with single type:\nbJ(Œ∏) =\nL\nX\ni=1\n[1\n2œà(ti | Hti; Œ∏)2 + ‚àÇtœà(ti | Hti; Œ∏)],\n(4)\nwhere ‚àÇtœà(ti | Hti; Œ∏) = ‚àÇ2\nt log Œª(ti | Hti; Œ∏) ‚àí‚àÇtŒª(ti |\nHti; Œ∏). We state in the follow theorem that the score match-\ning objective in Eq. (4) satisfies local consistency: minimiz-\ning bJ(Œ∏) is as sufficient as maximizing the first term of Eq.\n(3) for estimating the model parameters.\nTheorem 3.1. Assume the event time in sequence S fol-\nlows the model: p‚àó\nT (t | Ht) = pT (t | Ht; Œ∏‚àó) for some Œ∏‚àó,\nand that no other parameter gives a pdf that is equal 1 to\npT (¬∑; Œ∏‚àó). Assume further that the optimization algorithm is\nable to find the global minimum and pT (t | Ht; Œ∏) is positive\nfor all t and Œ∏. Then the score matching estimator obtained\nby minimizing Eq. (4) is consistent, i.e., it converges in prob-\nability towards Œ∏‚àówhen sample size approaches infinity.\nProof. Let œà‚àó(¬∑) and œà(¬∑; Œ∏) be the associated score func-\ntion of p‚àó\nT (¬∑) and pT (¬∑; Œ∏), respectively. The objective in Eq.\n(4) is an empirical estimator of the following objective:\nJ(Œ∏)= 1\n2\n* L\nX\ni=1\n(œà(ti | Hti; Œ∏) ‚àíœà‚àó(ti | Hti))2\n+\n{ti}L\ni=1‚àºp‚àó\nT\n.\nWe first prove that J(Œ∏) = 0 ‚áíŒ∏ = Œ∏‚àó. Since p‚àó\nT (¬∑) is\npositive, we can infer from J(Œ∏‚Ä≤) = 0 that œà(¬∑; Œ∏‚Ä≤) and\nœà‚àó(¬∑) are equal, which implies log p‚àó\nT (¬∑) = log pT (¬∑; Œ∏‚Ä≤)+c\nfor some constant c. Because both p‚àó\nT and pT are pdf‚Äôs,\nthe constant c must be 0 and hence we have p‚àó\nT = pT .\nBy assumption, Œ∏‚àóis the only parameter that fulfills this\nequation, so necessarily Œ∏‚Ä≤ = Œ∏‚àó.\nThen according to the law of large numbers, bJ converges\nto J as the sample size approaches infinity. Thus, the esti-\nmator converges to a point where J is globally minimized.\n1Equality of pdf‚Äôs are taken in the sense of equal almost every-\nwhere with respect to the Lebesgue measure.\n3",
        "semantic_similarity": 0.09065592288970947,
        "keyword_overlap": 0.032621951219512196,
        "combined_score": 0.07324573138865029,
        "paragraph_id": 2,
        "source_title": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process"
      }
    ],
    "resolved_title": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process"
  },
  {
    "citation_marker": "[13]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: YES. The statement accurately cites the title, authors, year, and conference (ACM SIG) of the 2DFQ paper, which matches the details provided in the cited paragraph. No misrepresentation, distortion, or fabrication is present."
    ],
    "evidence": [
      {
        "text_fragment": "2DFQ: Two-Dimensional Fair Queuing for\nMulti-Tenant Cloud Services\nJonathan Mace1, Peter Bodik2, Madanlal Musuvathi2, Rodrigo Fonseca1,\nKrishnan Varadarajan2\n1Brown University, 2Microsoft\nABSTRACT\nIn many important cloud services, diÔ¨Äerent tenants execute\ntheir requests in the thread pool of the same process, requiring\nfair sharing of resources. However, using fair queue schedulers\nto provide fairness in this context is diÔ¨Écult because of high ex-\necution concurrency, and because request costs are unknown\nand have high variance. Using fair schedulers like WFQ and\nWF2Q in such settings leads to bursty schedules, where large re-\nquests block small ones for long periods of time. In this paper,\nwe propose Two-Dimensional Fair Queuing (2DFQ), which\nspreads requests of diÔ¨Äerent costs across diÔ¨Äerent threads and\nminimizes the impact of tenants with unpredictable requests.\nIn evaluation on production workloads from Azure Storage,\na large-scale cloud system at Microsof, we show that 2DFQ\nreduces the burstiness of service by 1-2 orders of magnitude.\nOn workloads where many large requests compete with small\nones, 2DFQ improves 99th percentile latencies by up to 2\norders of magnitude.\nCCS Concepts\n‚Ä¢Networks ‚ÜíCloud computing; Packet scheduling;\n‚Ä¢Computer systems organization ‚ÜíAvailability;\nKeywords\nFair Request Scheduling; Multi-Tenant Systems\n1.\nINTRODUCTION\nMany important distributed systems and cloud services exe-\ncute requests of multiple tenants simultaneously. Tese include\nstorage, conÔ¨Åguration management, database, queuing, and\nco-ordination services, such as Azure Storage [9], Amazon\nDynamo [16], HDFS [53], ZooKeeper [36], and many more.\nIn this context, it is crucial to provide resource isolation to\nensure that a single tenant cannot get more than its fair share\nof resources, to prevent aggressive tenants or unpredictable\nworkloads from causing starvation, high latencies, or reduced\nPermission to make digital or hard copies of all or part of this work for personal\nor classroom use is granted without fee provided that copies are not made or\ndistributed for proÔ¨Åt or commercial advantage and that copies bear this notice\nand the full citation on the Ô¨Årst page. Copyrights for components of this work\nowned by others than the author(s) must be honored. Abstracting with credit is\npermitted. To copy otherwise, or republish, to post on servers or to redistribute to\nlists, requires prior speciÔ¨Åc permission and/or a fee. Request permissions from\npermissions@acm.org.\nSIGCOMM ‚Äô16, August 22 - 26, 2016, Florianopolis , Brazil\n¬© 2016 Copyright held by the owner/author(s). Publication rights licensed to\nACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00\nDOI: http://dx.doi.org/10.1145/2934872.2934878\nthroughput for others. Systems in the past have suÔ¨Äered cas-\ncading failures [19,27], slowdown [14,20,27,28,33], and even\ncluster-wide outages [14,19,27] due to aggressive tenants and\ninsuÔ¨Écient resource isolation.\nHowever, it is diÔ¨Écult to provide isolation in these sys-\ntems because multiple tenants execute within the same process.\nConsider the HDFS NameNode process, which maintains\nmetadata related to locations of blocks in HDFS. Users invoke\nvarious APIs on the NameNode to create, rename, or delete\nÔ¨Åles, create or list directories, or look up Ô¨Åle block locations.\nAs in most shared systems, requests to the NameNode wait\nin an admission queue and are processed in FIFO order by a\nset of worker threads. In this setting tenant requests contend\nfor resources, such as CPU, disks, or even locks, from within\nthe shared process. As a result, traditional resource manage-\nment mechanisms in the operating system and hypervisor\nare unsuitable for providing resource isolation because of a\nmismatch in the management granularity.\nIn many domains, resource isolation is implemented using\na fair queue scheduler, which provides alternating service to\ncompeting tenants and achieves a fair allocation of resources\nover time. Fair schedulers such as Weighted Fair Queuing [46],\nwhich were originally studied in the context of packet schedul-\ning, can be applied to shared processes since the setting is\nsimilar: multiple tenants submit Ô¨Çows of short-lived requests\nthat are queued and eventually processed by a server of lim-\nited capacity. However, in shared processes there are three\nadditional challenges that must be addressed:\n‚óèResource concurrency: Tread pools in shared processes\nexecute many requests concurrently, ofen tens or even\nhundreds, whereas packet schedulers are only designed for\nsequential execution of requests (i.e. on a network link);\n‚óèLarge cost variance: Request costs vary by at least 4 orders\nof magnitude across diÔ¨Äerent tenants and API types, from\nsub-millisecond to many seconds. By contrast, network\npackets only vary in length by up to 1.5 orders of mag-\nnitude (between 40 and 1500 bytes). Unlike CPU thread\nschedulers, requests are not preemptible by the application;\n‚óèUnknown and unpredictable resource costs: Te execu-\ntion time and resource requirements of a request are not\nknown at schedule time, are diÔ¨Écult to estimate up front,\nand vary substantially based on API type, arguments, and\ntransient system state (e.g., caches). By contrast, the length\nof each network packet is known a priori and many packet\nschedulers rely on this information.",
        "semantic_similarity": 0.4648873209953308,
        "keyword_overlap": 0.017980107115531753,
        "combined_score": 0.33081515683139107,
        "paragraph_id": 0,
        "source_title": "2DFQ: Two-Dimensional Fair Queuing for Multi-Tenant Cloud Services"
      }
    ],
    "resolved_title": "2DFQ: Two-Dimensional Fair Queuing for Multi-Tenant Cloud Services"
  },
  {
    "citation_marker": "[14]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: YES. The article's citation for [14] matches the details in the cited paragraph: authors (Mellor-Crummey and Scott), title (\"Algorithms for scalable synchronization on shared-memory multiprocessors\"), journal (ACM Trans. Comput. Syst.), volume (9), issue (1), month (Feb. 1991), and publication context (the paragraph ends with \"ACM Transactions on Computer Systems, Vol. 9, No. 1, February 1991\"). No misrepresentation, distortion, or fabrication is present."
    ],
    "evidence": [
      {
        "text_fragment": "Algorithms\nfor Scalable\nSynchronizahon\non Shared\n‚Äî Memory\nMultiprocessors\n.\n65\n31.\nMAE~AWA,\nM,\nA\n~\nalgorithm\nfor\nmutual\nexclusion\nin\ndecentralized\nsystems.\nACM\nTrans.\nComput.\nSyst.\n3, 2 (May\n1985),\n145-159,\n32.\nMELLOR-CRUMMEY,\nJ.\nM.\nConcurrent\nqueues:\nPractical\nfetch-and-@\nalgorithms,\nTech.\nRep,\n229,\nComputer\nScience\nDept,,\nUniv.\nof Rochester,\nNov.\n1987.\n33.\nMELLOR-CRUMMEY,\nJ.\nM.,\nAND\nScorn,\nM.\nL.\nAlgorithms\nfor\nscalable\nsynchronization\non\nshared-memory\nmultiprocessors.\nTech.\nRep.\n342,\nComputer\nScience\nDept,,\nUniv.\nof Rochester,\nApr.\n1990.\nAlso\nCOMP\nTR90-114,\nDept.\nof Computer\nScience,\nRice\nUniv.,\nMay\n1990\n34.\nOUSTERHOUT,\nJ.\nK\n, SCELZA,\nD,\nA.,\nAND\nSINDHU,\nP.\nS,\nMeciusa:\nAn\nexperiment\nin\ndis-\ntributed\noperating\nsystem\nstructure.\nCornmun.\nACM\n23,\n2 (Feb.\n1980),\n92-105.\n35,\nP1596\nWorking\nGroup\nof the\nIEEE\nComputer\nSociety\nMicroprocessor\nStandards\nCommittee.\nSCI\n(scalable\ncoherent\ninterface):\nA.n overview\nof extended\ncache-coherence\nprotocols,\nFeb\n5,\n1990.\nDraft\n0.59\nP1596/Part\nIII-D.\n36.\nPETERSON,\nG,\nL.\nA\nnew\nsolution\nto\nLamport‚Äôs\nconcurrent\nprogramming\nproblem\nusing\nsmall\nshared\nvariables.\nACM\nTrans.\nProgram.\nLang,\nSyst.\n5, 1 [Jan,\n1983),\n56-65.\n37.\nPFISTER, G,, BRANTLEY,\nW.\nC., GEORGE, D. A,,\nHARVEY,\nS. L.,\nKLEINFELDER,\nW. J , MCAVLIFF~,\nK.\nP.,\nMELTON,\nT\nA.,\nNORTON,\nV\nA\n, AND WEISS,\nJ.\nThe\nIBM\nresearch\nparallel\nprocessor\nprototype\n(RP3):\nIntroduction\nand\narchitecture.\nIn\nProceedings\nof\nthe\n1985\nInternational\nConference\non Parallel\nProcessing\n(Aug.\n1985),\n764-771.\n38.\nPFISTER,\nG.,\nAND\nNORTON,\nV.\nA.\n‚ÄúHot\nspot‚Äù\ncontention\nand\ncombining\nin\nmultistage\ninterconnection\nnetworks.\nIEEE\nTrans.\nComput,\nC-34,\n10 (Ott,\n1985),\n943-948.\n39.\nRAYMOND,\nK.\nA tree-based\nalgorithm\nfor\ndistributed\nmutual\nexclusion.\nACM\nTrans.\nCom -\nput.\nSyst\n7, 1 (Feb.\n1989),\n61-77.\n40.\nRAYNAL,\nM.\nAlgorithms\nfor\nMutual\nExcluszon.\nMIT\nPress\nSeries\nin\nScientific\nComputation.\nMIT\nPress,\nCambridge,\nMass,,\n1986,\nTranslated\nfrom\nthe\nFrench\nby\nD.\nBeeson.\n41.\nREED,\nD.\nP.,\nAND\nKANODIA,\nR\nK.\nSynchronization\nwith\neventcounts\nand\nsequencers\nCommun.\nACM\n22,\n2 (Feb.\n1979),\n115-123.\n42,\nRETTBERG,\nR.\nD.,\nCROWJT~ER, W,\nR.,\nCARVEY,\nP.\nP.,\nAND TOMLINSON,\nR.\nS.\nThe\nMonarch\nparallel\nprocessor\nhardware\ndesign,\nComputer\n23,\n4 (Apr\n1990),\n18-30\n43.\nRIGART,\nG.,\nAND AGRAWALA,\nA K.\nAn\noptimal\nalgorithm\nfor\nmutual\nexclusion\nin\ncomputer\nnetworks.\nCommzm\nACM\n24, 1 (Jan\n1981)\n9-17.\n44.\nRUDOLPH,\nL.,\nAND\nSEGALL,\nZ.\nDynamic\ndecentralized\ncache\nschemes\nfor\nMIMD\nparallel\nprocessors.\nIn\nProceedings\nof the International\nSymposium\non Computer\nArchitecture\n(1984),\n340-347.\n45.\nSANDERS,\nEt,\nA.\nThe\ninformation\nstructure\nof\ndistributed\nmutual\nexclusion\nalgorithms.\nACM\nTrans.\nComput.\nSyst.\n5, 3 (Aug.\n1987),\n284-299\n46.\nSCHNEIDER,\nF.\nB.\nSynchronization\nin\ndistributed\nprograms,\nACM\nTrans.\nProgram\nLang.\nSyst.\n4, 2 (Apr.\n1982),\n179-195.\n47,\nSCOTT,\nM.\nL.,\nLEBLANC,\nT.\nJ.,\nAND\nMARSH,\nB.\nD.\nMulti-model\nparallel\nprogramming\nin\nPsyche.\nIn\nProceedings\nof the Second\nACM\nSymposium\non Principles\nand\nPractice\nof Parallel\nProgramming\n(Mar.\n1990),\n70-78.\n48.\nTANG,\nP.,\nAND\nYEW,\nP.-C,\nProcessor\nself-scheduling\nfor\nmultiple-nested\nparallel\nloops,\nIn\nProceedings\nof the 1986\nInternational\nConference\non Parallel\nProcessing\n(Aug\n1986),\n528-535.\n49.\nTANG,\nP.,\nAND\nYEW,\nP.-C.\nAlgorithms\nfor\ndistributing\nhot-spot\naddressing\nCSRD\nRep.\n617,\nCenter\nfor\nSupercomputing\nResearch\nand\nDevelopment,\nUniv.\nof\nIllinois\nat\nUrbana-\nChampaign,\nJan.\n1987.\n50.\nYEW,\nP.-C.\nArchitecture\nof the\nCedar\nparallel\nsupercomputer\nCSRD\nRep\n609,\nCenter\nfor\nSupercomputing\nResearch\nand\nDevelopment,\nUniv.\nof Illinois\nat\nUrbana-Champaign,\nAug.\n1986.\n51.\nYEW,\nP.-C.,\nTZENG, N.-F.,\nAND LAWRIE,\nD. H.\nDistributing\nhot-spot\naddressing\nin\nlarge-scale\nmultiprocessors.\nIEEE\nTrans.\nComput.\nC-36,\n4 (Apr.\n1987),\n388-395.\n52,\nZAHORIAN,\nJ.,\nLAZOWSKA,\nE.\nD.,\nAND\nEAGER,\nD,\nl..\nThe\neffect\nof\nscheduling\ndiscipline\non\nspin\noverhead\nin\nshared\nmemory\nparallel\nprocessors.\nTech.\nRep.\nTR-89-07-03,\nComputer\nScience\nDept.,\nUniv.\nof Washington,\nJuly\n1989.\nReceived\nJune\n1990;\nrevised\nJanuary\n1991;\naccepted\nJanuary\n1991\nACM\nTransactions\non Computer\nSystems,\nVol.\n9, No.\n1, February\n1991.",
        "semantic_similarity": 0.6820339858531952,
        "keyword_overlap": 0.029312830370014416,
        "combined_score": 0.48621763920824096,
        "paragraph_id": 44,
        "source_title": "Algorithms for scalable synchronization on shared-memory multiprocessors"
      }
    ],
    "resolved_title": "Algorithms for scalable synchronization on shared-memory multiprocessors"
  },
  {
    "citation_marker": "[15]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.52, overlap: 0.01): NO. The cited paragraph discusses conventional resource scheduling issues (e.g., CPU, I/O, page cache) in multi-user systems, focusing on performance unfairness among users and the need to prioritize foreground applications. It does not mention \"intraprocess locks,\" \"locks for page faults,\" \"unfairness of locks,\" or \"priority inversion\"‚Äîkey elements of the original statement. Thus, the statement misrepresents the cited content by linking it to lock-related priority inversion, which is not addressed in [15].",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.50, overlap: 0.02): NO. The cited paragraph discusses CPU and I/O scheduling for user-level performance fairness/isolation via task groups and 'share' values, with no mention of intraprocess locks, priority inversion, or thread priority optimizations. The original statement's claims about locks causing priority inversion and failing thread priority optimizations are not supported by the cited content.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.48, overlap: 0.02): NO. The cited paragraph discusses URS's page cache management prioritizing foreground applications and compares it to conventional schemes, but does not mention intraprocess locks, page fault locks, priority inversion, or failures of thread priority optimizations due to lock unfairness. The original statement's claims about locks and priority inversion are not supported by the cited content.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "428\nIEEE TRANSACTIONS ON CONSUMER ELECTRONICS, VOL. 65, NO. 3, AUGUST 2019\nFig. 2.\nThe conventional page cache management.\nLike this, conventional resource scheduling schemes dis-\ntribute CPU and I/O resources only based on processes\nor sessions, without considering users. These conventional\nscheduling schemes can incur various problems in multi-\nuser surface computing systems. First, a user that runs more\nprocesses or sessions will use more system resources than\nother users, resulting in performance unfairness among the\nusers. Second, when a malicious user run a large number of\nprocesses, the entire system can halt because the malicious\nuser monopolizes almost all the system resources. Hence, con-\nventional scheduling schemes has a lack of consideration for\nmulti-user surface computing systems. To solve these prob-\nlems, a new resource scheduling scheme for multi-user surface\ncomputing systems need to be developed.\nC. Conventional Page Cache Management\nThe page cache is a specialized space residing in main\nmemory to bridge the performance gap between main memory\nand secondary storage. Since secondary storage is much slower\nthan main memory, issuing I/Os to the storage can severely\nincrease the latencies. To compensate for the low performance\nof the storage, the page cache retains frequently or recently\naccessed data in main memory. When the upcoming I/Os\naccess the cached pages, the I/Os are processed in the page\ncache without data transmission from/to the storage.\nAs depicted in Fig. 2, the modern page cache maintains two\ndifferent LRU queues, called the active list and the inactive list,\nto efÔ¨Åciently manage pages in the page cache [17]. (A) When\na new page is allocated in the page cache, it is Ô¨Årst inserted into\nthe inactive list. (B) After the page is accessed several times\n(by default, more than twice), it is promoted to the active\nlist. The eviction process is opposite to the allocation pro-\ncess. (C) When the system has insufÔ¨Åcient free pages in main\nmemory, the page cache management de-allocates pages at the\ntail of the inactive list to reclaim free pages. Additionally, (D)\nit demotes pages at the tail of the active list to the inactive\nlist, in order to balance the size of the lists.\nThe conventional page cache management does not distin-\nguish pages used by a foreground application from those used\nby background applications. Thus, the latency of a foreground\napplication can be increased, diminishing the quality of user\nexperience. In multi-user surface computing systems, there can\nbe multiple foreground applications, and therefore, a new page\ncache management scheme that prioritizes multiple foreground\napplications is necessary to enhance multi-user experiences.\nIII. DESIGN AND IMPLEMENTATION\nAs mentioned in Section II, conventional resource schedul-\ning, including CPU, I/O, and the page cache, has a lack of\nconsideration for the features of multi-user surface computing\nsystems. Therefore, conventional resource scheduling schemes\ncan cause severe degradation of user experience in multi-user\nsurface computing systems.\nIn this paper, we present a user-based resource scheduling\nscheme for multi-user surface computing systems, called URS,\nthat improves multi-user experience by providing performance\nfairness and isolation, and prioritizing each user‚Äôs foreground\napplication. For user-based scheduling and performance isola-\ntion among users, URS creates an individual resource group\nfor each user, called a user group, and inserts each applica-\ntion to its corresponding user group. To prioritize foreground\napplications, URS detects pages that are used by the fore-\nground applications, and tries to keep the pages in the page\ncache longer. We implement the user group using the Cgroups\nimplementation, which controls resources based on groups and\nprovides isolation among the groups. The details of Cgroups\nwill be discussed in Section V.\nA. Constructing User Groups for CPU and I/O Resources\nURS creates an individual user group per user for CPU\nand I/O resources, called ‚ÄúUser Task Group‚Äù and ‚ÄúUser CFQ\nGroup‚Äù, respectively. To create a user group for each user,\nURS needs to distinguish the ownership of applications. There\nhave been several research results [18], [19], [20] related to\nhow to pair applications and their owners. Since the scope\nof our paper does not include this subject, we suppose that\nusers log in to the system before they use it, and thus the\ncurrent version of URS simply utilizes the ‚Äòuid‚Äô (user id) of\napplications to deÔ¨Åne the owners of applications.\nAs described in Fig. 3, when a user logs in, URS checks\nif a user group with the ‚Äòuid‚Äô exists in the user group list\nby hooking system calls. If it exists, URS inserts the current\nprocess of the user into the user group. Otherwise, URS creates\na new user group linked to the user group list and inserts the\ncurrent (log-in) process of the user into the user group. A child\nprocess forked by a parent process belongs to the same user\ngroup because the child process copies the group information\nof the parent process. Therefore, if URS inserts a user‚Äôs current\nprocess into the user group on log-in, all the processes that\nthe user executes, belong to the same user group.\nB. CPU Scheduling for Performance Fairness and Isolation\nAmong Users\nIn CPU scheduling, URS constructs three layers of task\ngroups using the method explained in the previous subsec-\ntion. As described in Fig. 4, the Ô¨Årst layer is the root task\ngroup, of which runqueue contains all the user task groups\nas its scheduling entities. The second layer is the user task\ngroup, which URS provides to each user. Under user task\ngroups, there exist session task groups that the user owns.\nWe maintain the session task groups to beneÔ¨Åt from the con-\nventional CPU scheduling. Finally, a runqueue of each session\ntask group contains actual processes created on the session.\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:24:52 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.5237538516521454,
        "keyword_overlap": 0.011516314779270634,
        "combined_score": 0.3700825905902829,
        "paragraph_id": 2,
        "source_title": "URS: User-Based Resource Scheduling for Multi-User Surface Computing Systems"
      },
      {
        "text_fragment": "PARK AND EOM: URS\n429\nFig. 3.\nHow to construct user groups.\nFig. 4.\nCPU scheduling of URS for performance fairness and isolation\namong users.\nWhen URS schedules CPU resources, it Ô¨Årst chooses a user\ntask group with the lowest ‚Äòvruntime‚Äô. And then, URS chooses\na session task group with the lowest ‚Äòvruntime‚Äô in the user\ntask group. Finally, URS allocates CPU resources to the pro-\ncess with the lowest ‚Äòvruntime‚Äô in the session task group. In\nour CPU scheduling scheme, each user task group is an iso-\nlated scheduling entity from the other user task groups, and\nthe system allocates CPU resources to each user task group,\ndepending on its speciÔ¨Åed ‚Äòshare‚Äô. (The higher the ‚Äòshare‚Äô\nvalue is, the more CPU resources are allocated because the\n‚Äòvruntime‚Äô increases slowly.) Therefore, CPU scheduling of\nURS provides CPU performance isolation among users by cre-\nating a task group for each user. Additionally, it can provide\nCPU performance fairness by assigning the same ‚Äòshare‚Äô value\nFig. 5.\nComparison between the conventional and proposed CPU scheduling.\nFig. 6.\nI/O scheduling of URS for performance fairness and isolation\namong users.\nto all the user task groups, or it can prioritize applications of\na particular user by assigning a higher ‚Äòshare‚Äô value to the user.\nFor example, let us assume that there are four users who\nrun 1-4 applications, each of which is running on an individ-\nual session, as shown in Fig. 5. In the conventional system,\nCPU resources are distributed based on sessions. Therefore,\nthe system allocates quadruple the amount of resources to\nthe user who runs four applications, compared to the user\nwho runs only a single application. However, in case of the\nproposed scheme, all users have their own user group above\nthe session task groups. Therefore, the system can allocate the\nsame amount of CPU resources to each user regardless of the\nnumber of their applications, when all users have the same\n‚Äòshare‚Äô value.\nC. I/O Scheduling for Performance Fairness and Isolation\nAmong Users\nI/O scheduling of URS is similar to the CPU schedul-\ning. In conventional I/O scheduling, each process has its own\nCFQ queue to ensure fairness among the processes. On the\nother hand, the proposed scheme additionally creates a user\nCFQ group for each user above the process CFQ queue, as\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:24:52 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.5030742287635803,
        "keyword_overlap": 0.0228734810578985,
        "combined_score": 0.35901400445187576,
        "paragraph_id": 3,
        "source_title": "URS: User-Based Resource Scheduling for Multi-User Surface Computing Systems"
      },
      {
        "text_fragment": "432\nIEEE TRANSACTIONS ON CONSUMER ELECTRONICS, VOL. 65, NO. 3, AUGUST 2019\nFig. 10.\nUser-based scheduling experiments for I/O scheduling of URS.\nFig. 11.\nPerformance isolation experiments for I/O scheduling of URS.\nshows around two times higher read performance in total than\nthe other users because URS gives two times higher ‚Äòweight‚Äô\nvalue to the corresponding user CFQ group than the other CFQ\nuser groups.\nOn the isolation experiments in Fig. 11, the proposed\nscheme shows 1.66 times higher aggregate throughput than\nthe conventional scheme. As with the CPU scheduling exper-\niments, URS isolates each user from other users in terms\nof I/O performance by providing users with individual user\nCFQ groups. Accordingly, URS diminishes the inÔ¨Çuence of\nthe malicious user on the normal users. As a result, URS\nprovides user-based resource scheduling and performance iso-\nlation among multiple users for I/O resources, as well as CPU\nresources.\nC. Page Cache Management of URS\nTo verify the page cache management of URS, we sim-\nulate a situation where four users run one foreground and\none background application each. For both of the foreground\nand background applications, each user runs two FIO bench-\nmarks, each of which reads a 1GB Ô¨Åle with a request size\nof 128KB. To inform whether an application runs in the\nforeground or not, we explicitly set the different nice value\nto the foreground application. Also, we repeatedly run the\napplications for their data to be cached in the page cache layer.\nFig. 12.\nFIO-read experiments for verifying the page cache management\nof URS.\nThe experimental results in Fig. 12 are normalized to\nthe baseline where applications run with bypassing the page\ncache. In the baseline, all data should be read from the sec-\nondary storage. Only-FG in Fig. 12 shows the performance\nof the best case when only foreground applications are run-\nning without background ones. In Fig. 12, the conventional\nscheme shows 1.78 times higher average throughput than\nthe baseline, whereas URS shows 6.12 times higher aver-\nage throughput than the baseline. The conventional scheme\ndoes not distinguish the foreground applications, and thus their\ndata are frequently deallocated from the page cache because of\nbackground applications. However, URS differentiates data of\nforeground applications and prioritizes them by giving more\nopportunities to stay in the page cache. Eventually, the average\nthroughput of URS is only 11% less than that of Only-FG.\nV. RELATED WORK\nFair share scheduler [21] brings up the problem of schedul-\ning resources based on processes and tries to equally distribute\nCPU resources to users or groups. This work presents the\nconcept of ‚Äòshare‚Äô which indicates the entitlement to take\nresources. Fair share scheduler assigns the same amount of\n‚Äòshare‚Äô to each user and distributes resources based on the\n‚Äòshare‚Äô values, resulting in performance fairness among users\nor groups. However, the fair share scheduler only focuses on\nthe users‚Äô fairness without prioritizing the users according\nto the real-world priorities. Additionally, it does not accel-\nerate foreground applications which are critical to the user\nexperience of multi-user surface computing systems.\nAnother previous work related to the user-based resource\nscheduling is Cgroups. Cgroups is a user-space API to control\nand limit system resources, such as CPU, memory, and I/O,\nvia creating a resource group. By making a resource group per\nuser in the user-space, Cgroups can distribute system resources\nbased on users and isolate users from each other. However,\nsince Cgroups works in the user-space, it should keep track of\nthe ownership information of all applications by continuously\ninquiring it to the kernel layer. Besides, Cgroups cannot con-\ntrol pages for the page cache, and thus it cannot prioritize\nforeground applications in the page cache layer.\nFastTrack [12] mentions the importance of a foreground\napplication regarding user experience. It improves the I/O\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:24:52 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.48427003622055054,
        "keyword_overlap": 0.016129032258064516,
        "combined_score": 0.3438277350318047,
        "paragraph_id": 6,
        "source_title": "URS: User-Based Resource Scheduling for Multi-User Surface Computing Systems"
      }
    ],
    "resolved_title": "URS: User-Based Resource Scheduling for Multi-User Surface Computing Systems"
  },
  {
    "citation_marker": "[16]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.47, overlap: 0.04): NO. The original statement claims [16] \"prioritizes a foreground application on mobile environments by handling lock contentions in the page cache layer,\" but the cited paragraph from [16] focuses on a compiler framework for high-performance graph algorithms via a DSL and sparse linear algebra, with no mention of mobile environments, foreground applications, lock contentions, or the page cache layer. This is a misrepresentation.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.46, overlap: 0.02): NO. The original statement claims [16] \"prioritizes a foreground application on mobile environments by handling lock contentions in the page cache layer,\" but the cited paragraph from [16] focuses on a domain-specific compiler framework for high-performance graph algorithms via sparse linear algebra, with no mention of mobile environments, foreground applications, lock contentions, or the page cache layer. This misrepresents the content of [16].",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.40, overlap: 0.03): NO. The original statement claims [16] \"prioritizes a foreground application on mobile environments by handling lock contentions in the page cache layer,\" but the cited paragraph from [16] discusses a compiler framework for high-performance graph algorithms, with no mention of mobile environments, foreground applications, lock contentions, or the page cache layer. This is a misrepresentation.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "Automatic Code Generation for High-Performance\nGraph Algorithms\nZhen Peng‚àó, Rizwan A. Ashraf‚àó, Luanzheng Guo‚àó, Ruiqin Tian‚Ä†¬ß and Gokcen Kestor‚àó‚Ä°\n‚àóPacific Northwest National Laboratory, Richland, Washington, USA\n‚Ä† Horizon Robotics, Shanghai, China\n‚Ä° University of California, Merced, Merced, California, USA.\nEmails: {zhen.peng, rizwan.ashraf, lenny.guo, gokcen.kestor}@pnnl.gov, ruiqin.tian@horizon.ai\nAbstract‚ÄîGraph problems are common across many fields,\nfrom scientific computing to social sciences. Despite their im-\nportance and the attention received, implementing graph al-\ngorithms effectively on modern computing systems remains a\nchallenging task that requires significant programming effort\nand generally results in customized implementations. Current\ncomputing and memory hierarchies are not architected for\nirregular computations, resulting performance that is far from\nthe theoretical architectural peak. In this paper, we propose\na compiler framework to simplify the development of graph\nalgorihtm implementations that can achieve high performance\non modern computing systems. We provide a high-level domain\nspecific language (DSL) to represent graph algorithms through\nsparse linear algebra expressions and graph primitives including\nsemiring and masking. The compiler leverages the semantics\ninformation expressed through the DSL during the optimization\nand code transformation passes, resulting in more efficient IR\npassed to the compiler backend. In particular, we introduce\nan Index Tree Dialect that preserves the semantic information\nof the graph algorithm to perform high-level, domain-specific\noptimizations, including workspace transformation, two-phase\ncomputation, and automatic parallelization. We demonstrate that\nthis work outperforms state-of-the-art graph libraries LAGraph\nby up to 3.7√ó speedup in semiring operations, 2.19√ó speedup in\nan important sparse computational kernel, and 9.05√ó speedup\nin graph processing algorithms.\nIndex Terms‚Äîcompiler, optimization, graph algorithms, tri-\nangle counting, breadth-first search, code generation, semiring,\nmasking, sparse linear algebra\nI. INTRODUCTION\nGraphs data structures are used in many domains, from\ncomputer security [1] and computational genomics [2] to\nnetwork sciences [3] and scientific computing [4], to represent\ncomplex interactions and casual relationships (edges) among\nentities (nodes). While graphs adapt well to solve problems at\ndifferent scales, real-life problems often produce graph data\nstructures that are highly irregular and extremely large. These\ntwo factors pose challenges while implementing efficient graph\nalgorithms on modern computer architectures, which have\nbeen developed and optimized mostly for regular computation.\nTo achieve high-performance, developers are often forced to\nwrite ad-hoc code specifically tailored for given architectures\nusing a fairly low-level programming language, such as C/C++\n¬ßWork was performed while the author was at Pacific Northwest National\nLaboratory\nor CUDA, which, then, impedes the portability of the imple-\nmentation on different systems, productivity, and reusability.\nWith the proliferation of modern computing systems, the\ncurrent practice of manually reimplementing graph algorithms\nfor each new architecture is simply not sustainable.\nIn this work, we seek a solution to develop graph algo-\nrithms that provide high performance on modern computer\narchitectures but do not hinder portability and productivity.\nIn this endover, we identify two main challenges: 1) find the\nright level of abstraction to represent graph algorithms and\n2) lower that abstraction to efficient machine code. The level\nof abstraction should be high enough to enable developers\nto express graph algorithms effectively and with notations\nthat make sense in the application domain, both of which\nincrease productivity. Hand-tuned, architecture-specific imple-\nmentations (e.g., CUDA) may achieve high performance but\ndeveloping such solutions is time-consuming and not portable\nacross systems. The abstraction should carry sufficient seman-\ntics information to be used during code optimizations and\nmachine code generation to increase performance on specific\narchitectures. Finally, the abstraction should be architecture-\nindependent and semantically rich to guarantee portability\nacross different systems. In fact, it is generally easier to\nport high-level operations (e.g., sparse matrix-sparse matrix\nmultiplication) than low-level constructs (e.g., nested loops)\nacross systems. In this work, we opt for (sparse) linear\nalgebra as a reasonable programming abstraction to develop\nefficient graph algorithms. Algebraic representations of graph\nalgorithms are architecture-independent, sufficiently high-level\nso that users can effectively implement graph applications\nin their domains, and carry enough semantics information\nto inform the underlying system about which architecture-\nindependent and architecture-specific optimizations should be\nemployed. Compared to vertex-based and edge-traversal im-\nplementations, algebraic representations provide a compact\nand expressive way to represent graph algorithms, carrying\nsemantic information through the characteristics of the matrix\nused to represent the graph [5], [6], are easier to develop,\nmore portable, and can leverage a large body of research and\noptimization.\nThe second challenge is represented by mapping (lowering)\nthe high-level abstraction to efficient code for specific com-\nputing systems. The inherent irregularity of graph processing",
        "semantic_similarity": 0.4697279930114746,
        "keyword_overlap": 0.03699421965317919,
        "combined_score": 0.33990786100398596,
        "paragraph_id": 0,
        "source_title": "Automatic Code Generation for High-Performance Graph Algorithms"
      },
      {
        "text_fragment": "algorithms and the size of real-life graphs pose considerable\nchallenges when performing this process. The sheer size of\nreal-life problems makes it difficult, if not impossible, to store\ngraphs (i.e., adjacency matrix) using dense data structures.\nGiven the intrinsic sparse nature of graph structures, storing\ngraphs in dense format would introduce excessive pressure on\nthe memory subsystem and unnecessary computation. Efficient\ngraph implementations generally prefer sparse representations\nof the graph to reduce memory requirements and use sparse\noperators to increase computing efficiency by eliminating\nunnecessary computation. However, modern computing archi-\ntectures and memory technologies have been designed and\noptimized for dense computation and do not perform as\nwell for sparse computation [7]. The process of lowering\nhigh-level abstraction to efficient machine code must employ\ndifferent kinds of optimizations, both architecture-independent\nand architecture-specific, and should be performed at all\nlevels of the lowering process. First, the language should\nprovide high-level, graph-oriented operators that carry enough\ninformation for efficient code generation. Second, architecture-\nindependent, graph-specific optimizations, such as fusion and\nautomatic parallelization, should be applied to the high-level\ncode. Next, generic architecture-independent optimizations\n(loop unrollling, dead code elimination, etc.) should be con-\nsidered. Finally, the resulting code should be optimized for\nthe target architecture. This process should be automated to\nincrease productivity and portability and, to the extend that\nthe abstraction carries enough semantics information, should\nhave the user out of the loop.\nIn this work, we propose a domain-specific compiler frame-\nwork to develop graph algorithm implementations that can\nachieve high performance, are portable across different sys-\ntems, and are easy to develop. We propose a high-level\nDomain-Specific Language (DSL) to represent graph algo-\nrithms through (sparse) linear algebra expressions and specific\ngraph-oriented operators. The DSL allows users to embed\ndomain-specific semantics that is leveraged internally during\ncode generation through a series of optimizations and lowering\nsteps to generate efficient Intermediate Representation (IR),\nsuch as specific graph primitives including semiring and\nmasking. The proposed compiler is based on a multi-level IR\nand progressive lowering from high-level IRs (or dialects) that\nencapsulate the semantics of the application to low-level IRs,\nwhich are closer to the architecture. The compiler leverages the\nsemantics information expressed through the DSL during the\noptimization and code transformation passes. This generally\nresults in more efficient IR that can be passed to the compiler\nbackend (e.g., LLVM) to generate machine code compared\nto general-purpose programming environments, such as C or\nC++. In particular, we introduce an Index Tree Dialect. This\ndialect preserves the semantic information of the graph al-\ngorithm to perform high-level, domain-specific optimizations.\nSeveral code optimizations and transformations are applied\nwhile lowering the index tree IR to lower-level dialects in\nthe compilation pipeline, including optimizations specifically\ndeveloped in this work: workspace transformation, two-phase\ncomputation, and automatic parallelization. Workspace trans-\nformation takes advantage of intermediate dense structure to\nimprove the data locality and reduce computation complexity\nwhile preserving the sparse format of the resulting outputs.\nThe two-phase computation employs symbolic computation\nto deduce the minimum size for the output‚Äôs sparse data\nstructure. We also introduce a novel optimization algorithm\nthat leverages the symbolic information to perform automatic\nparallelization of sparse linear algebra primitives.\nWe show that by combining our DSL, optimizations\n(workspace transformation, two-phase computation, and paral-\nlelization), and efficient graph primitives (semiring and mask-\ning), we are able to outperform state-of-the-art graph libraries\n(e.g., LAGraph [6], which implements the GraphBLAS stan-\ndard [8]) by a significant margin. We evaluate the performance\nof several graph primitives and graph processing algorithms.\nOur results show that our work outperforms LAGraph by up to\n3.7√ó for semiring operations, 2.19√ó for SpGEMM kernel, and\n9.05√ó for graph processing algorithms Breadth First Search\n(BFS) and Triangle Counting (TC). In summary, this work\nmakes the following contributions:\n‚Ä¢ a novel compiler framework and DSL, which enable users to\nproductively develop the algebraic implementation of graph\nalgorithms and achieve high-performance.\n‚Ä¢ important graph primitives (semirings and masking op-\nerations)\nand\ncode\noptimizations\nand\ntransformations\n(workspace transformation, two-phase computation, paral-\nlelization) for efficient execution;\n‚Ä¢ a performance evaluation of sparse linear algebra kernels\nand two prominent graph processing algorithms and com-\nparison with LAGraph.\nThe rest of this work is organized as follows: Section II\nprovides an overview of the compiler; Section III introduces\nthe code generation optimizations of graph computations;\nSection IV demonstrates extended linear algebra primitives\nfor graph algorithms; Section V provides an exhaustive perfor-\nmance evaluation; finally, Section VI and Section VII compare\nthis work to other efforts and draw conclusions, respectively.\nII. OVERVIEW\nThis work proposes a domain-specific compiler framework\nto develop efficient graph algorithms represented by linear\nalgebra operations. Our work adheres to the GraphBLAS stan-\ndard, which provides a comprehensive set of graph primitives\nfor sparse matrices and vectors of various types and extends\nthe traditional linear algebra operators with semirings and\nmasking to achieve higher performance. The GraphBLAS-\nbased approach provides a consistent way for graph algorithm\nimplementation through common graph primitives that can\nbe optimized using well-studied techniques, and it avoids the\ncomplexity of writing different ad-hoc implementations com-\nmon with traditional vertex- or edge-centric approaches [9]‚Äì\n[11]. In most cases, the algorithmic complexity of the graph\nalgorithms implemented using linear algebra is close to the\ncomplexity of the implementation based on vertex- or edge-\ntransverses [12]. Given a graph G(V, E), where V is the set of",
        "semantic_similarity": 0.45952165126800537,
        "keyword_overlap": 0.017424975798644726,
        "combined_score": 0.3268926486271972,
        "paragraph_id": 1,
        "source_title": "Automatic Code Generation for High-Performance Graph Algorithms"
      },
      {
        "text_fragment": "VI. RELATED WORK\nGraph Libraries. There are numerous graph libraries,\nsuch as [6], [9]‚Äì[11], [24]‚Äì[30], that aim to provide high-\nperformance implementations of graph kernels using differ-\nent sequential and parallel algorithms. LAGraph [6] is a\nlibrary that contains representative graph algorithms and is\nbased on sparse linear algebra operations from the SuiteS-\nparse:GraphBLAS package [24]. On the other hand, NW-\nGraph [10] is a high-performance header-only graph library\nthat leverages C++20 features. However, different libraries\nhave their own approach to optimization and are tied to specific\nprogramming models. In contrast, the compiler potentially\noffers a unified solution for sequential and parallel code gener-\nation through the MLIR back-end while being complementary\nto existing library-based approaches.\nCompilers for Sparse Computations. There are several\ndomain-specific compilers designed for generating code of\nsparse operations in graph algorithms, including Green-\nMarl [31], GraphIt [32], and TACO [33]. These compilers,\nsuch as Green-Marl and TACO, perform source-to-source\ntranslation, where TACO translates its DSL operations to\nC++ using computational templates. However, TACO does not\nsupport parallel sparse computation (e.g., parallel SpGEMM),\nand its optimizations mainly focus on sequential code. In\ncontrast, the compiler in this work proposes optimizations\nincluding two-phase computation and parallelization for sparse\nkernels.\nRecently, MLIR infrastructure added support for sparse ten-\nsors through the sparse-tensor dialect [19]. COMET precedes\nthis support and does not utilize the sparse-tensor dialect.\nAs a result of MLIR‚Äôs support for sparse tensors, we can\nexpect more MLIR-based compilers to include support for\ngraph algorithms in the future. One such example is the\nmlir-graphBLAS [34] effort that plans to lower to linalg\ndialect. Previously, it used to generate code at the loop level\n(SCF dialect) in a similar manner to this work albeit without\noptimizations such as workspace transforms.\nVII. CONCLUSIONS\nWe present a compiler framework to simplify the devel-\nopment of graph algorithms and generate efficient code for\ntarget computing architectures. Built on top of COMET, this\ncompiler consists of a DSL for developing graph algorithms\nusing algebraic operations, optimized graph operators (such\nas semiring and masking), and various optimizations and code\ntransformations (such as workspace transformation, two-phase\ncomputation, and automatic parallelization). We demonstrate\nthe performance benefits of code generation through our\ncompiler using common graph algorithms and compare it to a\nstate-of-the-art library-based approach LAGraph. Our results\nshow that compared to LAGraph, our compiler can achieve up\nto 3.7√ó speedup in semiring operations, 2.19√ó speedup in an\nimportant sparse computational kernel, and 9.05√ó speedup in\ngraph processing algorithms.\nACKNOWLEDGMENT\nThe authors thank anonymous reviewers for their con-\nstructive comments and helpful suggestions. The research\ndescribed in this paper is part of the Data Model Convergence\nInitiative at Pacific Northwest National Laboratory (PNNL).\nIt was conducted under the Laboratory Directed Research and\nDevelopment Program at PNNL, a multi-program national\nlaboratory operated by Battelle for the U.S. Department of\nEnergy (DOE). This work was also supported by the U.S. DOE\nOffice of Science, Office of Advanced Scientific Computing\nResearch, under award 66150: ‚ÄúCENATE ‚Äì Center for Ad-\nvanced Architecture Evaluation‚Äù project. This work was also\nsupported by the High-Performance Data Analytics (HPDA)\nprogram at PNNL. PNNL is operated by Battelle for the U.S.\nDOE under Contract DE-AC05-76RL01830.",
        "semantic_similarity": 0.4045248031616211,
        "keyword_overlap": 0.025487256371814093,
        "combined_score": 0.290813539124679,
        "paragraph_id": 10,
        "source_title": "Automatic Code Generation for High-Performance Graph Algorithms"
      }
    ],
    "resolved_title": "Automatic Code Generation for High-Performance Graph Algorithms"
  },
  {
    "citation_marker": "[17]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: YES. The article's citation correctly references the authors (Shue, Freedman, Shaikh), year (2012), title (\"Performance Isolation and Fairness for Multi-Tenant Cloud Storage\"), and conference (USENIX Symposium on Operating Systems Design and Implementation, OSDI ‚Äô12). The cited paragraph from the document discusses the paper's focus on performance isolation and fairness in multi-tenant cloud storage, aligning with the citation details. No misrepresentation, distortion, or fabrication is present."
    ],
    "evidence": [
      {
        "text_fragment": "362‚ÄÉ 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI ‚Äô12)\t\nUSENIX Association\ncould adapt these techniques for memory and disk I/O re-\nsources. Stout [20] exploits batch processing to minimize\nrequest latency, but does not address fairness.\nSeveral other systems focused on course-grained alloca-\ntion. Autocontrol [23] and Mesos [15] allocate per-node\nCPU and memory to schedule batch jobs and VMs, using\nutility-driven optimization and dominant resource fair-\nness, respectively. They operate on a coarse per-task or\nper-VM level, however, rather than on per-application\nrequests. In [10], the authors apply DRF to Ô¨Åne-grained\nmulti-resource allocation, speciÔ¨Åcally to enforce per-Ô¨Çow\nfairness in middleboxes. However, their DRF queuing\nalgorithm relies on virtual time, and it scans each per-Ô¨Çow\npacket queue for the lowest virtual start time.\n8.\nConclusion\nThis paper seeks to provide system-wide per-tenant\nweighted fair sharing and performance isolation in multi-\ntenant, key-value cloud storage services.\nBy decom-\nposing this problem into a novel combination of four\nmechanisms‚Äîpartition placement, weight allocation,\nreplica selection, and fair queuing‚Äîour Pisces system\ncan fairly share the aggregate system throughput, even\nwhen tenants contend for shared resources and demand\ndistributions vary across partitions and over time. Our pro-\ntotype implementation achieves near ideal fairness (0.99\nMin-Max Ratio) and strong performance isolation.\nAcknowledgments. We thank Jennifer Rexford for help-\nful discussions early in this project. Funding was provided\nthrough NSF CAREER Award #0953197.\nReferences\n[1] http://aws.amazon.com/dynamodb/faqs/, 2012.\n[2] http://docs.amazonwebservices.com/\namazondynamodb/latest/developerguide/\nLimits.html, 2012.\n[3] http://www.couchbase.org/, 2012.\n[4] http://code.google.com/p/spymemcached/, 2012.\n[5] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity,\ndata center network architecture. In SIGCOMM, 2008.\n[6] H. Ballani, P. Costa, T. Karagiannis, and A. Rowstron. Towards\npredictable datacenter networks. In SIGCOMM, 2011.\n[7] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and\nR. Sears. Benchmarking cloud serving systems with YCSB. In\nSOCC, 2010.\n[8] S. L. GarÔ¨Ånkel. An evaluation of Amazon‚Äôs grid computing ser-\nvices: EC2, S3 and SQS. Technical Report TR-08-07, Harvard\nUniv., 2007.\n[9] A. Ghodsi, M. Zaharia, B. Hindman, A. Konwinski, S. Shenker,\nand I. Stoica. Dominant resource fairness: Fair allocation of\nmultiple resource types. In NSDI, 2011.\n[10] A. Ghodsi, V. Sekar, M. Zaharia, and I. Stoica. Multi-resource\nscheduling for middleboxes. In SIGCOMM, 2012.\n[11] A. Gulati, I. Ahmad, and C. A. Waldspurger. PARDA: Proportional\nallocation of resources for distributed storage access. In FAST,\n2009.\n[12] A. Gulati, A. Merchant, and P. J. Varman. mClock: Handling\nthroughput variability for hypervisor IO scheduling. In OSDI,\n2010.\n[13] C. Guo, G. Lu, H. J. Wang, S. Yang, C. Kong, P. Sun, W. Wu,\nand Y. Zhang. SecondNet: A data center network virtualization\narchitecture with bandwidth guarantees. In CoNext, 2010.\n[14] J. He, R. Zhang-Shen, Y. Li, C.-Y. Lee, J. Rexford, and M. Chiang.\nDavinci: dynamically adaptive virtual networks for a customized\ninternet. In CoNext, 2008.\n[15] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. Joseph,\nR. Katz, S. Shenker, and I. Stoica. Mesos: A platform for Ô¨Åne-\ngrained resource sharing in the data center. In NSDI, 2011.\n[16] A. Iosup, N. Yigitbasi, and D. Epema. On the performance vari-\nability of production cloud services. In CCGrid, 2011.\n[17] T. Lam, S. Radhakrishnan, A. Vahdat, and G. Varghese. Netshare:\nVirtualizing data center networks across services. Technical Report\nCS2010-0957, UCSD, 2010.\n[18] T. Li, D. Baumberger, and S. Hahn. EÔ¨Écient and scalable multi-\nprocessor fair scheduling using distributed weighted round-robin.\nIn PPoPP, 2009.\n[19] Y. Mao, E. Kohler, and R. Morris.\nCache craftiness for fast\nmulticore key-value storage. In EuroSys, 2012.\n[20] J. C. McCullough, J. Dunagan, A. Wolman, and A. C. Snoeren.\nStout: an adaptive interface to scalable cloud storage. In USENIX\nAnnual, 2010.\n[21] M. M. Michael and M. L. Scott. Simple, fast, and practical non-\nblocking and blocking concurrent queue algorithms. In PODC,\n1996.\n[22] R. M. Nauss. Solving the generalized assignment problem: An\noptimizing and heuristic approach. INFORMS J. Computing, 15\n(Summer):249‚Äì266, 2003.\n[23] P. Padala, K.-Y. Hou, K. G. Shin, X. Zhu, M. Uysal, Z. Wang,\nS. Singhal, and A. Merchant. Automated control of multiple\nvirtualized resources. In EuroSys, 2009.\n[24] D. Palomar and M. Chiang. A tutorial on decomposition methods\nfor network utility maximization. JSAC, 24(8):1439‚Äì1451, 2006.\n[25] L. Peterson, A. Bavier, and S. Bhatia. VICCI: A programmable\ncloud-computing research testbed. Technical Report TR-912-11,\nPrinceton CS, 2011.\n[26] L. Popa, G. Kumar, M. Chowdhury, A. Krishnamurthy, S. Rat-\nnasamy, and I. Stoica. FairCloud: Sharing the network in cloud\ncomputing. In SIGCOMM, 2012.\n[27] H. Rodrigues, J. R. Santos, Y. Turner, P. Soares, and D. Guedes.\nGatekeeper: supporting bandwidth guarantees for multi-tenant\ndatacenter networks. In WIOV, 2011.\n[28] A. Shieh, S. Kandula, A. Greenberg, C. Kim, and B. Saha. Sharing\nthe data center network. In NSDI, 2011.\n[29] D. B. Shmoys and E. Tardos. An approximation algorithm for\nthe generalized assignment problem. Math. Prog., 62(1):461‚Äì474,\n1993.\n[30] M. Shreedhar and G. Varghese. EÔ¨Écient fair queuing using deÔ¨Åcit\nround-robin. Trans. Networking, 4(3):375‚Äì385, 1996.\n[31] M. Stonebraker. The case for shared nothing. IEEE Database Eng.\nBulletin, 9(1):4‚Äì9, 1986.\n[32] M. Wachs, M. Abd-el-malek, E. Thereska, and G. R. Ganger.\nArgon: Performance insulation for shared storage servers. In\nFAST, 2007.\n[33] J. Wang, P. Varman, and C. Xie. Optimizing storage performance\nin public cloud platforms. J. Zhejiang Univ. ‚Äì Science C, 11(12):\n951‚Äì964, 2011.\n[34] D. X. Wei, C. Jin, S. H. Low, and S. Hegde. Fast TCP: Motivation,\narchitecture, algorithms, performance. Trans. Networking, 14(6):\n1246‚Äì1259, 2006.\n[35] M. Zaharia, A. Konwinski, A. D. Joseph, R. Katz, and I. Stoica.\nImproving MapReduce performance in heterogeneous environ-\nments. In OSDI, 2008.",
        "semantic_similarity": 0.6548703610897064,
        "keyword_overlap": 0.01745877788554801,
        "combined_score": 0.4636468861284589,
        "paragraph_id": 24,
        "source_title": "Performance Isolation and Fairness for Multi-Tenant Cloud Storage"
      }
    ],
    "resolved_title": "Performance Isolation and Fairness for Multi-Tenant Cloud Storage"
  },
  {
    "citation_marker": "[18]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.45, overlap: 0.01): NO. The original statement claims data centers co-locate multiple applications in a single physical machine to overcome over-provisioned resource idleness, citing [18]. However, the cited paragraph focuses on fairness in resource allocation for YARN (system model, fairness definitions, policy design) and does not discuss co-locating applications in a single physical machine or the motivation of overcoming over-provisioned resource idleness. The cited content does not support the specific claim.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.43, overlap: 0.01): NO. The original statement claims data centers co-locate multiple applications to overcome over-provisioned resource idleness, citing [18]. However, the cited paragraph focuses on LTYARN's resource allocation mechanisms (e.g., max-min fairness), evaluation setup with VMs/workloads, and does not mention co-locating applications to address over-provisioned resource idleness. The claim is not supported by the cited content.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.41, overlap: 0.01): NO. The original statement claims data centers co-locate multiple applications in a single physical machine to overcome over-provisioned resource idleness, citing [18]. However, the cited paragraph focuses on general resource sharing in cloud systems, YARN's architecture, and fairness properties (e.g., resource-as-you-contributed fairness) but does not explicitly mention \"co-locating multiple applications in a single physical machine\" as the method to address over-provisioned idleness. The specific claim in the original statement is not supported by the cited content.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "of fairness, which is used to assess fair policies in the follow-\ning sections.\n4.1\nSystem Model\nThis paper considers the single-resource allocation fairness\n(e.g., memory) for YARN in the most commonly used homoge-\nneous environment [3]. Let M ¬º f1 . . . mg be the set of\nmachines (or instances) in the shared computing system. For\neach machine, we assume the amount of resources (e.g., mem-\nory) is Ri. Thus, the total resource capacity R of the system is\nR ¬º Pm\ni¬º1 Ri. Let N ¬º f1 . . . ng be the set of users in the\nshared computing systems. Assume that the resource contri-\nbutions (i.e., shared weights) for n users are W ¬º fw1 . . . wng.\nAccording to the practical needs of resource allocation, these\nusers can be grouped into multiple queues of either single-\nlevel resource allocation structure or hierarchical resource\nallocation structure in YARN.\nWithout loss of generality, for example, there is a cloud\nsystem consisting of 100 instances of t2.medium type on the\ncloud, contributed by four users A; B; C and D with diverse\ndata-intensive workloads (e.g., MapReduce, Tez, HIVE, and\nSpark) equally. In that case, we can establish a shared com-\nputing system with YARN. According to practical needs,\nthe four users can be organized either into a single group\nfor single-level resource allocation (e.g., Fig. 1a) or multiple\ngroups for hierarchical resource allocation (e.g., Fig. 1b ).\nIn our work below, we focus on the fairness for these two\ntypes of resource allocation structures for n users, namely,\nsingle-level resource allocation (Section 5.1) and hierarchical\nresource allocation (Section 5.2).\n4.2\nFairness DeÔ¨Ånition\nWe consider the fairness from the resource allocation\nperspective. In a shared cloud environment, ideally,\nevery user wants to get more resources or at least the\nsame amount of resources in a shared computing system\nthan the ones of exclusively using her partition of the\nsystem. We call it fair for a user (i.e., sharing beneÔ¨Åt)\nwhen that can be achieved. In contrast, due to the\nresource contention in the shared system, it is also possi-\nble for the total resources a user received are less than\nthat without sharing, which we call unfair (i.e., sharing\nloss). To ensure resource-as-you-contributed fairness and\nthe maximization of sharing incentive property in the\nshared system, it is important to minimize sharing loss\nÔ¨Årstly and then maximize sharing beneÔ¨Åt.\nIn the remainder of this paper, we refer to the total\nresources as accumulated resources along the time. Let gi√∞t√û\nbe the currently allocated resources for the ith user at time t.\nLet fi√∞t√û denote the accumulated resources for the ith user at\ntime t. Thus,\nfi√∞t√û ¬º\nZ t\n0\ngi√∞t√û dt:\n(1)\nLet di√∞t√û and Si√∞t√û denote the current demand and current\nresource share for the ith user at time t, respectively. Given\nthe total resource capacity R of the system and the shared\nweight wi for the ith user, there is\nSi√∞t√û ¬º R \u0003 wi\n\u0002 X\nn\nk¬º1\nwk:\n(2)\nThe fairness degree ri√∞t√û for the ith user at time t is\ndeÔ¨Åned as the normalization result of the amount of resour-\nces a user obtained in a shared environment with respect to\nthe non-shared environment, i.e.,\nri√∞t√û ¬º\nAllocationResultWithSharing\nAllocationResultWithoutSharing ¬º\nR t\n0 gi√∞t√û dt\nR t\n0 minfdi√∞t√û; Si√∞t√ûgdt\n:\n(3)\nri√∞t√û \u0004 1 implies the absolute resource fairness for the ith\nuser at time t. In contrast, ri√∞t√û < 1 indicates unfair. We can\neasily see that for a user i in a non-shared partition of the\nsystem, it always holds ri√∞t√û ¬º 1, since it has gi√∞t√û ¬º\nminfdi√∞t√û; Si√∞t√ûg at any time t in this scenario. To measure\nhow much better or worse for sharing with a fair policy\nthan without sharing (i.e., ri√∞t√û \u0005 1), we propose two con-\ncepts sharing beneÔ¨Åt degree and sharing loss degree to quantify\nit, respectively. Let C√∞t√û be sharing beneÔ¨Åt degree, as a sum of\nall √∞ri√∞t√û \u0005 1√û subject to ri√∞t√û \u0004 1, i.e.,\nC√∞t√û ¬º\nX\nn\ni¬º1\nmaxfri√∞t√û \u0005 1; 0g;\n(4)\nand let V√∞t√û denote sharing loss degree, as a sum of all\n√∞ri√∞t√û \u0005 1√û subject to ri√∞t√û < 1, i.e.,\nV√∞t√û ¬º\nX\nn\ni¬º1\nminfri√∞t√û \u0005 1; 0g:\n(5)\nThereby, it always holds that C√∞t√û \u0004 0 \u0004 V√∞t√û. Moreover,\nwe see that in a non-shared partition of the computing sys-\ntem, it always holds C√∞t√û ¬º V√∞t√û ¬º 0, indicating that there\nare neither sharing beneÔ¨Åt nor sharing loss. In contrast, in a\nshared cloud computing system, either of them could be\nnonzero. For a good fair policy, it should be able to maxi-\nmize V√∞t√û Ô¨Årst (e.g., V√∞t√û ! 0) and next try to maximize\nC√∞t√û as much as possible. Finally, we can use this two met-\nrics to compare the fairness among different policies.\n5\nFAIR POLICY DESIGN AND PRINCIPLE FOR YARN\nIn this section, we give our design and principle of fair poli-\ncies for YARN under cloud computing environment. Both\nsingle-level resource allocation and hierarchical resource\nallocation are considered.\n5.1\nSingle-Level Resource Allocation\nFor single-level resource allocation, we Ô¨Årst give a motiva-\ntion example to show that MemoryLess Resource Fairness\nis not suitable for cloud computing system. Then we\nFig. 1. A comparison example of the long-term resource fairness\nbetween single-level and multi-level resource allocations for four users\nA; B; C and D with equal share of the whole resource service, where Gi\ndenotes the ith group and G0 represents the total resource service of\nthe whole system.\nTANG ET AL.: FAIR RESOURCE ALLOCATION FOR DATA-INTENSIVE COMPUTING IN THE CLOUD\n23\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:32:54 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.45003825426101685,
        "keyword_overlap": 0.011627906976744186,
        "combined_score": 0.31851515007573505,
        "paragraph_id": 3,
        "source_title": "Fair Resource Allocation for Data-Intensive Computing in the Cloud"
      },
      {
        "text_fragment": "RC Ô¨Årst estimates the assumed execution time for each\nrunning/completed task with the updated quantum value\nfrom QU. Next, it computes and updates the accumulated\nresource for each application/queue.\n6.2.3\nResource Allocator\nResource Allocator locates at each queue of different levels,\nas shown in Fig. 4. It is triggered whenever there are pend-\ning tasks and idle resources. RA can now support FIFO,\nmemoryless max-min fairness and long-term max-min fair-\nness for each queue. Users can choose either of them\naccordingly. For long-term max-min fairness, it performs\nfair resource allocation for each application/queue with\nthe provided resource information from RC, based on For-\nmula (6). We provide two important conÔ¨Åguration argu-\nments for each queue, e.g., time quantum Q and round\nlength L in the default conÔ¨Åguration Ô¨Åle, to meet different\nrequirements for different queues. Moreover, we also sup-\nport minimum (maximum) resource share for queues\nunder long-term max-min fairness.\n7\nEVALUATION\nWe ran our experiments in a local cloud environment,\nwhich is established in a cluster consisting of 10 compute\nnodes, each with two Intel X5675 CPUs (six CPU cores per\nCPU with 3.07 GHz), 24 GB DDR3 memory. We emulate\nthe t2:medium instances of Amazon EC2 by conÔ¨Åguring h2\ncores, 4 GBi per VM and thereby create 6 VMs per node.\nThe Hadoop-2.2.0 is chosen in our experiment. We conÔ¨Åg-\nure 1 instance as master, and the remaining 59 instances\nas slaves.\nOur evaluation methodology is as follows. We Ô¨Årst con-\nstruct a single-level hierarchy for LTYARN with the four\nmacro-workloads below. We compare LTYARN with the\ndefault Hadoop Fair Scheduler (HFS). Second, we construct\na two-level hierarchy for LTYARN by grouping four macro-\nworkloads into groups to assess our design on hierarchical\nresource allocations.\n7.1\nMacro-Benchmarks\nTo evaluate our long-term fair scheduler LTYARN for\nYARN, we ran a macro-benchmark consisting of four differ-\nent workloads:\n\u0001\nA MapReduce instance with a mix of small and large\njobs based on the workload at the Facebook.\n\u0001\nA MapReduce instance running a set of large-size\nbatch\njobs\ngenerated\nwith\nPurdue\nMapReduce\nBenchmarks Suite [33].\n\u0001\nHive [25] running a series of TPC-H queries.\n\u0001\nSpark [13] running a series of machine learning\napplications.\n7.2\nMacro-Workloads\nSynthetic facebook workload. We synthesize our Facebook work-\nload based on the distribution of jobs sizes and inter-arrival\ntime at Facebook in Oct. 2009 provided by Zaharia et. al. [20].\nThe workload consists of 100 jobs. We categorize them into\nnine bins of job types and sizes, as listed in Table 3. It is a mix\nof large number of small-sized jobs (1 \u0007 15 tasks) and small\nnumber of large-sized jobs (e.g., 800 tasks3). The job submis-\nsion time is derived from one of SWIM‚Äôs Facebook workload\ntraces (e.g., FB-2009_samples_24_times_1hr_1.tsv) [34]. The\njobs are from Hive benchmark [35], containing four types of\napplications, i.e., rankings selection, grep search (selection),\nuservisits aggregation and rankings-uservisits join.\nPurdue workload. We select Ô¨Åve benchmarks (e.g., Word-\nCount, TeraSort, Grep, InvertedIndex, HistogramMovices)\nrandomly from Purdue MapReduce Benchmarks Suite [33].\nWe use 40 G wikipedia data [36] for WordCount, Inverte-\ndIndex and Grep, 40G generated data for TeraSort and His-\ntogramMovices with their provided tools. To emulate a\nseries of regular job submissions in a data warehouse, we\nsubmit these Ô¨Åve jobs sequentially at a Ô¨Åxed interval of 3\nmins to the system.\nTPC-H. To emulate continuous analytic query, such as\nanalysis of users‚Äô behavior logs, we ran TPC-H benchmark\nqueries on hive [14]. 40 GB data are generated with pro-\nvided data tools. Four representative queries Q1, Q9, Q12,\nQ17 are chosen, each of which we create Ô¨Åve instances. We\nwrite a script to launch each one after the previous one Ô¨Ån-\nished in a round robin fashion.\nSpark. Latest version of Spark has supported its job to run\non the YARN system. We consider two CPU-intensive\nmachine learning algorithms, namely, kmeans and alternat-\ning least squares (ALS) with provided example bench-\nmarks. We ran 10 instances of each algorithm, which are\nlaunched by a script that waits 2 minutes after each job com-\npleted to submit the next.\nWe conÔ¨Ågure a single-level hierarchy with the four work-\nloads, as shown in Fig. 5. Each leaf queue corresponding\nto a workload. We use it for the following sections, i.e., Sec-\ntions 7.3, 7.4, 7.5, and Appendix D of the supplemental\nmaterial, available online. Besides, the hierarchy of more\nlevels is given and used in Section 7.6.\nTABLE 3\nJob Types and Sizes for Each Bin in Our Synthetic\nFacebook Workloads\nBin\nJob Type\n# Maps\n# Reduces\n# Jobs\n1\nrankings selection\n1\nNA\n38\n2\ngrep search\n2\nNA\n18\n3\nuservisits aggregation\n10\n2\n14\n4\nrankings selection\n50\nNA\n10\n5\nuservisits aggregation\n100\n10\n6\n6\nrankings selection\n200\nNA\n6\n7\ngrep search\n400\nNA\n4\n8\nrankings-uservisits join\n400\n30\n2\n9\ngrep search\n800\n60\n2\nFig. 5. Single-level hierarchical for LTYARN. Four different leaf queues\nare conÔ¨Ågured, i.e., Facebook, Purdue, Spark, TPC-H, corresponding to\neach workload, respectively.\n3. We reduce the size of the largest jobs in [20] to have the workload\nÔ¨Åt our cluster size.\n28\nIEEE TRANSACTIONS ON SERVICES COMPUTING,\nVOL. 11,\nNO. 1,\nJANUARY/FEBRUARY 2018\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:32:54 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.42848527431488037,
        "keyword_overlap": 0.010388739946380697,
        "combined_score": 0.3030563140043305,
        "paragraph_id": 8,
        "source_title": "Fair Resource Allocation for Data-Intensive Computing in the Cloud"
      },
      {
        "text_fragment": "single user‚Äôs application is changing over time, implying\nthat it is hard to keep the high resource utilization all the\ntime. More effective resource sharing is critical for improv-\ning the resource utilization in the cloud.\nIn this paper, let‚Äôs consider a cloud-based computing sys-\ntem shared by n users, where user i has a resource contribu-\ntion of ki to the pool of cloud resources. To enable resource\nsharing sustainable between users in the long run, we\nshould guarantee the proportional relationship between\nthe amount of total resources a user used over a period of\ntime and the amount of resources contributed by the user to\nthe shared cloud (i.e., resource-as-you-contributed fairness).\nOur aim thus turns to explore a fair resource allocation pol-\nicy that can meet all the aforementioned good properties\nlisted in Section 3.\n2.2\nResource Management in YARN\nYARN, as the next generation of Hadoop (i.e., Hadoop\nMRv2), has evolved to be a large-scale data operating plat-\nform and cluster resource management system. There is a\nnew architecture for YARN, which separates the resource\nmanagement from the computation model. Such a separa-\ntion enables YARN to support a number of diverse data-\nintensive computing frameworks including Dryad [6],\nGiraph, Hoya, Spark [13], Storm [12] and Tez. In YARN‚Äôs\narchitecture, there is a global master named ResourceManager\n(RM) and a set of per-node slaves called NodeManagers(NM),\nwhich forms a generic system for managing applications in a\ndistributed manner. The RM is responsible for tracking and\narbitrating resources among applications. In contrast, the\nNM has responsibility for launching tasks and monitoring\nthe resource usage per slave node. Moreover, there is another\ncomponent called ApplicationMaster(AM), which is a frame-\nwork-speciÔ¨Åc entity. It is responsible for negotiating resour-\nces from the RM and working with the NM to execute and\nmonitor the progress of tasks. Particularly, all resources of\nYARN are requested in the form of ‚Äòcontainer‚Äô, which is a logi-\ncal bundle of resources (e.g., <1 CPUs, 2G memory> ).\nAs a multi-tenant platform, YARN organizes users‚Äô sub-\nmitted\napplications\ninto\nqueues\nand\nshare\nresources\nbetween these queues. Users can set their own queues in a\nconÔ¨Åguration Ô¨Åle provided by YARN. When all users‚Äô\nqueues are conÔ¨Ågured at the same level, the cluster resour-\nces will then be allocated at one level, which we call the\nsingle-level resource allocation. Moreover, to reÔ¨Çect the hier-\narchical tree structure for organizations of users in practice,\nYARN also supports hierarchical queues of tree topology.\nEach queue can represent an organization or a user. In the\ntree topology, there is a root node called Root Queue. It dis-\ntributes the resources of the whole system to the intermedi-\nate nodes called Parent Queues. Each parent queue further\nre-distributes resources into its sub-queues (parent queues\nor leaf queues) recursively until to the bottom nodes called\nLeaf Queues. Finally, users‚Äô submitted applications within\nthe same leaf queue share the resources. We call this alloca-\ntion as hierarchical resource allocation.\nThere is a Fair Scheduler [22] inside YARN, which can\nsupport both single-level and hierarchical resource alloca-\ntions. Moreover, both single-resource and multi-resource\nallocations are also supported. For the single-resource allo-\ncation, current version of YARN adopts the max-min fair\npolicy and focuses only on the memory resources. With\nregard to the multi-resource allocation, it takes the Domi-\nnant Resource Fairness (DRF) [23] and considers both CPU\nand memory resources.\nIn our paper, we focus on the single-resource allocation\nfor YARN by considering both single-level and hierarchical\nresource allocation in cloud computing. We remain the con-\nsideration for multi-resource allocation as future work.\n3\nCLOUD-ORIENTED RESOURCE ALLOCATION\nPROPERTIES\nWe present a set of desirable properties for cloud comput-\ning. Based on these properties, we design our fair allocation\npolicies for YARN. We have found the following Ô¨Åve impor-\ntant properties:\n\u0001\nSharing incentive: Each user should be better off shar-\ning the resources with others, than exclusively using\nthe resources individually. Consider a cloud system\nequally shared by n users over t period time. Then\neach user should get at least t \u0003 1\nn resources in the\nshared system.\n\u0001\nCost-efÔ¨Åcient workload incentive: Resources in the\ncloud are priced (i.e., not free). In a shared cloud sys-\ntem, we should encourage users to submit work-\nloads that generate positive utility to them (i.e., cost-\nefÔ¨Åcient workload) for cost efÔ¨Åciency and avoid\nthose spam workloads with no positive utility (i.e.,\ncost-inefÔ¨Åcient workload). That is, a user should be\nbetter off submitting cost-efÔ¨Åcient workload and\nyielding unused resources to others when not\nneeded. Otherwise, she may be selÔ¨Åsh and possesses\nall unneeded resources under her share by submit-\nting cost-inefÔ¨Åcient tasks in a shared computing\nenvironment.\n\u0001\nResource-as-you-contributed\nfairness:\nIn\nthe\ncloud,\nassume that each user contributes a certain number\nof machines (resources) to its common pool of\nmachines\n(resources).\nThen,\nthe\naccumulated\nresource that each user used received over time\nshould be in proportion to her contribution in the\nshared environment. This property is important as it\nis a Service-Level Agreement guarantee for users.\n\u0001\nStrategy-proofness: Users should not be able to get\nbeneÔ¨Åts by lying about their resource demands. This\nproperty is compatible with sharing incentive and\nresource-as-you-contributed fairness, since no user\ncan obtain more resources by lying.\n\u0001\nPareto efÔ¨Åciency: In a shared resource environment, it\nis impossible for a user to get more resources with-\nout decreasing the resource of at least one user. This\nproperty can ensure the system resource utilization\nto be maximized.\nAlthough this paper is focused on YARN, it is worth\nmentioning that our methodology can be applied to other\nresource management systems such as Mesos [10].\n4\nSYSTEM MODEL AND FAIRNESS DEFINITION\nIn this section, we Ô¨Årst model the single-resource allocation\nfor YARN in cloud environment. Next, we give the deÔ¨Ånition\n22\nIEEE TRANSACTIONS ON SERVICES COMPUTING,\nVOL. 11,\nNO. 1,\nJANUARY/FEBRUARY 2018\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:32:54 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.4125661253929138,
        "keyword_overlap": 0.010968921389396709,
        "combined_score": 0.29208696419185864,
        "paragraph_id": 2,
        "source_title": "Fair Resource Allocation for Data-Intensive Computing in the Cloud"
      }
    ],
    "resolved_title": "Fair Resource Allocation for Data-Intensive Computing in the Cloud"
  },
  {
    "citation_marker": "[19]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.56, overlap: 0.02): NO. The article's reference entry [19] has an incomplete and incorrect title (\"Enhancin\" instead of the full \"Enhancing Performance in Modern NVMe Solid State Drives\" as in the cited paragraph), misrepresenting the cited work.",
      "DeepSeek LLM confirms support for this match: YES. The article's citation references \"FLIN: Enabling Fairness and Enhancing...\" (truncated title), and the cited paragraph details FLIN's mechanisms for fairness (e.g., priority-aware arbitration, proportional waiting to avoid unfairness) and performance optimization (e.g., wait-balancing to minimize interference). The content aligns with the title's focus on fairness and performance, with no misrepresentation."
    ],
    "evidence": [
      {
        "text_fragment": "FLIN: Enabling Fairness and Enhancing Performance\nin Modern NVMe Solid State Drives\nArash Tavakkol‚Ä†\nMohammad Sadrosadati‚Ä†\nSaugata Ghose‚Ä°\nJeremie S. Kim‚Ä°‚Ä†\nYixin Luo‚Ä°\nYaohua Wang‚Ä†¬ß\nNika Mansouri Ghiasi‚Ä†\nLois Orosa‚Ä†‚àó\nJuan G√≥mez-Luna‚Ä†\nOnur Mutlu‚Ä†‚Ä°\n‚Ä†ETH Z√ºrich\n‚Ä°Carnegie Mellon University\n¬ßNUDT\n‚àóUnicamp\nModern solid-state drives (SSDs) use new host‚Äìinterface pro-\ntocols, such as NVMe, to provide applications with fast access\nto storage. These new protocols make use of a concept known\nas the multi-queue SSD (MQ-SSD), where the SSD has direct\naccess to the application-level I/O request queues. This removes\nmost of the OS software stack that was used in older protocols\nto control how and when the I/O requests were dispatched to\nstorage devices. Unfortunately, while the elimination of the OS\nsoftware stack leads to a significant performance improvement,\nwe show in this paper that it introduces a new problem: unfair-\nness. This is because the elimination of the OS software stack\neliminates the mechanisms that were used to provide fairness\namong applications in older SSDs.\nTo study application-level unfairness, we perform experiments\nusing four real state-of-the-art MQ-SSDs. We demonstrate that\nthe lack of fair scheduling mechanisms leads to high unfairness\namong concurrently-executing applications due to the interfer-\nence among them. For instance, when one of these applications\nissues many more I/O requests than others, the other applications\nare slowed down significantly. We perform a comprehensive\nanalysis of interference in real MQ-SSDs, and find four major\ninterference sources: (1) the intensity of requests sent by each\napplication, (2) differences in request access patterns, (3) the\nratio of reads to writes, and (4) garbage collection.\nTo alleviate unfairness in MQ-SSDs, we propose the Flash-\nLevel INterference-aware scheduler (FLIN). FLIN is a lightweight\nI/O request scheduling mechanism that provides fairness among\nrequests from different applications. FLIN uses a three-stage\nscheduling algorithm that protects against all four major sources\nof interference, while respecting the application-level priorities\nassigned by the host. FLIN is implemented fully within the\nSSD controller firmware, requiring no new hardware, and has\nnegligible (<0.06%) storage cost. Compared to a state-of-the-art\nI/O scheduler, FLIN improves the fairness and performance of a\nwide range of enterprise and datacenter storage workloads, with\nan average improvement of 70% and 47%, respectively.\n1. Introduction\nSolid-state drives (SSDs) are widely used as a storage\nmedium today due to their high throughput, low response\ntime, and low power consumption compared to conventional\nhard disk drives (HDDs). As more SSDs are deployed in data-\ncenters and enterprise platforms, there has been a continued\nneed to improve SSD performance. One area where SSD man-\nufacturers have innovated on SSD performance is the host‚Äì\ninterface protocol, which coordinates communication between\napplications and the SSD. SSDs initially adopted existing host‚Äì\ninterface protocols (e.g., SATA [88]) that were originally de-\nsigned for lower-performance HDDs. As the performance of\nthe underlying storage technology (e.g., NAND flash memory)\nused by the SSD increased, these host‚Äìinterface protocols\nbecame a significant performance bottleneck [114], mainly\nbecause these protocols rely on the OS to manage I/O requests\nand data transfers between the host system and the SSD.\nTo overcome this bottleneck, modern enterprise SSDs (e.g.,\n[30‚Äì33,65,66,87,103,104,110,111]) use new high-performance\nprotocols, such as the Non-Volatile Memory Express (NVMe)\nprotocol [21,83]. These new protocols make use of the multi-\nqueue SSD (MQ-SSD) [8,48,101] concept, where multiple host-\nside I/O request queues (in software) are directly exposed to\nthe SSD controller. There are two benefits to directly expos-\ning the request queues to the SSD controller: (1) there is no\nlonger any need for the OS software stack to manage the I/O\nrequests; and (2) the SSD can make more effective I/O request\nscheduling decisions than the OS, since the SSD knows ex-\nactly which of its internal resources are available or are busy\nserving other requests. Thus, the protocols eliminate the OS\nsoftware stack, enabling MQ-SSDs to provide significantly\nhigher performance than traditional SSDs [101,114].\nUnfortunately, eliminating the OS software stack also elim-\ninates critical mechanisms that were previously implemented\nas part of the stack, such as fairness control [6,8,35,55,56,84,\n89,106,118]. Fairness control mechanisms work to equalize\nthe effects of interference across applications when multiple\napplications concurrently access a shared device. Fairness is\na critical requirement in multiprogrammed computers and\nmulti-tenant cloud environments, where multiple I/O flows\n(i.e., series of I/O requests) from different applications concur-\nrently access a single, shared SSD [36,43,84,91,94,101].\nFor older host‚Äìinterface protocols, the OS software stack\nprovides fairness by limiting the number of requests that each\napplication can dispatch from the host to the SSD. Since fair-\nness was handled by the OS software stack, the vast majority\nof state-of-the-art SSD I/O request schedulers did not con-\nsider request fairness [20,36,37,40,77,90,112]. Surprisingly,\neven though new host‚Äìinterface protocols, such as NVMe, do\nnot use the OS software stack, modern MQ-SSDs still do not\ncontain any fairness mechanisms. To demonstrate this, we\nperform experiments using four real state-of-the-art enter-\nprise MQ-SSDs. We make two major findings. First, when two\napplications share the same SSD, one of the applications typi-\ncally slows down (i.e., it takes more time to execute compared\nto if it were accessing the SSD alone) significantly. When we\nrun a representative workload on our four SSDs, we observe\nthat such slowdowns range from 2x to 106x. Second, with the\nremoval of the fairness control mechanisms that were in the\nsoftware stack, an application that makes a large number of\nI/O requests to an MQ-SSD can starve requests from other\napplications, which can hurt overall system performance and\nlead to denial-of-service issues [8,84,101]. Therefore, we con-\nclude that there is a pressing need to introduce fairness control\nmechanisms within modern SSD I/O request schedulers.\nIn order to understand how to control fairness in a modern\nSSD, we experimentally analyze the sources of interference\namong I/O flows from different applications in an SSD using\nthe detailed MQSim simulator [101]. Our experimental results\nenable us to identify four major types of interference:\n1. I/O Intensity: The SSD controller breaks down each I/O\nrequest into multiple page-size (e.g., 4 kB) transactions. An\n397\n2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture\n2575-713X/18/$31.00 ¬©2018 IEEE\nDOI 10.1109/ISCA.2018.00041\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:34:02 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.5644833445549011,
        "keyword_overlap": 0.024266365688487584,
        "combined_score": 0.40241825089497707,
        "paragraph_id": 0,
        "source_title": "FLIN: Enabling Fairness and Enhancing Performance in Modern NVMe Solid State Drives"
      },
      {
        "text_fragment": "termined threshold (Fthr), and the new transaction belongs to\nthe flow with the maximum slowdown (line 19), the heuristic\nmoves the transaction ahead of all other high-intensity-flow\ntransactions (line 20). This approach can often help flows that\nare transitioning from low-intensity to high-intensity, as such\nflows are particularly susceptible to interference from other\nhigh-intensity flows. If fairness is above the threshold, which\nmeans that all high-intensity flows are being treated some-\nwhat fairly, the heuristic only reorders the high-intensity-flow\ntransactions for fairness (line 21).\n4.2. Priority-Aware Queue Arbitration\nMany host‚Äìinterface protocols, such as NVMe [83], allow\nthe host to assign different priority levels to each flow. The\nsecond stage of FLIN enforces these priorities, and ensures\nthat flows at the same priority level are slowed down by an\nequal amount. As we discuss in Section 4.1, FLIN maintains a\nread and write queue for each priority level (i.e., if there are P\npriority levels as defined by the protocol, FLIN includes P read\nqueues and P write queues in the DRAM of the SSD, for each\nflash chip). Priority-aware queue arbitration selects one ready\nread transaction from the transactions at the head of the P\nread queues, and one ready write transaction from the trans-\nactions at the head of the P write queues. The two selected\ntransactions then move to the last stage of the scheduler.\nWhenever more than one queue has a transaction at the\nqueue head that is ready to execute (i.e., the back end re-\nsources it requires are not being used by another transaction),\nthe queue arbiter uses a weighted round-robin policy [76] to\nselect the read and write transactions that move to the next\nscheduling stage. If the protocol defines P priority levels,\nwhere level 0 is the lowest priority and level P ‚Äì 1 is the\nhighest priority, FLIN assigns the weight 2i to priority level\ni. Under weighted round-robin, this means that out of every\n\u0002\ni 2i scheduling decisions in the second stage, transactions\nin the priority level i queue receive 2i of these slots.\n4.3. Wait-Balancing Transaction Selection\nWe design wait-balancing transaction selection, the last\nstage of FLIN, to minimize interference that occurs due to the\nread/write ratios (Section 3.3) and garbage collection demands\nof concurrently-running flows (Section 3.4). A transaction\nstalls if the back-end resources that it needs are being used by\nanother transaction (which can be a transaction from another\nflow, or a garbage collection transaction). Wait-balancing\ntransaction selection attempts to distribute this stall time\nevenly across all read and write transactions.\nWait-balancing transaction selection chooses one of the fol-\nlowing transactions to dispatch to the FCC: (1) the ready read\ntransaction determined by priority-aware queue arbitration\n(Section 4.2), which is stored in a read slot in DRAM; (2) the\nready write transaction determined by priority-aware queue\narbitration, which is stored in a write slot in DRAM; (3) the\ntransaction at the head of the garbage collection read queue\n(GC-RDQ); and (4) the transaction at the head of the garbage\ncollection write queue (GC-WRQ). Algorithm 2 depicts the\nselection heuristic, which consists of two steps.\nStep 1: Estimate the Proportional Wait. FLIN uses a\nnovel approach to determine when to prioritize reads over\nwrites, which we call proportional waiting. Previous schedul-\ning techniques [20,112] always prioritize reads over writes,\nwhich as we show in Section 3.3 leads to unfairness. Propor-\ntional waiting avoids this unfairness.\nAlgorithm 2 Wait-balancing transaction selection in FLIN.\n1: Inputs:\n2:\nReadSlot: the read transaction waiting to be issued to the SSD back end\n3:\nWriteSlot: the write transaction waiting to be issued to the SSD back end\n4:\nGC-RDQ, GC-WRQ: the garbage collection read and write queues\n5:\n6: PW read ‚ÜêEstimateProportionalWait (ReadSlot)\n// Section 5.4\n7: PW write ‚ÜêEstimateProportionalWait (WriteSlot)\n// Section 5.4\n8: if PW read > PW write then\n9:\nDispatch ReadSlot to FCC (flash chip controller)\n10: else\n11:\nif number of free pages < GCFLIN then\n12:\nGCMigrationCount ‚ÜêAssignGCMigrations(WriteSlot) // Section 5.4\n13:\nwhile GCMigrationCount > 0 do\n14:\nDispatch the transaction at the head of GC-RDQ to FCC\n15:\nDispatch the transaction at the head of GC-WRQ to FCC\n16:\nGCMigrationCount ‚ÜêGCMigrationCount - 1\n17:\nDispatch WriteSlot to FCC\nWe define the proportional wait time (PW) of a transaction\nas the ratio of its wait time in the scheduler (Twait), from the\ntime that the transaction is received by the scheduler until\nthe time the transaction is dispatched to the FCC, over the\nsum of the time required to perform the operation in the\nmemory (Tmemory) and transfer data back to the front end\n(Ttransfer). The transaction in the read slot is prioritized over\nthe transaction in the write slot when the read transaction‚Äôs\nproportional wait time is greater than the write transaction‚Äôs\nproportional wait time (lines 6‚Äì8 in Algorithm 2).\nStep 2: Dispatch Transactions. If the read transaction is\nprioritized, it is dispatched to the flash channel controller (see\nSection 2.1) right away (line 9). If the write transaction is pri-\noritized, the scheduler then considers also dispatching garbage\ncollection operations, since a write transaction takes signifi-\ncantly longer to complete than a read transaction and the cost\nof garbage collection operations can be amortized by serv-\ning them together with the selected write transaction. If the\nnumber of free pages available in the memory is lower than a\npre-determined threshold (GCFLIN), and there are garbage col-\nlection transactions waiting in the GC-RDQ or GC-WRQ, the\nscheduler (1) determines how many of these transactions to\nperform, which is represented by GCMigrationCount (line 12),\nand (2) issues GCMigrationCount transactions (lines 13‚Äì16).\nSince garbage collection transactions read a valid page and\nwrite the page somewhere else in the memory, FLIN always ex-\necutes a pair of read and write transactions. Once the garbage\ncollection transactions are done, the scheduler dispatches the\nwrite transaction (line 17).\nOptimizations. FLIN employs two optimizations beyond\nthe basic read-write wait-balancing algorithm. First, FLIN\nsupports write transaction suspension [112] whenever the pro-\nportional wait time (PW) of the transaction in the read slot is\nlarger than the PW of the currently-executing write operation.\nSecond, when the read and write slots are empty, FLIN dis-\npatches a pair of garbage collection read/write transactions.\nIf a transaction arrives at the read slot when the garbage col-\nlection transactions are being executed, FLIN preempts the\ngarbage collection write and executes the transaction at the\nread slot, to avoid unnecessarily stalling the incoming read.\n5. Implementation of FLIN\nTo realize the high-level behavior of FLIN (Section 4), we\nneed to implement a number of functions, particularly for\nStages 1 and 3 of the scheduler. We discuss each function in\ndetail in this section, and then discuss the overhead required\nto implement FLIN in an MQ-SSD.\n403\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:34:02 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.504313588142395,
        "keyword_overlap": 0.0045212765957446804,
        "combined_score": 0.3543758946783999,
        "paragraph_id": 6,
        "source_title": "FLIN: Enabling Fairness and Enhancing Performance in Modern NVMe Solid State Drives"
      }
    ],
    "resolved_title": "FLIN: Enabling Fairness and Enhancing Performance in Modern NVMe Solid State Drives"
  },
  {
    "citation_marker": "[1]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: YES. The article states FIFO-based locks exacerbate the convoy effect, which aligns with the cited paragraph's key point: \"the key issue of convoys is associated with the granting of locks in first-come first-served order.\" The cited work identifies FCFS (FIFO) scheduling as a cause of convoys, supporting the article's claim."
    ],
    "evidence": [
      {
        "text_fragment": "for the same lock (decreasing the traffic on a particular lock). \nThese techniques make convoys less likely and less stable. \nBut we suspect that convoys will \ncontinue to occur. In particular, there was no easy way to fix system entry-exit lock convoys \nusing mode or granularity techniques (we had to resort to special logic for this problem). \nWe also considered two strategies which seem to have few virtues: spin locks and integration of \nthe dispatcher and lock manager. \nSpin locks come in two flavors: \no \nBusy wait: holds the CPU until quantum runout. \n¬∞ \nLazy wait: branches to the dispatcher and tests the lock the next time it is dispatched. \nSpin locks eliminate convoys (as explained below convoys are caused by first-come first-served \nscheduling, spin locks don't have FCFS the discipline). In one set of experiments we performed, \nbusy wait locks increased system execution time (elapsed) by 75%. Lazy wait locks increased \nexecution time by 20%. That is, the CPU time wasted by spinning is greater than the cost of \nconvoys. \nAnother way to solve the convoy problem is to involve the dispatcher. \nNotice in the example \nthat P1 stupidly gave up the lock to P2. If P1 had hung on to the lock until it waited, and P2 \ndid the same then the convoy would flush itself immediately. The obvious solution is to have the \ndispatcher know about locks and have the dispatcher grant locks when tasks are switched. The \narguments against this approach are: \no \nThe book-keeping associated with giving up a lock at task switch is intimidating. \no \nFor reasons of modularity, the dispatcher should not know about locks, they are a higher \nlevel notion. \n‚Ä¢ \nThe solution does not generalize to multiple processors. \n¬∞ \nThe solution does not address pre-emption due to page faults. \nA SOLUTION \nThe key issue of convoys is associated with the granting of locks in first-come first-served order. \nSo we elect to grant all lock requests in random order in the hope that eventually everyone will \nget service. In theory, some process might ' starve' (never be granted its request) but in fact \nthe underlying scheduler and the stochastic nature of real systems cause each process to \neventually get service. \nThe proposed solution is: \nWhen releasing a lock, broadcast to all waiters that the lock is free: \nsp \nDO; \nCONVOY=LATCH.QUEUE; \n,/* atomic pair */ \nLATCH=FREE; \n/* atomic pair */ \nDO WHILE(CONVOY -,= NIL); \nWAKEUP FIRST OF CONVOY; /* CAR of list */ \nCONVOY = REST OF CONVOY;/* CDR of list */ \nEND; \nEND; \n23",
        "semantic_similarity": 0.6800855994224548,
        "keyword_overlap": 0.02567313713212273,
        "combined_score": 0.48376186073535515,
        "paragraph_id": 3,
        "source_title": "The convoy phenomenon"
      }
    ],
    "resolved_title": "The convoy phenomenon"
  },
  {
    "citation_marker": "[20]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.39, overlap: 0.02): NO. The cited paragraph discusses adversarial contrastive learning, coreset selection, and CIFAR-10 experiments (e.g., running time, accuracy), with no mention of lock workloads, rename benchmarks, CFL, Syncord, throughput, or Jain‚Äôs fairness index. The original statement's claims about kernel mutex benchmarks and performance comparisons are unsupported by the cited content, which is unrelated.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.34, overlap: 0.02): NO. The cited paragraph discusses adversarial contrastive learning, coreset selection (RCS/ACS), training methods (Fast-AT/Free-AT), and metrics like running time/accuracy on CIFAR-10. It does not mention locks, read-only workloads, cross-directory rename benchmarks, throughput, fairness index, CFL, or Syncord, which are the focus of the original statement. The citation [20] is misapplied to unrelated content.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.33, overlap: 0.03): NO. The cited paragraph discusses adversarial pre-training, coreset selection (RCS), and transfer learning for image classification (e.g., ResNet-50, CIFAR-10/100), with no mention of locks, read-only workloads, cross-directory rename benchmarks, throughput, or fairness indices. The original article's claims about lock performance are unrelated to the content of [20], indicating misrepresentation.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "Table 10: Impact of I for RCS evaluated on the CIFAR-10 task.\nI\nRuning time\n(hours)\nSLF\nSA (%)\nRA (%)\n10\n15.6\n76.31\n38.17\n20\n13.0\n75.96\n37.21\n50\n12.2\n75.87\n35.54\nTable 11: Impact of batch size for coreset selection evaluated on the CIFAR-10 task.\nPre-training\nBatch size for RCS\nRuning time\n(hours)\nSLF\nSA (%)\nRA (%)\nACL-Entire\n-\n42.8\n78.87\n39.19\nACL with Random\n-\n11.8\n72.01\n29.87\nACL with RCS\n64\n13.6\n76.21\n37.48\nACL with RCS\n128\n13.3\n76.15\n37.41\nACL with RCS\n256\n13.1\n75.89\n37.17\nACL with RCS\n512\n13.0\n75.96\n37.21\nTable 12: Compatibility with AdvCL [15] evaluated on the CIFAR-10 task via SLF.\nPre-training\nRuning time\n(hours)\nSLF\nSA (%)\nRA (%)\nAdvCL-Entire\n57.8\n80.89\n42.36\nAdvCL with Random\n11.0\n73.67\n33.61\nAdvCL with RCS\n13.5\n77.93\n38.89\nB.11\nEfficient ACL via RCS with Various Batch Sizes\nIn this subsection, we show the impact of the batch size during coreset selection. We trained ResNet-\n18 via ACL with RCS on CIFAR-10 and then linearly finetuned ResNet-18 models on CIFAR-10.\nThe batch size for RCS is selected from {64, 128, 256, 512} and the batch size for ACL keeps as 512.\nThe subset fraction keeps as 0.2. Other training settings exactly follow Section 4.1. We report the\nstandard and robust test accuracy in Table 11.\nWe can find that as the batch size for RCS decreases, the running time becomes larger. It is because\nthere is a larger number of batches needed to calculate the loss gradients during RCS when the batch\nsize becomes smaller. Besides, we observe that the test accuracy on the downstream tasks seems to\ngradually increase as the batch size decreases. Especially, ACL with RCS using 64 batch size gains\nconsistent improvement compared with ACL with RCS using 512 batch size, which indicates that a\nsmaller batch size for RCS is useful to improve the performance but consumes more running time.\nTherefore, there could be a trade-off between the running time and the transferability. We leave how\nto further improve the efficiency and effectiveness in maintaining the transferability of RCS as the\nfuture work.\nB.12\nRCS for Accelerating Another Variant AdvCL [15]\nFan et al. [15] proposed a variant of ACL method, called ‚ÄúAdvCL‚Äù, that leverages a standardly pre-\ntrained model on ImageNet-1K to generate pseudo-labels for CIFAR-10 training data via K-means\nclustering. Based on ACL [14], the loss function of AdvCL is composed of a weighted sum of the\nadversarial contrastive loss and an ensemble of the CE loss between the adversarial data and its\npseudo label over different choices of cluster number. By simply replacing the loss function of the\nACL LACL(¬∑) with that of AdvCL in Algorithm 1 and 2, we can apply RCS for efficient AdvCL.\nWe pre-trained ResNet-18 via AdvCL with RCS on CIFAR-10 using SGD for 1000 epochs with an\ninitial learning rate 0.5 and a cosine annealing scheduler. We set Œ≤ = 512, TACL = 5, œµACL = 8/255,\nœÅACL = 2/255, and TRCS = 3. The pre-training settings of AdvCL exactly follow Fan et al. [15].\nWe take the same configuration of RCS as that of RCS for ACL in Section 4.1, i.e., warmup epoch\nW = 100, the epoch interval for conducting RCS I = 20, and the subset fraction k = 0.1. Then, we\nevaluate the performance on the CIFAR-10 task via SLF.\nTable 12 shows that RCS can speed up AdvCL [15] as well. Besides, RCS is a principled method\nthat helps AdvCL to obtain effective robust representations since AdvCL with RCS always achieves\na higher test accuracy compared with AdvCL with Random. Therefore, the experimental results\n23",
        "semantic_similarity": 0.38545799255371094,
        "keyword_overlap": 0.02349979018044482,
        "combined_score": 0.27687053184173105,
        "paragraph_id": 22,
        "source_title": "Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection"
      },
      {
        "text_fragment": "Table 13: Comparison between Fast-AT [24] with ACS [33] and Fast-AT with RCS on CIFAR-10.\nTraining method\nRunning time (minutes)\nSA (%)\nRA under PGD-20 (%)\nRA under AutoAttack (%)\nFast-AT on the entire set\n25.0\n86.20\n45.80\n41.04\nFast-AT with ACS\n12.8\n82.71\n45.71\n40.92\nFast-AT with RCS\n12.0\n83.47\n45.89\n41.06\nTable 14: Compatibility with Free-AT [25].\nTraining method\nRunning time (minutes)\nSA (%)\nRA under PGD-20 (%)\nRA under AutoAttack (%)\nFree-AT on the entire set\n116.5\n84.18\n49.05\n45.14\nFast-AT with RCS\n60.6\n82.39\n49.24\n45.15\nthe loss function of SAT is\nLSAT(D; Œ∏) =\nN\nX\ni=1\n\u001a\nmax\nÀúxi‚ààBœµ[xi] ‚Ñì(g ‚ó¶fŒ∏(Àúxi), yi)\n\u001b\n,\n(28)\nwhere ‚Ñìis the Cross-Entropy (CE) loss and Àúxi is adversarial training data generated by PGD within\nthe œµ-ball centered at xi.\nThe loss function of TRADES is\nLTRADES(D; Œ∏) =\nN\nX\ni=1\n\u001a\n‚Ñì(g ‚ó¶fŒ∏(xi), yi) + c ¬∑\nmax\nÀúxi‚ààBœµ[xi] KL(g ‚ó¶fŒ∏(Àúxi), g ‚ó¶fŒ∏(xi))\n\u001b\n,\n(29)\nwhere ‚Ñìis the CE loss, KL(¬∑, ¬∑) is the KL divergence, c > 0 is a trade-off parameter, and Àúxi\nis adversarial training data generated by PGD within the œµ-ball centered at xi. We set c = 6,\nfollowing Zhang et al. [35]. Note that the parameters of g are updated during supervised AT. Here we\nomit the parameters of g since we only use the parameters of the feature extractor fŒ∏ in downstream\ntasks.\nThe RCS problem for supervised AT is formulated as follows:\nS‚àó=\narg max\nS‚äÜD,|S|/|D|=k\n‚àíLRD(U; Œ∏ ‚àíŒ∑‚àáŒ∏Lsupervised(S; Œ∏)),\n(30)\nin which we replace the ACL loss LACL(¬∑) in Eq. (5) with the supervised AT loss Lsupervised(¬∑)\n(e.g., LSAT(¬∑) and LTRADES(¬∑)). Due to that LRD(¬∑) only needs data and does not need any label,\nRCS is applicable to supervised AT, no matter if the validation set is unlabeled or labeled. By\nleveraging greedy search, we show the algorithm of RCS for supervised AT in Algorithm 3 and\nefficient supervised AT via RCS in Algorithm 4.\nB.13.1\nComparison Between RCS and ACS [33] in Speeding Up Supervised AT including\nFast-AT [24], SAT [34] and TRADES [35]\nComparison between Fast-AT with RCS and Fast-AT [24] with ACS [33].\nWe conducted Fast-\nAT, Fast-AT with ACS, and Fast-AT with RCS on CIFAR-10. The experimental setting of Fast-AT\nand Fast-AT with ACS exactly follows that in Section 4.3 of Dolatabadi et al. [33]. That is, we trained\nResNet-18 on CIFAR-10 via Fast-AT (œµ = 8/255, Œ± = 10/255) using SGD with 0.9 momentum for\n60 epochs with the initial learning rate of 0.1 and divided by 10 at Epoch 40 and 50. As for the RCS\nfor speeding up Fast-AT, we set the subset fraction as k = 0.5, warmup epoch as W = 10, and epoch\ninterval for executing RCS I = 10. We report the standard test accuracy and robust test accuracy\nevaluated by PGD-20 (œµ = 8/255, Œ± = 8/2550) and AutoAttack in Table 13. All experiments are\nconducted on one RTX A5000 GPU. Table 13 validate that RCS, without using labels, is more\nefficient and effective in speeding up Fast-AT.\nCompatibility with Free-AT [25].\nFurther, we conducted Free-AT [25] and Free-AT with RCS\non CIFAR-10. We trained ResNet-18 on CIFAR-10 via Free-AT (œµ = 8/255) using SGD with 0.9\nmomentum for 60 epochs with the initial learning rate of 0.01 and divided by 10 at Epoch 40 and\n50. We keep the configurations of RCS for speeding up Free-AT the same as above. We report the\nstandard test accuracy and robust test accuracy evaluated by PGD-20 and AutoAttack in Table 14.\nTherefore, RCS, without using label information, can further speed up both Fast-AT [24] and Free-\nAT [25] while almost maintaining the adversarial robustness. In addition, RCS without using labels\nis more efficient than ACS while achieving similar adversarial robustness compared with ACS.\nTherefore, it validates the effectiveness of RCS in efficient supervised AT.\n25",
        "semantic_similarity": 0.335759699344635,
        "keyword_overlap": 0.020776166209329674,
        "combined_score": 0.24126463940404339,
        "paragraph_id": 24,
        "source_title": "Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection"
      },
      {
        "text_fragment": "Table 19: Cross-task standard transferability [48] of adversarially pre-trained ResNet-50 from\nImageNet-1K to CIFAR-10 and CIFAR-100, respectively. We report the standard test accuracy (%)\nvia standard linear finetuning (SLF) and standard full finetuning (SFF). The number after the dash\nline denotes subset fraction k ‚àà{0.05, 0.1, 0.2}.\nPre-training\nRuning time\n(hours)\nCIFAR-10\nCIFAR-100\nSLF\nSFF\nSLF\nSFF\nStandard training [48] on entire set\n-\n78.84\n97.41\n57.09\n84.21\nSAT [48] on entire set\n286.1\n93.53\n98.09\n77.29\n86.99\nFast-AT [24] on entire set\n10.4\n90.91\n97.54\n73.35\n83.33\nSAT with Random-0.05\n38.7\n85.72\n95.27\n69.29\n82.34\nSAT with RCS-0.05\n48.2\n92.68\n97.65\n75.35\n84.71\nSAT with Random-0.1\n45.8\n87.14\n95.60\n71.23\n83.62\nSAT with RCS-0.1\n55.4\n92.92\n97.82\n75.41\n85.22\nSAT with Random-0.2\n70.3\n87.69\n96.10\n72.05\n84.14\nSAT with RCS-0.2\n79.8\n93.48\n98.06\n76.39\n85.44\nwe finetuned all the parameters of the encoder. We set the initial learning rate of 0.001 for standard\nfull finetuning and 0.01 for standard partial finetuning. The learning rate is divided by 10 at Epoch\n50 and 100. Here, we use the same data augmentation as Salman et al. [48] to resize the images of\nCIFAR-10 and CIFAR-100 to 224 √ó 224. Note that we used the standardly pre-trained ResNet-50\non the entire set and adversarially pre-trained ResNet-50 on the entire set released in the GitHub\nof Salman et al. [48] to reproduce the results of baselines.\nTable 19 shows that RCS substantially accelerates SAT on ImageNet-1K while consistently achieving\nhigher standard test accuracy on downstream tasks than standard pre-training. Besides, SAT with\nRCS always obtains much higher standard test accuracy than SAT with Random. It validates that\nRCS is an effective method for efficient supervised AT.\nIn addition, we also provide a comparison between the standard transferability of Fast-AT [24]\nand SAT with RCS. We downloaded the pre-trained ResNet-50 vis Fast-AT released in the GitHub\nof Wong et al. [24]. Although SAT with RCS consumes more running time than Fast-AT, SAT with\nRCS obtains significantly higher standard test accuracy on downstream tasks, which validates the\neffectiveness of RCS in maintaining the transferability of supervised robust pre-training methods.\nTherefore, our proposed RCS can be a unified and effective framework for speeding up both supervised\nand unsupervised robust pre-training while maintaining the transferability of pre-trained models.\nC\nPossible Negative Societal Impacts\nOur paper aims to speed up robust pre-training by decreasing the number of training data, which\ndecreases the running time of pre-training and should be friendly to the environment. However, to\nimplement all the experiments in this project, we have conducted the pre-training procedure at least\n264 times and the finetuning procedure at least 2376 times, which definitely emitted a bunch of CO2\nand consumed a large amount of electricity. Therefore, we hope that our project can help improve the\nefficiency of robust pre-training and make pre-training more environmental-friendly in the future.\n28",
        "semantic_similarity": 0.33224761486053467,
        "keyword_overlap": 0.029551954242135366,
        "combined_score": 0.24143891667501485,
        "paragraph_id": 27,
        "source_title": "Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection"
      }
    ],
    "resolved_title": "Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection"
  },
  {
    "citation_marker": "[2]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.19, overlap: 0.00): NO. The statement lists authors \"Irina Calciu, Dave Dice, Yossi Lev, Victor Luchangco, Virendra J.\" for citation [2], but the cited document's reference [2] is authored by \"P. J. Courtois, F. Heymans, and D. L. Parnas\". The authors in the statement do not match the actual authors of reference [2] in the cited document.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.09, overlap: 0.00): NO. The statement lists authors for citation [2], but the provided paragraph from \"NUMA-Aware Reader-Writer Locks\" does not include author information. There is no content in the cited paragraph to support or verify the listed authors, making the statement unsupported by the given text.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.06, overlap: 0.01): NO. The statement omits author Nir Shavit and truncates \"Virendra J. Marathe\" to \"Virendra J.\", misrepresenting the complete list of authors in the cited document.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "Locks\n1T\n2T\n3T\n4T\n6T\n8T\n12T\n16T\n24T\n32T\n48T\n64T\nC-RW-WP\n.510\n1.20\n1.78\n1.95\n3.08\n4.24\n6.99\n9.24\n14.5\n18.0\n26.7\n37.7\nC-RW-NP\n.521\n1.09\n1.64\n2.16\n3.09\n4.26\n6.58\n9.22\n14.3\n17.6\n25.4\n36.4\nC-RW-RP\n.550\n1.12\n1.77\n2.23\n3.31\n4.53\n7.09\n10.4\n13.3\n18.9\n34.5\n52.5\nC-RW-RP-opt\n.531\n1.15\n1.76\n2.19\n3.30\n4.54\n7.45\n10.5\n13.5\n17.9\n27.0\n41.2\nCohort\n.516\n1.25\n1.74\n2.35\n3.41\n4.36\n7.03\n8.55\n13.3\n18.2\n29.6\n44.3\nDR-MCS\n.531\n1.13\n1.58\n1.94\n3.17\n4.19\n6.79\n10.2\n17.6\n26.7\n49.6\n77.1\nDV\n.511\n1.18\n1.75\n2.14\n3.18\n4.12\n6.59\n9.88\n12.8\n21.3\n52.0\n56.8\nROLL\n.547\n1.25\n1.55\n1.99\n3.32\n4.50\n7.79\n11.3\n20.2\n29.0\n46.4\n63.3\nShirako\n.554\n1.16\n1.61\n2.07\n3.33\n4.53\n7.30\n10.7\n18.5\n27.0\n35.5\n55.2\nFigure 8. Scalability results of the Kyoto Cabinet kccachetest benchmark (with the command line arguments: wicked -th Thrds -capsiz\n2000000 100000). Each entry in the table reports wall clock time to completion in seconds.\nical sections, including short read-only and read-write ones, and\nlong and complex read-write ones. Overall, the workload is domi-\nnated by read-write critical sections, where the threads acquire the\nRW locks in write mode. As a result, Cohort performs compara-\nbly to our NUMA RW locks, and much better than all other locks\nthat contain NUMA-oblivious writers ‚Äì DR-MCS, DV, ROLL, and\nShirako. DR-MCS scales poorly because the underlying MCS lock\nacquired by writers forces the cache lines of the lock and the data it\nprotects to bounce between NUMA nodes more often than other\nlocks. Since Cohort signiÔ¨Åcantly curtails lock migration, it per-\nforms much better. Our NUMA-aware RW locks, except C-RW-RP,\nfurther extend the cohorting advantage because of NUMA-friendly\nreader-reader concurrency. C-RW-RP succumbs to the superÔ¨Çuous\nwriter ownership circulation performance problem described ear-\nlier, and, as a result, does not scale as well as our other locks. It\ndoes however scale better than all prior locks. Overall, C-RW-WP\nand C-RW-NP, which perform the best, outperform the best of the\nprior locks (DV and Shirako) by about 40%.\n5.\nConclusion\nThe rapid growth of multi-core multi-chip systems is making\nNUMA architectures commonplace, and fundamental data struc-\ntures and synchronization primitives must be redesigned to adapt\nto these environments. We introduced a new family of surprisingly\nsimple NUMA-aware reader-writer locks that outperform prior\nlock algorithms by a large margin. Writers use centralized lock\nmetadata and readers use decentralized metadata. Microbenchmark\nexperiments suggest that our best lock exceeds the performance of\nthe prior state-of-the-art by up to a factor of 10, and our exper-\niments on a real-world application, the Kyoto Cabinet database,\nshow our locks can boost the application‚Äôs performance by up to\n40%.\nAcknowledgments\nWe thank Doug Lea for useful discussions. Nir Shavit was sup-\nported in part by NSF grant 1217921.\nReferences\n[1] B. B. Brandenburg and J. H. Anderson. Spin-based Reader-Writer\nSynchronization for Multiprocessor Real-time Systems.\nReal-Time\nSyst., 46(1):25‚Äì87, 2010.\n[2] P. J. Courtois, F. Heymans, and D. L. Parnas. Concurrent control with\n‚Äùreaders‚Äù and ‚Äùwriters‚Äù. Communications of the ACM, 14(10):667‚Äì\n668, 1971.\n[3] D. Dice, V. J. Marathe, and N. Shavit.\nFlat Combining NUMA\nLocks. In Proceedings of the 23rd ACM Symposium on Parallelism\nin Algorithms and Architectures, 2011.\n[4] D. Dice. Solaris Scheduling: SPARC and CPUIDs. URL https://\nblogs.oracle.com/dave/entry/solaris_scheduling_and_\ncpuids.\n[5] D. Dice. A Partitioned Ticket Lock. In Proceedings of the 23rd ACM\nAymposium on Parallelism in Algorithms and Architectures, pages\n309‚Äì310, 2011.\n[6] D. Dice and N. Shavit. TLRW: Return of the Read-Write Lock. In Pro-\nceedings of the 22nd ACM Symposium on Parallelism in Algorithms\nand Architectures, pages 284‚Äì293, 2010.\n[7] D. Dice, V. J. Marathe, and N. Shavit. Lock Cohorting: A General\nTechnique for Designing NUMA Locks. In Proceedings of the 17th\nACM SIGPLAN symposium on Principles and Practice of Parallel\nProgramming, pages 247‚Äì256, 2012.\n[8] E. W. Dijkstra.\nThe origin of concurrent programming.\nchapter\nCooperating sequential processes, pages 65‚Äì138. 2002.\n[9] F. Ellen, Y. Lev, V. Luchangco, and M. Moir. SNZI: Scalable NonZero\nIndicators. In Proceedings of the 26th Annual ACM Symposium on\nPrinciples of Distributed Computing, pages 13‚Äì22, 2007.\n[10] E. Freudenthal and A. Gottlieb.\nProcess coordination with fetch-\nand-increment. In Proceedings of the 4th International Conference\non Architectural Support for Programming Languages and Operating\nSystems, pages 260‚Äì268, 1991.\n[11] W. C. Hsieh and W. E. Weihl.\nScalable Reader-Writer Locks for\nParallel Systems. In Proceedings of the Sixth International Parallel\nProcessing Symposium, 1991.\n[12] J. M. Mellor-Crummey and M. L. Scott. Algorithms for Scalable Syn-\nchronization on Shared-Memory Multiprocessors. ACM Transactions\non Computer Systems, 9(1):21‚Äì65, 1991.\n[13] J. M. Mellor-Crummey and M. L. Scott.\nSynchronization without\nContention. In Proceedings of the 4th International Conference on\nArchitectural Support for Programming Languages and Operating\nSystems, pages 269‚Äì278, 1991.\n[14] O. Krieger, M. Stumm, R. Unrau, and J. Hanna. A Fair Fast Scal-\nable Reader-Writer Lock. In Proceedings of the 1993 International\nConference on Parallel Processing, pages 201‚Äì204, 1993.\n[15] Y. Lev, V. Luchangco, and M. Olszewski.\nScalable Reader-Writer\nLocks. In Proceedings of the 21st Annual Symposium on Parallelism\nin Algorithms and Architectures, pages 101‚Äì110, 2009.\n[16] J. M. Mellor-Crummey and M. L. Scott.\nScalable Reader-Writer\nSynchronization for Shared-Memory Multiprocessors. In Proceedings\nof the 3rd ACM SIGPLAN Symposium on Principles and Practice of\nParallel Programming, pages 106‚Äì113, 1991.\n[17] Z. Radovi¬¥c and E. Hagersten. Hierarchical Backoff Locks for Nonuni-\nform Communication Architectures.\nIn HPCA-9, pages 241‚Äì252,\nAnaheim, California, USA, Feb. 2003.\n[18] J. Shirako, N. Vrvilo, E. G. Mercer, and V. Sarkar. Design, veriÔ¨Åcation\nand applications of a new read-write lock algorithm. In Proceedinbgs\nof the 24th ACM symposium on Parallelism in algorithms and archi-\ntectures, pages 48‚Äì57, 2012.\n[19] Victor Luchangco and Dan Nussbaum and Nir Shavit. A Hierarchical\nCLH Queue Lock. In Proceedings of the 12th International Euro-Par\nConference, pages 801‚Äì810, 2006.\n[20] D.\nVyukov.\nDistributed\nReader-Writer\nMu-\ntex.\nURL\nhttp://www.1024cores.net/home/\nlock-free-algorithms/reader-writer-problem/\ndistributed-reader-writer-mutex.",
        "semantic_similarity": 0.185624361038208,
        "keyword_overlap": 0.0022688598979013048,
        "combined_score": 0.130617710696116,
        "paragraph_id": 9,
        "source_title": "NUMA-Aware Reader-Writer Locks"
      },
      {
        "text_fragment": "Hsieh and Weihl [11] and Vyukov [20] independently suggested\na simple distributed2 approach to building scalable RW locks. Each\ndistributed RW lock contains N RW locks where N is the number\nof processors in the system. Each reader is mapped to a single\nRW lock, and must acquire that lock in read mode in order to\nexecute its critical section. A writer must acquire all the underlying\nRW locks in write mode to execute its critical section. Deadlocks\nbetween writers are avoided by forcing a speciÔ¨Åc locking order.\nThe approach can be made NUMA-aware by restricting N to the\nnumber of NUMA nodes in the system, and mapping each reader to\nthe lock dedicated to its node. This variant algorithm which we call\nDV (representing the initials of Vyukov), is partially NUMA-aware,\njust like the SNZI-based RW locks. Absent any writers, readers\non different nodes can obtain and release read permission without\ngenerating any inter-node write coherence trafÔ¨Åc. However, every\nwriter incurs the overhead of acquiring write permission for the RW\nlock of every node, potentially generating signiÔ¨Åcant coherence\ntrafÔ¨Åc. Thus, the performance of DV plummets with increased\nwriter activity. Also, because of the canonical locking order used\nto avoid deadlock, readers on nodes that appear late in the order\nmay enjoy an unfair performance advantage over readers running\non nodes that appear earlier.\nIn this paper we present a novel family of RW locks that are de-\nsigned to leverage NUMA features and deliver better performance\nand scalability than any prior RW lock algorithm. We take a three-\npronged approach in our lock designs. First, similar to DV, we\nmaintain a distributed structure for the readers metadata such that\nreaders denote their intent by updating only locations associated\nwith their node. By localizing updates to read indicators we reduce\ncoherence trafÔ¨Åc on the interconnect. Second, writers preferentially\nhand off access permission to blocked writers on the same node,\nenhancing reference locality in the node‚Äôs cache for both the lock\nmetadata and data accessed in the critical section it protects. Fi-\nnally, our algorithms maintain tight execution paths for both read-\ners and writers, reducing latency of the lock acquisition and release\noperations.\nOur RW lock algorithms build on the recently developed lock\ncohorting technique [7], which allows for the construction of\nNUMA-aware mutual exclusion locks. BrieÔ¨Çy, writers use a co-\nhort lock to synchronize with each other and to maintain writer-\nvs-writer exclusion. Using the cohort locking approach, a writer\nreleasing the lock generally prefers to transfer ownership to a pend-\ning local writer (if there is one), thus reducing lock migrations3\nbetween nodes.\nOur RW locks also contain distributed implementations of read\nindicators, a data structure that tracks the existence of readers [15].\nReaders ‚Äúarrive‚Äù at these read indicators during lock acquisition\nand ‚Äúdepart‚Äù from them during lock release. Writers query the\nread indicators to detect concurrently active readers. Because of\nthe distributed nature of our read indicators, the readers need to\naccess just the node-speciÔ¨Åc metadata of the lock. We additionally\nuse simple Ô¨Çags and checks for coordination between readers and\nwriters. The result is a family of surprisingly simple algorithms that\npush the performance envelope of RW locks on NUMA systems far\nbeyond the prior state-of-the-art algorithms.\nOur various RW locks can be differentiated on the basis of\nthe fairness properties they provide as recognized by Courtois et\n2 The term ‚Äúdistributed‚Äù was coined by Vyukov for his algorithm [20],\nbut this algorithm appears to be the same as Hsieh and Weihl‚Äôs ‚Äústatic\nalgorithm‚Äù [11]\n3 We say that lock migration occurs when the lock is consecutively acquired\nby threads residing on distinct NUMA nodes. On a cache-coherent NUMA\nsystem, lock migration leads to the transfer of cache lines‚Äìboth for lines\nunderlying the lock metadata as well as for lines underlying mutable data\naccessed in the critical section protected by the lock‚Äîfrom the cache\nassociated with the Ô¨Årst thread to that of the second thread.\nFigure 1. An example multi-core multi-chip NUMA system con-\ntaining 2 chips with 4 cores per chip. Each chip is a NUMA node.\nEach core can have multiple hardware thread contexts (not shown\nin the Ô¨Ågure). Each core has its individual L1 cache, and all cores\non a chip share an L2 cache. Inter-thread communication via local\ncaches (L1 and L2) is signiÔ¨Åcantly faster than via remote caches\nbecause the latter involve coherence messages across the intercon-\nnect. In the Ô¨Ågure, threads r1..r6 intend to acquire a RW lock in\nread mode, and threads w1..w6 intend to acquire the same lock in\nwrite mode.\nal. [2]. In particular, we present locks exhibiting different ‚Äúprefer-\nence‚Äù policies: reader-preference, writer-preference, and neutral-\npreference. The reader-preference policy dictates that readers\nshould acquire (be granted) the lock as early as possible, regard-\nless of arrival order, whereas the writer-preference policy has a\nsymmetric bias towards writers. More concretely, these preference\npolicies allow readers or writers to ‚Äúbypass‚Äù prior pending writ-\ners or readers (respectively) in the race to acquire the lock. The\npreference policies‚Äîexcept for the neutral policy‚Äîmay lead to\nstarvation of threads engaged in the non-preferred lock acquisition\noperation. We avoid such situations by allowing the lock mecha-\nnism to temporarily override the preference policy so as to allow\nforward progress of starving threads. Starving threads become ‚Äúim-\npatient‚Äù and transiently change the preference policy.\nWe present an empirical evaluation of our RW locks, compar-\ning them with each other and with prior RW lock implementations.\nOur evaluation, conducted on a 256-way 4-node Oracle SPARC\nT5440T M server, shows that our locks signiÔ¨Åcantly outperform all\nprior RW locks on a diverse set of workloads. In our microbench-\nmark experiments, our locks outperform the prior best RW lock\n(the SNZI-based ROLL lock [15]) by up to a factor of 10.\nWe discuss our RW lock design approach in Section 2. In Sec-\ntion 3, we present our lock algorithms in detail. We present our\nempirical evaluation in Section 4, and conclude in Section 5.\n2.\nLock Design Rationale\nNUMA-aware mutex locks have been explored in depth [3, 7, 17,\n19]. However, to the best of our knowledge, there has been no prior\neffort toward constructing NUMA-aware RW locks. NUMA-aware\nmutex lock designs pursue only one goal ‚Äì reduction of the lock",
        "semantic_similarity": 0.08977174758911133,
        "keyword_overlap": 0.00030674846625766873,
        "combined_score": 0.06293224785225524,
        "paragraph_id": 1,
        "source_title": "NUMA-Aware Reader-Writer Locks"
      },
      {
        "text_fragment": "NUMA-Aware Reader-Writer Locks\nIrina Calciu\nBrown University\nirina@cs.brown.edu\nDave Dice\nOracle Labs\ndave.dice@oracle.com\nYossi Lev\nOracle Labs\nyossi.lev@oracle.com\nVictor Luchangco\nOracle Labs\nvictor.luchangco@oracle.com\nVirendra J. Marathe\nOracle Labs\nvirendra.marathe@oracle.com\nNir Shavit\nMIT\nshanir@csail.mit.edu\nAbstract\nNon-Uniform Memory Access (NUMA) architectures are gain-\ning importance in mainstream computing systems due to the rapid\ngrowth of multi-core multi-chip machines. Extracting the best pos-\nsible performance from these new machines will require us to re-\nvisit the design of the concurrent algorithms and synchronization\nprimitives which form the building blocks of many of today‚Äôs appli-\ncations. This paper revisits one such critical synchronization prim-\nitive ‚Äì the reader-writer lock.\nWe present what is, to the best of our knowledge, the Ô¨Årst fam-\nily of reader-writer lock algorithms tailored to NUMA architec-\ntures. We present several variations which trade fairness between\nreaders and writers for higher concurrency among readers and bet-\nter back-to-back batching of writers from the same NUMA node.\nOur algorithms leverage the lock cohorting technique to manage\nsynchronization between writers in a NUMA-friendly fashion, bi-\nnary Ô¨Çags to coordinate readers and writers, and simple distributed\nreader counter implementations to enable NUMA-friendly concur-\nrency among readers. The end result is a collection of surprisingly\nsimple NUMA-aware algorithms that outperform the state-of-the-\nart reader-writer locks by up to a factor of 10 in our microbench-\nmark experiments. To evaluate our algorithms in a realistic setting\nwe also present performance results of the kccachetest benchmark\nof the Kyoto-Cabinet distribution, an open-source database which\nmakes heavy use of pthread reader-writer locks. Our locks boost\nthe performance of kccachetest by up to 40% over the best prior\nalternatives.\nCategories and Subject Descriptors\nD.1.3 [Programming Tech-\nniques]: Concurrent Programming\nGeneral Terms\nAlgorithms, Design, Performance\nKeywords\nNUMA, hierarchical locks, mutual exclusion, reader-\nwriter locks\n1.\nIntroduction\nAs microprocessor vendors aggressively pursue the production of\nbigger multi-core multi-chip systems (Intel‚Äôs Nehalem-based and\nOracle‚Äôs Niagara-based systems are typical examples), the com-\nputing industry is witnessing a shift toward distributed and cache-\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation\non the Ô¨Årst page. To copy otherwise, to republish, to post on servers or to redistribute\nto lists, requires prior speciÔ¨Åc permission and/or a fee.\nPPoPP‚Äô13,\nFebruary 23‚Äì27,2013,Shenzhen, China.\nCopyright c‚Éù2013 ACM 978-1-4503-1922/13/02. . . $15.00\ncoherent Non-Uniform Memory Access (NUMA) architectures. 1\nThese systems contain multiple nodes where each node has locally\nattached memory, a local cache and multiple processing cores. Such\nsystems present a uniform programming model where all memory\nis globally visible and cache-coherent. The set of cache-coherent\ncommunications channels between nodes is referred to collectively\nas the interconnect. These inter-node links normally suffer from\nhigher latency and lower bandwidth compared to the intra-node\nchannels. To decrease latency and to conserve interconnect band-\nwidth, NUMA-aware policies encourage intra-node communica-\ntion over inter-node communication.\nCreating efÔ¨Åcient software for NUMA systems is challenging\nbecause such systems present a naive uniform ‚ÄúÔ¨Çat‚Äù model of the\nrelationship between processors and memory, hiding the actual un-\nderlying topology from the programmer. The programmer must\nstudy architecture manuals and use special system-dependent li-\nbrary functions to exploit the system topology. NUMA-oblivious\nmultithreaded programs may suffer performance problems arising\nfrom long access latencies caused by inter-node coherence trafÔ¨Åc\nand from interconnect bandwidth limits. Furthermore, inter-node\ninterconnect bandwidth is a shared resource so coherence trafÔ¨Åc\ngenerated by one thread can impede the performance of other un-\nrelated threads because of queueing delays and channel contention.\nConcurrent data structures and synchronization constructs at the\ncore of modern multithreaded applications must be carefully de-\nsigned to adapt to the underlying NUMA architectures. One key\nsynchronization construct is the reader-writer (RW) lock.\nA RW lock relaxes the central property of traditional mutual ex-\nclusion (mutex) locks by allowing multiple threads to hold the lock\nsimultaneously in read mode. A thread may also acquire the lock in\nwrite mode for exclusive access. RW locks are used in a wide range\nof settings including operating system kernels, databases, high-end\nscientiÔ¨Åc computing applications and software transactional mem-\nory implementations [6].\nRW locks have been studied extensively for several decades [1,\n2, 11, 13‚Äì16], with proposals ranging from simple counter- or\nsemaphore-based solutions [2], to solutions leveraging centralized\nwait-queues [14, 16], to solutions that use more sophisticated data\nstructures such as Scalable Non-Zero Indicators (SNZI) [15]. Of\nthese, all but the SNZI-based solutions rely on centralized struc-\ntures to coordinate threads, and thus encounter scalability im-\npediments [15]. The SNZI-based algorithms keep track of read-\ners ‚Äì threads acquiring the RW lock in read mode ‚Äì with each\nreader arriving at a leaf in the ‚ÄúSNZI tree‚Äù. Readers can be made\nNUMA-aware by partitioning the leaves of the SNZI-tree among\nthe NUMA nodes, with threads arriving at SNZI leaves associated\nwith their node. Writers, however, remain NUMA-oblivious, which\ncan impair scalability.\n1 We use the term NUMA broadly to include Non-Uniform Communication\nArchitecture (NUCA) [17] machines as well.",
        "semantic_similarity": 0.05920892953872681,
        "keyword_overlap": 0.005724508050089445,
        "combined_score": 0.04316360309213559,
        "paragraph_id": 0,
        "source_title": "NUMA-Aware Reader-Writer Locks"
      }
    ],
    "resolved_title": "NUMA-Aware Reader-Writer Locks"
  },
  {
    "citation_marker": "[3]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: YES. The reference entry [3] correctly cites the title \"CLoF: A Compositional Lock Framework for Multi-level NUMA Systems\" and authors, and the cited paragraph discusses CLoF as a compositional framework for multi-level NUMA systems, aligning with the reference's title and focus. No misrepresentation or fabrication is present."
    ],
    "evidence": [
      {
        "text_fragment": "14 8\n16 24 32\n48\n64\n95\n127\nNumber of threads\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nMedian throughput (iter./¬µs)\nCLoF‚ü®4‚ü©-Arm\nHMCS‚ü®4‚ü©\nMCS\nCNA\nShÔ¨ÇLock\nFigure 4. LevelDB with increasing contention, comparing\ndifferent state-of-art locks and CLoF on Armv8.\n3.4\nIn Search for a Composable Approach\nGiven that modern NUMA architectures have deep hierar-\nchies (Section 3.1) and the best lock differs among archi-\ntectures and levels (Section 3.2), we propose CLoF in Sec-\ntion 4, a framework for building multi-level locks with level-\nheterogeneity. CLoF generates NUMA-aware locks that are\ncorrect by construction and tailored to the target platform\n(and underlying architecture).\nWe now demonstrate the potential of CLoF. In Figure 4,\nwe compare the throughput of LevelDB with the best CLoF-\nlock on Armv8, several state-of-the-art NUMA-aware locks,\nand MCS lock. For fewer than 32 threads, CNA lock and\nShflLock suffer from a shuffling overhead, degrading their\nperformance with respect to MCS lock. Once the NUMA\nlevel is crossed (> 32 threads), CNA lock and ShflLock match\nand later (> 64 threads) improve over MCS lock because they\nsupport NUMA-node level. Note that CLoF and HMCS do\nnot introduce that overhead. The support of the full hier-\narchy allows HMCS‚ü®4‚ü©to greatly outperform these locks.\nIntroducing heterogeneity gives CLoF additional 10% to 15%\nhigher performance from 8 to 128 threads. Similarly, the\nheterogeneity aspect is also beneficial for x86. In Figure 2,\nCLoF‚ü®4‚ü©outperforms HMCS‚ü®4‚ü©for most contention levels,\ne.g., by 5% with 8 threads and 33% with 96 threads.\nAs we are going to see in Section 5, the best CLoF-lock on\nx86 is not composed of the same NUMA-oblivious locks as\nthe best CLoF-lock on Armv8.\n4\nThe Compositional Lock Framework\nFigure 5 describes the CLoF user‚Äôs perspective. First, CLoF\nrequires a hierarchy configuration, i.e., a file describing the\nmemory hierarchy levels of target platform. As in Section 3.1,\nCLoF produces a heatmap of the target platform, from which\nthe user can identify these levels by grouping tiles colored\nwith similar intensity. The resulting configuration can be\ntuned to select only those levels most relevant to the user (see\nexample in Section 5.2). Note that, although not currently\nimplemented, identifying levels in a heatmap can be easily\nautomated. Next, the user selects a set of NUMA-oblivious\nNUMA-oblivious\nspinlocks ¬ß2:\n‚Äì Ticketlock\n‚Äì CLH lock\n‚Äì MCS lock\n‚Äì Hemlock\ntarget platform\ne.g., 128-core\nArmv8 server\nBest CLoF-lock(s)\nfor target platform,\ne.g., tkt-mcs-tkt\nVerify correctness,\ne.g., with GenMC\nand VSync [32]\nExperimentally\nfind memory\nhierarchy (see ¬ß3.1)\nTuning point:\nchoose selection\npolicy (e.g., ¬ß5)\nApply CLoF Lock\nGenerator\n(see ¬ß4.1)\nTuning point:\nchoose hierarchy\nlevels (e.g., ¬ß5)\nRun scripted bench-\nmark (see ¬ß4.3)\n0 4\n32\n64\n127\ncore 1\n127\n64\n32\n40\ncore 2\n148 162432\n48\n64\n95\n127\nNumber of threads\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nMedian throughput (iter./¬µs)\nOnly locks correct on\nthe target architecture\nhierarchy\nconfiguration\n100s of multi-level\nheterogeneous locks\nFigure 5. Workflow of CLoF: input is a target platform\nand a set of NUMA-oblivious locks; output is a correct best\nperforming multi-level heterogeneous NUMA-aware lock.\nGreen boxes are the contributions of this work; blue boxes\nare optional tuning points for the user; and gray boxes are\noutside the scope of this work.\nspinlocks, e.g., those described in Section 2. We call these\nthe basic locks. We verify and optimize these locks using the\nVSync framework [32].\nThe core of CLoF is the lock generator described in Sec-\ntion 4.1. Based on the set of correct basic locks and the hier-\narchy configuration, the CLoF lock generator outputs hun-\ndreds of multi-level heterogeneous NUMA-aware locks. In\nSection 4.2, we use an induction argument with model check-\ning to show that CLoF locks are correct by construction.\nFinally, the scripted benchmark described in Section 4.3\nselects the best CLoF lock for the target platform. The user\ncan change the default selection policy to prioritize low con-\ntention performance over high contention (we evaluate both\npolicies in Section 5).\nThe CLoF workflow (Figure 5) is fully automated with the\nexception of the tuning points: (1) creating the hierarchy\nconfiguration, and (2) choosing the selection policy.\n4.1\nThe Lock Generator\nWe now present our lock generator in two steps: first, we\naddress the issue of supporting multiple levels of heteroge-\nneous locks; second, we show how to abstract the spinlocks\nto isolate the framework from the lock implementations.\n4.1.1\nSyntactic Recursion. CLoF employs syntactic re-\ncursion to support different locks on each level of the hierar-\nchy. Since syntactic recursion unfolds at compile-time (e.g.,\nvia C++ templates or C macros), it does not have the over-\nhead of virtual function pointers. We describe the recursion\nin the lock generator with a simple domain specific language\nshown in Figure 6.\nLock sets. Let ùëôbe a lock in BasicLocks, i.e., the set of all\nNUMA-oblivious locks. Let ùêøbe a lock in ClofLocks, the set\nof all CLoF-generated locks. By construction, the composed\n856",
        "semantic_similarity": 0.6760393977165222,
        "keyword_overlap": 0.010812696198116497,
        "combined_score": 0.47647138726100047,
        "paragraph_id": 5,
        "source_title": "CLoF: A Compositional Lock Framework for Multi-level NUMA Systems"
      }
    ],
    "resolved_title": "CLoF: A Compositional Lock Framework for Multi-level NUMA Systems"
  },
  {
    "citation_marker": "[4]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.64, overlap: 0.02): NO. The article states that the Linux stock qspinlock [4] is an MCS-based qspinlock, but the cited paragraph from \"Malthusian Locks\" (ref [4]) does not mention the Linux qspinlock at all. It discusses MCSCR, MCSCRN, and other locks but provides no information about the Linux stock qspinlock's structure. Thus, the statement is not supported by the cited content.",
      "DeepSeek LLM confirms support for this match: YES. The article states the Linux stock qspinlock [4] is MCS-based, with a single global lock and a single waiting queue for local spin. The cited paragraph describes a lock with an outer (global spinning) lock and an inner MCS lock (local waiting queue), aligning with \"MCS-based\" and the presence of a global lock and local waiting queue. No misrepresentation is found."
    ],
    "evidence": [
      {
        "text_fragment": "NUMA-aware locks exploit the inter-socket topology, while\nour approach focuses on intra-socket resources. The NUMA-\naware HCLH lock [55] edits the nodes of a queue-based lock\nin a fashion similar to that of MCSCR, but does not provide\nCR and was subsequently discovered to have an algorithmic\nÔ¨Çaw.\nJohnson et al. [47] and Lim et al. [53] explored the trade-\noÔ¨Äs between spinning and blocking.\nEbrahimi et al. [33] proposed changes to the system\nscheduler, informed in part by lock contention and mutual\ninter-thread DRAM interference, to shuÔ¨Ñe thread priorities\nin order to improve overall throughput.\nHardware and software transactional memory systems\nuse contention managers to throttle concurrency in order to\noptimize throughput [74]. The issue is particularly acute for\ntransactional memory as failed optimistic transactions are\nwasteful of resources.\nVarious hardware schemes have been proposed to mitigate\nLLC thrashing, but none are available in commonly available\nprocessors [70]. Intel [45] allows static partitioning of the\nLLC in certain models designed for real-time environments.\n9.\nConclusion\nModern multicore systems present the illusion of having a\nlarge number of individual independent ‚Äúclassic‚Äù processors,\nconnected via shared memory. This abstraction, which un-\nderlies the symmetric multiprocessing SMP programming\nmodel, is a useful simpliÔ¨Åcation for programmers. In practice,\nhowever, the logical processors comprising these multicore\nsystems share considerable infrastructure and resources. Con-\ntention for those shared resources manifests in surprising\nperformance issues.\n* Destructive interference means we often face negative-sum situations. * sub-additive * Illusional; notional; illusory; facade; * Locks are in the business of medium-term scheduling.\n* Performance isolation failure; * cite spaa14-dice\nMulticore systems are fundamentally a deceit. Most of the time we live happily with the ‚ÄúSMP‚Äù illusion that we have a large number of independent processors. MLP and lowered\ncommunication costs active as palliative factors. But sometimes we have to face reality and deal with the fact that there are really lots of shared resources, subject to contention and\neven destructive interference, under the facade.\nWe describe a lock admission policy ‚Äì concurrency re-\nstriction ‚Äì that is intentionally unfair over the short term.\nOur algorithm intentionally culls excess threads ‚Äì supernu-\nmerary threads not required to sustain contention ‚Äì into an\nexplicit passive set. CR moderates and reduces the size of\nthe active circulating set, often improving throughput rela-\ntive to fair FIFO locks. Periodically, we reschedule, shifting\nthreads between the active and passive sets, aÔ¨Äording long-\nterm fairness. CR conserves shared resources and can reduce\nthrashing eÔ¨Äects and performance drop that can occur when\ntoo many threads compete for those resources, demonstrating\nthat judiciously managed and intentionally imposed short\nterm unfairness can improve throughput. CR provides a num-\nber of modes of beneÔ¨Åt for the various types of shared and\ncontended resources. We further show the subtle interplay\nof waiting policy, which must be carefully selected to fully\nleverage CR.\nWhile scalability collapse is not uncommon, it remains\na challenge to characterize which shared resources underly\na drop in performance. The analysis is diÔ¨Écult and in our\nexperience, multiple resources are often involved 36. While\nCR typically does no harm, it is also diÔ¨Écult to determine\nin advance if CR will provide any beneÔ¨Åt. CR gates access\nHowever, since CR typically does no harm, the decision to use it is simple.\n36 Suggesting the need for enhanced hardware performance facilities to\ndetect excessive competition for shared resources.\nto the resources involved in scalability collapse by moderat-\ning access to locks ‚Äì an unrelated resource. In the future we\nhope to employ more direct means to measure and control\nscalability collapse. Locks remain convenient, however, and\ndetecting oversubscription (contention) is relatively simple\ncompared to determining when some of the complex hard-\nware resources are oversubscribed. Contention is a convenient\nbut imprecise proxy for overthreading.\n* graceful; predictable; robust; automatic; autonomic; * adjusts automatically and promptly to varying load; * respond; response; react; adapt; adjust; lag; latency; promptly; reaction\ntime; response time; * Unsatisfying; * CR works in concert with both operating system and hardware * under high load ‚Äì handles preemption gracefully; under varying load ‚Äì fast\nresponse time\n9.1\nFuture Work\nThrottling in current CR designs is driven by the detection\nof contention. In the future we hope to vary the admission\nrate (and the ACS size) in order to maximize lock transit\nrates, possibly allowing non-working conserving admission\n[38]. This attempts to close the performance gap between\nsaturation and peak shown in Figure 1. We also intend\nWe also plan to further explore the application of intentionally unfair CR-based activation policies to semaphores and the pthread cond condition variable construct, tending to\nwake the most recently arrived threads. This approach shows promise for pools of worker threads where idle threads wait on a central condition variable.\nto explore energy-eÔ¨Écient locking in more detail, and the\nperformance advantages of CR on energy-capped systems.\nClassic CR is concerned with the size of the ACS. But\nwe can easily extend CR to be NUMA-aware by taking\nthe demographics of the ACS into account in the culling\ncriteria. For NUMA environments we prefer the ACS to be\nhomogeneous and composed of threads from just one NUMA\nnode. This reduces the NUMA-diversity of the ACS, reduces\nlock migrations and improves performance. Our MCSCRN\ndesign starts with MCSCR, but we add two new Ô¨Åelds: the\nidentity of the currently preferred ‚Äúhome‚Äù NUMA node, and\na list of remote threads. At unlock-time, the owner thread\ninspects the next threads in the MCS chain and culls remote\nthreads from the main chain to the remote list. A thread is\nconsidered remote if it runs on some node other than the\ncurrently preferred node. Periodically, the unlock operator\nalso selects a new home node from the threads on the remote\nlist, and drains threads from that node into the main MCS\nchain, conferring long-term fairness. If we encounter a deÔ¨Åcit\non the main list at unlock-time, then we simply reprovision\nfrom the remote list.\n* cull; splice; extract; excise; demote; remove; move; shift; sift; redact; resect * relegate; demote; * ACS = enabled set; PS = disabled set; * Surplus; excess; supernumerary; nimiety;\noverabundance; superabundance; surfeit; superÔ¨Çuity; redundant threads; * reduce NUMA-diversity of ACS; homogenize\nEarly experiments with NUMA-aware CR show that MC-\nSCRN performs as well as or better than CPTLTKTD[31],\nthe best known cohort lock. In addition, cohort locks re-\nquire one node-level lock for each NUMA node. Because\nof padding and alignment concerns to avoid false sharing,\nthose node-level locks themselves are large. Unlike cohort\nMCSCRN avoids that ‚Äì the lock size is Ô¨Åxed and small.\nlocks, MCSCRN locks are small and of Ô¨Åxed size In the un-\ncontended case, cohort locks require acquisition of both the\nnode-level and top-level, although a fast-path can be imple-\nmented that tries to avoid that overhead by opportunistically\nbypassing the node-level locks under conditions of no or light\ncontention when cohort formation is not feasible. MCSCRN\nis non-hierarchical, and avoids that concern, always using the\nfast-path. The system tends to converge quickly to a steady-\nstate where the arriving threads are largely from the home\nnode, so accesses to lock metadata elements avoids inter-node\ncoherence traÔ¨Éc. Finally, we note that it is a challenge to im-\nplement polite spin-then-park waiting in CPTLTKTD, but it\nis trivial to do so in MCSCRN. MCSCRN will be the topic\nof a subsequent publication.\n16\n2024-04-05",
        "semantic_similarity": 0.6350373923778534,
        "keyword_overlap": 0.015853356452811492,
        "combined_score": 0.4492821816003408,
        "paragraph_id": 16,
        "source_title": "Malthusian Locks"
      },
      {
        "text_fragment": "Arriving threads start with global spinning on the outer\nlock, and if they can‚Äôt manage to obtain the lock within\nthe arrival spinning phase, they then revert to the MCS\nlock, which uses local waiting. Global spinning allows more\neÔ¨Écient lock hand-over, but local spinning generates less\ncoherence traÔ¨Éc and provides gracefully performance under\nhigh contention [54]. Threads waiting on the inner MCS lock\nsimply spin or spin-then-park on the thread-local variable,\navoiding concerns about back-oÔ¨Äpolicies. All park-unpark\nactivity takes place on paths outside the critical section. The\ninner lock provides succession by direct handoÔ¨Ävia MCS,\nwhile the outer lock provides succession by competitive\nhandoÔ¨Ä. This constitutes a 3-stage waiting policy : threads\nÔ¨Årst spin globally; then, if necessary, enqueue and spin\nlocally; and then park.\nThe LOITER transformation allows us to convert a lock\nsuch as MCS, which uses direct handoÔ¨Ä, into a composite\nform that allows a fast path with barging. The resultant\ncomposite LOITER lock enjoys the beneÔ¨Åts of both direct\nhandoÔ¨Äand competitive succession, while mitigating the\nundesirable aspects of each of those policies. SpeciÔ¨Åcally,\nthe new construct uses direct handoÔ¨Äfor threads in the slow\ncontention path, but allows competitive succession for threads\ncirculating outside the slow path, retaining the best properties\nof both MCS and TAS locks.\nTo further restrict and constrain concurrency, the implementation can restrict or cap the number of threads spinning on a lock at any given moment.\nA useful complementary thread-local policy in the spin-\nning phase implementation is to abandon the current spin\nepisode if the TAS atomic operation on the outer lock fails\ntoo frequently. This condition indicates a suÔ¨Écient Ô¨Çow of\nthreads in the ACS over the lock. Another variation is to\nmonitor either traÔ¨Éc over the lock or the arrival or spinners,\nand to abandon the spin attempt if the rate or Ô¨Çux is too high.\nBy abandoning the spin attempt early, the thread reverts from\nspinning to parking. This is tantamount to self-culling.\nIf the inner lock is NUMA-friendly ‚Äì say, a cohort lock\n‚Äì then the aggregate LOITER lock is NUMA-friendly. As\nthreads circulate between the active and passive sets, the inner\nignored. The implementation also bounds the number of concurrent spinning\nthreads and uses the schedctl facility to avoid spinning if the owner is not\niself running on a CPU. After releasing the lock in pthread mutex unlock,\nthe implementation checks if the queue is empty. If so, it returns immediately.\nOtherwise it waits brieÔ¨Çy to see if the lock happens to be acquired in the\ninterim by some other thread. If so, the caller can return without needing to\ndequeue and unpark an heir presumptive. The responsibility for succession\nand progress is delegated to the new owner. Such unpark avoidance reduces\nthe voluntary context switch park-unpark rate and reduces the latency of\nthe unlock operator. This defer-and-avoid strategy also tends to keep the\nACS stable. The policies of bounding the number of concurrent spinners and\nunpark avoidance act toward constraining the size of the ACS.\nThe\nimplementation\nalso\nprovides\nwait\nmorphing\n‚Äì\nif\na\npthread cond signal operation selects a thread that waits on a\nmutex held by the caller, then that thread is simply transferred from the\ncondition variable‚Äôs wait-set directly to the mutex‚Äôs queue of blocked threads,\navoiding the need to unpark the notifyee. This operation is fast, and reduces\nthe hold time when pthread cond signal is called within a critical\nsection. In addition, we avoid waking a thread while the lock that thread is\ntrying to acquire is already held by the caller, reducing futile and unnecessary\ncontention between the notiÔ¨Åer and notifyee. Morphing leverages the\nobservation that is is usually safe to shift a pthread cond signal call\nfrom within a critical section to immediately after the critical section.\nlock tends to Ô¨Ålter out threads from diÔ¨Äerent nodes, and the\nACS then tends to converge toward a set of threads located\non a given node. Decreased NUMA-diversity of the ACS\ndecreases lock migration rates and yields better throughput.\nA.2\nLIFO-CR\nThis design starts with a pure LIFO lock 41 with an explicit\nstack of waiting threads. Contended threads push an MCS-\nlike node onto the stack and then spin or spin-then-park on\na thread-local Ô¨Çag. These nodes can be allocated on stack.\nWhen threads are waiting, the unlock operator pops the head\nof stack ‚Äì the most recently arrived thread ‚Äì and directly\npasses ownership to that thread. (We also deÔ¨Åne a special\ndistinguished value for the stack pointer that indicates the\nlock is held and there are no waiters. 0 indicates that the\nlock is not held). Both ‚Äúpush‚Äù and ‚Äúpop‚Äù operations are\nimplemented via atomic compare-and-swap CAS instructions.\nOnly the lock holder can ‚Äúpop‚Äù elements, so the approach is\nimmune to ABA pathologies. The stack is multiple-producer\nbut, by virtue of the lock itself, single-consumer. The ACS\nconsists of the owner, the threads circulating through their\nrespective NCS regions, and the top of the stack. The PS\nconsists of the threads deeper on the stack. Admission order\nis eÔ¨Äectively cyclic round-robin over the members of the ACS,\nregardless of the prevailing LIFO lock admission policy. We\nthen augment the lock to periodically pick the tail of the stack\n‚Äì the eldest thread ‚Äì to be the next owner. This imposes long-\nterm fairness We refer to the resultant lock as LIFO-CR.\nLIFO admission order may improve temporal locality and\nreduce misses in shared caches. Both LIFO-CR and LOITER\noÔ¨Äer performance competitive with MCSCR.\nLIFO; MRA most-recently-arrived is warmest and thus fastest; Fastest Thread next admitted;\nIt is relatively simple to augment any given unfair lock so\nthat starving threads are periodically given a turn via direct\nhandoÔ¨Ä. The Solaris and windows schedulers employ similar\nanti-starvation policies. If threads languish too long on the run\nqueue because their eÔ¨Äective priority is too low, then they‚Äôll\nbe given transient priority boosts until they run. By analogy,\nthis policy can extend to locks, where waiting threads that\nlanguish too long can be explicitly granted ownership. This\nallows our locks to enjoy the beneÔ¨Åts of short-term unfairness\nbut explicitly manage long-term unfairness and to ensure\neventual progress.\nNormally the ‚Äúpop‚Äù operator would employ CAS in a\nloop. We can avoid the loop and, as an optional optimization,\nimplement a constant-time unlock operation by observing that\nif the CAS fails, then new threads have arrived and pushed\nthemselves onto the stack, and there are at least two elements\non the stack. We can thus implement a plausibly LIFO unlock\nby naively unlinking and passing ownership to the element\nthat follows the thread identiÔ¨Åed by the failed CAS return\nvalue.\nUnder LIFO-CR both arriving and departing (unlocking)\nthreads will update the the head of the stack, potentially cre-\nating an undesirable coherence hot-spot. MCSCR avoids this\nconcern. In practice, however, this does not seem to adversely\n41 If we use a pure LIFO lock then the LWSS should correspond to the\nACS size, giving an easy way to measure the ideally minimal ACS size and\nmaximum beneÔ¨Åt aÔ¨Äorded by CR.\n20\n2024-04-05",
        "semantic_similarity": 0.6059083640575409,
        "keyword_overlap": 0.016129032258064516,
        "combined_score": 0.42897456451769794,
        "paragraph_id": 20,
        "source_title": "Malthusian Locks"
      }
    ],
    "resolved_title": "Malthusian Locks"
  },
  {
    "citation_marker": "[5]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: YES. The statement accurately cites the authors (Dave Dice, Virendra J. Marathe, Nir Shavit), year (2011), title (\"Flat-combining NUMA locks\"), and conference (SPAA), which matches the content of the provided paragraph from the cited document. No misrepresentation, distortion, or fabrication is present."
    ],
    "evidence": [
      {
        "text_fragment": "Flat-Combining NUMA Locks\nDave Dice\nOracle Labs\ndave.dice@oracle.com\nVirendra J. Marathe\nOracle Labs\nvirendra.marathe@oracle.com\nNir Shavit\nOracle Labs\nnir.shavit@oracle.com\nABSTRACT\nMulticore machines are growing in size, and accordingly\nshifting from simple bus-based designs to NUMA and CC-\nNUMA architectures. With this shift, the need for scalable\nhierarchical locking algorithms is becoming crucial to per-\nformance.\nThis paper presents a novel scalable hierarchi-\ncal queue-lock algorithm based on the Ô¨Çat combining syn-\nchronization paradigm. At the core of the new algorithm is\na scheme for building local queues of waiting threads in a\nhighly eÔ¨Écient manner, and then merging them globally, all\nwith little interconnect traÔ¨Éc and virtually no costly syn-\nchronization operations in the common case. In empirical\ntesting on an Oracle SPARC Enterprise T5440 Server, a\n256-way CC-NUMA machine, our new Ô¨Çat-combining hier-\narchical lock signiÔ¨Åcantly outperforms all classic locking al-\ngorithms, and at high concurrency levels, provides up to a\nfactor of two improvement over HCLH, the most eÔ¨Écient\nknown hierarchical locking algorithm.\nCategories and Subject Descriptors\nD.1.3 [Programming Techniques]: Concurrent Program-\nming\nGeneral Terms\nAlgorithms, Design, Experimentation, Performance\nKeywords\nhierarchical locks, queue locks, Ô¨Çat combining\n1.\nINTRODUCTION\nQueue locks [1, 2, 3, 4], and in particular the CLH [2,\n3, 4] and MCS [3] locks, have long been the algorithms of\nchoice for locking in many high performance systems. They\nare known to reduce the overall cache coherence traÔ¨Éc by\nforming queues of threads, each spinning on a separate mem-\nory location as they await their turn to access the critical\nsection.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proÔ¨Åt or commercial advantage and that copies\nbear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc\npermission and/or a fee.\nSPAA‚Äô11, June 4‚Äì6, 2011, San Jose, California, USA.\nCopyright 2011 ACM 978-1-4503-0743-7/11/06 ...$10.00.\nCurrent trends in multicore architecture design imply that\nin coming years, there will be an accelerated shift towards\ndistributed nonuniform memory-access (NUMA) and cache-\ncoherent NUMA (CC-NUMA) architectures.\nSuch archi-\ntectures, examples of which include Intel‚Äôs 4 chip/32 way\nNehalem-based systems and Oracle‚Äôs 4 chip/256 way Niagara-\nbased systems, consist of collections of computing cores with\nfast local memory (as found on a single multicore chip), com-\nmunicating with each other via a slower (inter-chip) commu-\nnication medium. Access by a core to the local memory, and\nin particular to a shared local cache, can be several times\nfaster than access to the remote memory located on another\nchip [5].\nRadovi¬¥c and Hagersten [5] were the Ô¨Årst to show the ben-\neÔ¨Åts of designing locks that improve locality of reference on\nCC-NUMA architectures by developing hierarchical locks:\ngeneral-purpose mutual-exclusion locks that encourage thr-\neads with high mutual memory locality to acquire the lock\nconsecutively, thus reducing the overall level of cache misses\nwhen executing instructions in the critical section.\nRadovi¬¥c and Hagersten introduced the hierarchical back-\noÔ¨Älock (HBO): a test-and-test-and-set lock augmented with\na new backoÔ¨Äscheme to reduce contention on the lock vari-\nable. Their hierarchical backoÔ¨Ämechanism allows the back-\noÔ¨Ädelay to be tuned dynamically, so that when a thread\nnotices that another thread from its own local cluster owns\nthe lock, it can reduce its delay and increase its chances\nof acquiring the lock consecutively. However, because the\nlocks are test-and-test-and-set locks, they incur invalidation\ntraÔ¨Éc on every modiÔ¨Åcation of the shared global lock vari-\nable, which is especially costly on NUMA machines. In their\nwork [5], Radovi¬¥c and Hagersten did introduce a heuristic\ntechnique to throttle inter-chip coherence traÔ¨Éc. However,\nas we show in our evalution (Section 3), it does not nec-\nessarily translate to better scalability.\nMoreover, the dy-\nnamic adjustment of backoÔ¨Ädelay time in the lock intro-\nduces signiÔ¨Åcant starvation and fairness issues: it becomes\nlikely that two or more threads from the same cluster will\nrepeatedly acquire a lock while threads from other clusters\nstarve.\nRadovi¬¥c and Hagersten also introduced a heuris-\ntic to improve fairness, but this requires Ô¨Åne tuning of the\nbackoÔ¨Äparameters, which can change with the underlying\napplication‚Äôs characteristics.\nLuchangco et al. [6] overcome these drawbacks by intro-\nducing a hierarchical version of the CLH queue-locking al-\ngorithm (HCLH). Their HCLH algorithm collects requests\non each chip into a local CLH style queue, and then has\nthe thread at the head of the queue integrate each chip‚Äôs\n65",
        "semantic_similarity": 0.7032304108142853,
        "keyword_overlap": 0.020352250489236792,
        "combined_score": 0.49836696271677067,
        "paragraph_id": 0,
        "source_title": "Flat-combining NUMA locks"
      }
    ],
    "resolved_title": "Flat-combining NUMA locks"
  },
  {
    "citation_marker": "[6]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: YES. The article states that scheduling failures due to locks (e.g., priority inversion) are reported, citing [6]. The cited paragraph from [6] explicitly identifies \"priority inversion\" as a problem of locks that transactional memory aims to avoid, directly supporting the statement. No misrepresentation or fabrication is present."
    ],
    "evidence": [
      {
        "text_fragment": "Figure 5: Producer/Consumer Benchmark: Bus and Network\natomic operations on multiple locations. For example, the\nMotorola 68000 provides a COMPARE&SWAP2 that operates\non two independent locations. Massalin and Pu [25] use\nthis instruction for lock-free list manipulation in an oper-\nating system kernel. Transactional memory provides more\npowerfulsupportfor this‚Äúlock-free‚Äù style of programming.\nOther work that uses after-the-fact conÔ¨Çict detection to\nrecognize violations of desired correctness conditions in-\nclude Gharachorloo and Gibbons [11], who propose an\nimplementation of release consistency that exploits an un-\nderlying invalidation-based cache protocol to detect viola-\ntions of sequential consistency, and Franklin and Sohi [10],\nwho propose a hardware architecture that optimistically\nparallelizes sequential code at runtime.\nOther researchers who have investigated architectural\nsupport for multi-word synchronization include Knight\n[23], who suggests using cache coherence protocols to add\nparallelism to ‚Äúmostly functional‚Äù LISP programs, and the\nIBM 801 [7], which provides support for database-style\nlocking in hardware. Note that despite superÔ¨Åcial similar-\nities in terminology, the synchronization mechanisms pro-\nvided by transactional memory and by the 801 are intended\nfor entirely different purposes, and use entirely different\ntechniques.\nOur approach to performance issues has been heavily\ninÔ¨Çuenced by recent work on locking in multiprocessors,\nincluding work of Anderson [3], Bershad [4], Graunke and\nThakkar [17], and Mellor-Crummey and Scott [27].\n7\nConclusions\nThe primary goal of transactional memory is to make it\neasier to perform general atomic updates of multiple in-\ndependent memory words, avoiding the problems of locks\n(priorityinversion,convoying,anddeadlock). We sketched\nhow it can be implemented by adding new instructions\nto the processor, adding a small auxiliary, transactional\ncache (without disturbing the regular cache), and making\nstraightforward changes to the cache coherence protocol.\nWe investigated transactional memory for its added func-\ntionality, but our simulations showed that it outperforms\nother techniques for atomic updates. This is primarily be-\ncause transactional memory uses no explicit locks and thus\nperforms fewer shared memory accesses. Since transac-\ntional memory offers both improved functionality and bet-\nter performance, it shouldbe considered in future processor\narchitectures.\nPage 10",
        "semantic_similarity": 0.7296445965766907,
        "keyword_overlap": 0.039045553145336226,
        "combined_score": 0.5224648835472844,
        "paragraph_id": 11,
        "source_title": "Transactional Memory: Architectural Support for Lock-Free Data Structures"
      }
    ],
    "resolved_title": "Transactional Memory: Architectural Support for Lock-Free Data Structures"
  },
  {
    "citation_marker": "[7]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.05, overlap: 0.00): NO. The statement lists authors following \"[7]\", but in the cited paragraph, [7] refers to a citation for the \"libprocess\" library (\"we use a C++ library called libprocess [7]\"), not an author list. The statement misrepresents the content of the citation [7] in the document.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.03, overlap: 0.00): NO. The statement lists authors (Benjamin Hindman, Andy Konwinski, Matei Zaharia, Ali Ghodsi, Anthony D.), but the cited paragraph from the \"Mesos\" document does not contain any author information. The content provided focuses on technical details (e.g., Spark vs. Dryad, evaluation experiments) and does not support the author list in the statement.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.02, overlap: 0.00): NO. The statement is an incomplete list of authors, but the cited paragraph from the Mesos paper does not mention any authors. The paragraph discusses technical content (API functions, fault tolerance, Mesos behavior) and does not include author information, so the statement is not supported by the cited content.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "isting frameworks are online algorithms, because frame-\nworks cannot predict task times and must be able to han-\ndle failures and stragglers [18, 40, 38]. These policies\nare easy to implement over resource offers.\n5\nImplementation\nWe have implemented Mesos in about 10,000 lines of\nC++. The system runs on Linux, Solaris and OS X, and\nsupports frameworks written in C++, Java, and Python.\nTo reduce the complexity of our implementation, we\nuse a C++ library called libprocess [7] that provides\nan actor-based programming model using efÔ¨Åcient asyn-\nchronous I/O mechanisms (epoll, kqueue, etc). We\nalso use ZooKeeper [4] to perform leader election.\nMesos can use Linux containers [9] or Solaris projects\n[13] to isolate tasks. We currently isolate CPU cores and\nmemory. We plan to leverage recently added support for\nnetwork and I/O isolation in Linux [8] in the future.\nWe have implemented four frameworks on top of\nMesos. First, we have ported three existing cluster com-\nputing systems: Hadoop [2], the Torque resource sched-\nuler [33], and the MPICH2 implementation of MPI [16].\nNone of these ports required changing these frameworks‚Äô\nAPIs, so all of them can run unmodiÔ¨Åed user programs.\nIn addition, we built a specialized framework for iterative\njobs called Spark, which we discuss in Section 5.3.\n5.1\nHadoop Port\nPorting Hadoop to run on Mesos required relatively few\nmodiÔ¨Åcations, because Hadoop‚Äôs Ô¨Åne-grained map and\nreduce tasks map cleanly to Mesos tasks. In addition, the\nHadoop master, known as the JobTracker, and Hadoop\nslaves, known as TaskTrackers, Ô¨Åt naturally into the\nMesos model as a framework scheduler and executor.\nTo add support for running Hadoop on Mesos, we took\nadvantage of the fact that Hadoop already has a plug-\ngable API for writing job schedulers. We wrote a Hadoop\nscheduler that connects to Mesos, launches TaskTrackers\nas its executors, and maps each Hadoop task to a Mesos\ntask. When there are unlaunched tasks in Hadoop, our\nscheduler Ô¨Årst starts Mesos tasks on the nodes of the\ncluster that it wants to use, and then sends the Hadoop\ntasks to them using Hadoop‚Äôs existing internal interfaces.\nWhen tasks Ô¨Ånish, our executor notiÔ¨Åes Mesos by listen-\ning for task Ô¨Ånish events using an API in the TaskTracker.\nWe used delay scheduling [38] to achieve data locality\nby waiting for slots on the nodes that contain task in-\nput data. In addition, our approach allowed us to reuse\nHadoop‚Äôs existing logic for re-scheduling of failed tasks\nand for speculative execution (straggler mitigation).\nWe also needed to change how map output data is\nserved to reduce tasks.\nHadoop normally writes map\noutput Ô¨Åles to the local Ô¨Ålesystem, then serves these to\nreduce tasks using an HTTP server included in the Task-\nTracker. However, the TaskTracker within Mesos runs\nas an executor, which may be terminated if it is not run-\nning tasks. This would make map output Ô¨Åles unavailable\nto reduce tasks. We solved this problem by providing a\nshared Ô¨Åle server on each node in the cluster to serve\nlocal Ô¨Åles. Such a service is useful beyond Hadoop, to\nother frameworks that write data locally on each node.\nIn total, our Hadoop port is 1500 lines of code.\n5.2\nTorque and MPI Ports\nWe have ported the Torque cluster resource manager to\nrun as a framework on Mesos. The framework consists\nof a Mesos scheduler and executor, written in 360 lines\nof Python code, that launch and manage different com-\nponents of Torque. In addition, we modiÔ¨Åed 3 lines of\nTorque source code to allow it to elastically scale up and\ndown on Mesos depending on the jobs in its queue.\nAfter registering with the Mesos master, the frame-\nwork scheduler conÔ¨Ågures and launches a Torque server\nand then periodically monitors the server‚Äôs job queue.\nWhile the queue is empty, the scheduler releases all tasks\n(down to an optional minimum, which we set to 0) and\nrefuses all resource offers it receives from Mesos. Once\na job gets added to Torque‚Äôs queue (using the standard\nqsub command), the scheduler begins accepting new\nresource offers. As long as there are jobs in Torque‚Äôs\nqueue, the scheduler accepts offers as necessary to sat-\nisfy the constraints of as many jobs in the queue as pos-\nsible. On each node where offers are accepted, Mesos\nlaunches our executor, which in turn starts a Torque\nbackend daemon and registers it with the Torque server.\nWhen enough Torque backend daemons have registered,\nthe torque server will launch the next job in its queue.\nBecause jobs that run on Torque (e.g. MPI) may not be\nfault tolerant, Torque avoids having its tasks revoked by\nnot accepting resources beyond its guaranteed allocation.\nIn addition to the Torque framework, we also created\na Mesos MPI ‚Äúwrapper‚Äù framework, written in 200 lines\nof Python code, for running MPI jobs directly on Mesos.\n5.3\nSpark Framework\nMesos enables the creation of specialized frameworks\noptimized for workloads for which more general exe-\ncution layers may not be optimal. To test the hypoth-\nesis that simple specialized frameworks provide value,\nwe identiÔ¨Åed one class of jobs that were found to per-\nform poorly on Hadoop by machine learning researchers\nat our lab: iterative jobs, where a dataset is reused across\na number of iterations. We built a specialized framework\ncalled Spark [39] optimized for these workloads.\nOne example of an iterative algorithm used in ma-\nchine learning is logistic regression [22]. This algorithm\nseeks to Ô¨Ånd a line that separates two sets of labeled data\npoints. The algorithm starts with a random line w. Then,\n8",
        "semantic_similarity": 0.04597640037536621,
        "keyword_overlap": 0.0003506311360448808,
        "combined_score": 0.03228866960356981,
        "paragraph_id": 9,
        "source_title": "Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center"
      },
      {
        "text_fragment": ". . . \nw \nf(x,w) \nw \nf(x,w) \nx \nx \na) Dryad \nb) Spark \nw \nf(x,w) \nx \nFigure 4: Data Ô¨Çow of a logistic regression job in Dryad\nvs. Spark. Solid lines show data Ô¨Çow within the framework.\nDashed lines show reads from a distributed Ô¨Åle system. Spark\nreuses in-memory data across iterations to improve efÔ¨Åciency.\non each iteration, it computes the gradient of an objective\nfunction that measures how well the line separates the\npoints, and shifts w along this gradient. This gradient\ncomputation amounts to evaluating a function f(x, w)\nover each data point x and summing the results.\nAn\nimplementation of logistic regression in Hadoop must\nrun each iteration as a separate MapReduce job, because\neach iteration depends on the w computed at the previous\none. This imposes overhead because every iteration must\nre-read the input Ô¨Åle into memory. In Dryad, the whole\njob can be expressed as a data Ô¨Çow DAG as shown in Fig-\nure 4a, but the data must still must be reloaded from disk\nat each iteration. Reusing the data in memory between\niterations in Dryad would require cyclic data Ô¨Çow.\nSpark‚Äôs execution is shown in Figure 4b. Spark uses\nthe long-lived nature of Mesos executors to cache a slice\nof the dataset in memory at each executor, and then run\nmultiple iterations on this cached data. This caching is\nachieved in a fault-tolerant manner: if a node is lost,\nSpark remembers how to recompute its slice of the data.\nBy building Spark on top of Mesos, we were able to\nkeep its implementation small (about 1300 lines of code),\nyet still capable of outperforming Hadoop by 10√ó for\niterative jobs. In particular, using Mesos‚Äôs API saved us\nthe time to write a master daemon, slave daemon, and\ncommunication protocols between them for Spark. The\nmain pieces we had to write were a framework scheduler\n(which uses delay scheduling for locality) and user APIs.\n6\nEvaluation\nWe evaluated Mesos through a series of experiments on\nthe Amazon Elastic Compute Cloud (EC2). We begin\nwith a macrobenchmark that evaluates how the system\nshares resources between four workloads, and go on to\npresent a series of smaller experiments designed to eval-\nuate overhead, decentralized scheduling, our specialized\nframework (Spark), scalability, and failure recovery.\nBin\nJob Type\nMap Tasks\nReduce Tasks\n# Jobs Run\n1\nselection\n1\nNA\n38\n2\ntext search\n2\nNA\n18\n3\naggregation\n10\n2\n14\n4\nselection\n50\nNA\n12\n5\naggregation\n100\n10\n6\n6\nselection\n200\nNA\n6\n7\ntext search\n400\nNA\n4\n8\njoin\n400\n30\n2\nTable 3: Job types for each bin in our Facebook Hadoop mix.\n6.1\nMacrobenchmark\nTo evaluate the primary goal of Mesos, which is enabling\ndiverse frameworks to efÔ¨Åciently share a cluster, we ran a\nmacrobenchmark consisting of a mix of four workloads:\n‚Ä¢ A Hadoop instance running a mix of small and large\njobs based on the workload at Facebook.\n‚Ä¢ A Hadoop instance running a set of large batch jobs.\n‚Ä¢ Spark running a series of machine learning jobs.\n‚Ä¢ Torque running a series of MPI jobs.\nWe compared a scenario where the workloads ran as\nfour frameworks on a 96-node Mesos cluster using fair\nsharing to a scenario where they were each given a static\npartition of the cluster (24 nodes), and measured job re-\nsponse times and resource utilization in both cases. We\nused EC2 nodes with 4 CPU cores and 15 GB of RAM.\nWe begin by describing the four workloads in more\ndetail, and then present our results.\n6.1.1\nMacrobenchmark Workloads\nFacebook Hadoop Mix\nOur Hadoop job mix was\nbased on the distribution of job sizes and inter-arrival\ntimes at Facebook, reported in [38]. The workload con-\nsists of 100 jobs submitted at Ô¨Åxed times over a 25-\nminute period, with a mean inter-arrival time of 14s.\nMost of the jobs are small (1-12 tasks), but there are also\nlarge jobs of up to 400 tasks.4 The jobs themselves were\nfrom the Hive benchmark [6], which contains four types\nof queries: text search, a simple selection, an aggrega-\ntion, and a join that gets translated into multiple MapRe-\nduce steps. We grouped the jobs into eight bins of job\ntype and size (listed in Table 3) so that we could com-\npare performance in each bin. We also set the framework\nscheduler to perform fair sharing between its jobs, as this\npolicy is used at Facebook.\nLarge Hadoop Mix\nTo emulate batch workloads that\nneed to run continuously, such as web crawling, we had\na second instance of Hadoop run a series of IO-intensive\n2400-task text search jobs. A script launched ten of these\njobs, submitting each one after the previous one Ô¨Ånished.\n4We scaled down the largest jobs in [38] to have the workload Ô¨Åt a\nquarter of our cluster size.\n9",
        "semantic_similarity": 0.0288238525390625,
        "keyword_overlap": 0.0004012841091492777,
        "combined_score": 0.02029708201008853,
        "paragraph_id": 10,
        "source_title": "Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center"
      },
      {
        "text_fragment": "Scheduler Actions \nreplyToOffer(offerId, tasks) \nsetNeedsOffers(bool) \nsetFilters(filters) \ngetGuaranteedShare() \nkillTask(taskId) \nTable 1: Mesos API functions for schedulers and executors.\n3.6\nFault Tolerance\nSince all the frameworks depend on the Mesos master, it\nis critical to make the master fault-tolerant. To achieve\nthis, we have designed the master to be soft state, so that\na new master can completely reconstruct its internal state\nfrom information held by the slaves and the framework\nschedulers. In particular, the master‚Äôs only state is the list\nof active slaves, active frameworks, and running tasks.\nThis information is sufÔ¨Åcient to compute how many re-\nsources each framework is using and run the allocation\npolicy. We run multiple masters in a hot-standby conÔ¨Åg-\nuration using ZooKeeper [4] for leader election. When\nthe active master fails, the slaves and schedulers connect\nto the next elected master and repopulate its state.\nAside from handling master failures, Mesos reports\nnode failures and executor crashes to frameworks‚Äô sched-\nulers. Frameworks can then react to these failures using\nthe policies of their choice.\nFinally, to deal with scheduler failures, Mesos allows a\nframework to register multiple schedulers such that when\none fails, another one is notiÔ¨Åed by the Mesos master to\ntake over. Frameworks must use their own mechanisms\nto share state between their schedulers.\n3.7\nAPI Summary\nTable 1 summarizes the Mesos API. The ‚Äúcallback‚Äù\ncolumns list functions that frameworks must implement,\nwhile ‚Äúactions‚Äù are operations that they can invoke.\n4\nMesos Behavior\nIn this section, we study Mesos‚Äôs behavior for different\nworkloads. Our goal is not to develop an exact model of\nthe system, but to provide a coarse understanding of its\nbehavior, in order to characterize the environments that\nMesos‚Äôs distributed scheduling model works well in.\nIn short, we Ô¨Ånd that Mesos performs very well when\nframeworks can scale up and down elastically, tasks\ndurations are homogeneous, and frameworks prefer all\nnodes equally (¬ß4.2). When different frameworks pre-\nfer different nodes, we show that Mesos can emulate a\ncentralized scheduler that performs fair sharing across\nframeworks (¬ß4.3). In addition, we show that Mesos can\nhandle heterogeneous task durations without impacting\nthe performance of frameworks with short tasks (¬ß4.4).\nWe also discuss how frameworks are incentivized to im-\nprove their performance under Mesos, and argue that\nthese incentives also improve overall cluster utilization\n(¬ß4.5). We conclude this section with some limitations\nof Mesos‚Äôs distributed scheduling model (¬ß4.6).\n4.1\nDeÔ¨Ånitions, Metrics and Assumptions\nIn our discussion, we consider three metrics:\n‚Ä¢ Framework ramp-up time:\ntime it takes a new\nframework to achieve its allocation (e.g., fair share);\n‚Ä¢ Job completion time: time it takes a job to complete,\nassuming one job per framework;\n‚Ä¢ System utilization: total cluster utilization.\nWe characterize workloads along two dimensions: elas-\nticity and task duration distribution. An elastic frame-\nwork, such as Hadoop and Dryad, can scale its resources\nup and down, i.e., it can start using nodes as soon as it\nacquires them and release them as soon its task Ô¨Ånish. In\ncontrast, a rigid framework, such as MPI, can start run-\nning its jobs only after it has acquired a Ô¨Åxed quantity of\nresources, and cannot scale up dynamically to take ad-\nvantage of new resources or scale down without a large\nimpact on performance. For task durations, we consider\nboth homogeneous and heterogeneous distributions.\nWe also differentiate between two types of resources:\nmandatory and preferred. A resource is mandatory if a\nframework must acquire it in order to run. For example, a\ngraphical processing unit (GPU) is mandatory if a frame-\nwork cannot run without access to GPU. In contrast, a re-\nsource is preferred if a framework performs ‚Äúbetter‚Äù us-\ning it, but can also run using another equivalent resource.\nFor example, a framework may prefer running on a node\nthat locally stores its data, but may also be able to read\nthe data remotely if it must.\nWe assume the amount of mandatory resources re-\nquested by a framework never exceeds its guaranteed\nshare. This ensures that frameworks will not deadlock\nwaiting for the mandatory resources to become free.2 For\nsimplicity, we also assume that all tasks have the same re-\nsource demands and run on identical slices of machines\ncalled slots, and that each framework runs a single job.\n4.2\nHomogeneous Tasks\nWe consider a cluster with n slots and a framework, f,\nthat is entitled to k slots. For the purpose of this analy-\nsis, we consider two distributions of the task durations:\nconstant (i.e., all tasks have the same length) and expo-\nnential. Let the mean task duration be T, and assume that\n2In workloads where the mandatory resource demands of the ac-\ntive frameworks can exceed the capacity of the cluster, the allocation\nmodule needs to implement admission control.\n5",
        "semantic_similarity": 0.01531982421875,
        "keyword_overlap": 0.00040617384240454913,
        "combined_score": 0.010845729105846365,
        "paragraph_id": 6,
        "source_title": "Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center"
      }
    ],
    "resolved_title": "Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center"
  },
  {
    "citation_marker": "[8]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.45, overlap: 0.04): NO. The original article cites [8] regarding \"performance interference/degradation\", \"cross-directory rename benchmark\", and lock scheduling (spinlock/mutex usage), but the provided paragraph from [8] focuses on HugeGPT's address translation acceleration via host huge pages, discussing tail latencies, throughputs, page sizes, and overhead‚Äîwith no mention of lock scheduling, cross-rename benchmarks, or spinlock/mutex issues. The cited content does not support the article's claims about [8].",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.43, overlap: 0.02): NO. The article cites [8] to support claims about lock scheduling, cross-rename benchmarks, and spinlock/mutex usage. However, the cited paragraph's [8] refers to \"Smartmd: A high performance deduplication engine with mixed pages,\" which discusses data deduplication, not lock scheduling or cross-rename benchmarks. This misrepresents the content of the cited reference.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.42, overlap: 0.04): NO. The cited paragraph from [8] discusses HugeGPT's impact on address translation latency, workload performance (e.g., Redis, Memcached), and page walk overhead, with no mention of cross-rename benchmarks, mutexes, spinlocks, or SCL. The article's statement references [8] for a cross-rename benchmark and lock modifications, which are absent in the cited content, thus misrepresenting the cited material.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\nSphinx\nSpecjbb\nSpecjbb\nShore\nSphinx\nShore\nNormalized\nTail Latency (95th)\nHugeGPT\nFig. 15: 95th percentile tail latencies of latency sensitive workloads when\nthey are colocated on the same server. Tail latencies are normalized to\nvanilla Linux/KVM.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\nSphinx\nSpecjbb\nSpecjbb\nShore\nSphinx\nShore\nNormalized\nTail Latency (99th)\nHugeGPT\nFig. 16: 99th percentile tail latencies of latency sensitive workloads when\nthey are colocated on the same server. Tail latencies are normalized to\nvanilla Linux/KVM.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\nCanneal\nGUPS\nBTree\nNormalized Throughput\nHugeGPT\nFig. 17: Throughputs of six throughput oriented workloads colocated on\nthe same server. We run two copies of each workload. Throughputs are\nnormalized to vanilla Linux/KVM.\nin comparison to vanilla Linux/KVM. This shows HUGEGPT\ncan reduce average and tail latencies when latency sensitive\nworkloads are colocated on the same server.\nFigure 17 shows HUGEGPT‚Äôs throughput when six work-\nloads are colocated on the same server. We run two copies\nof each workload (Canneal, GUPS, and BTree). Since copies\nof the same workload have similar throughput, we plot the\naverage throughput for the copies of each workload. The VM\nrunning each workload has 12 vCPUs and 40GB memory. The\nworking set size of each workload is kept around 35GB. This\nprevents the total workload working set size from exceeding\nthe server‚Äôs memory capacity. On average, HUGEGPT outper-\nforms vanilla Linux/KVM by 13%. This is consistent with the\ntest results when two workloads are consolidated on the same\nserver, as shown in Figure 13.\nFigure 18 shows HUGEGPT‚Äôs throughput for different page\nsizes. We run XSBench to test HUGEGPT‚Äôs throughput. We\nchoose 4KB, 2MB, and 1GB memory page sizes because\ncurrent x86 CPU only supports those page sizes. As the page\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\n4KB\n2MB\n1GB\nNormalized Throughput\nHugeGPT\nFig. 18: HUGEGPT‚Äôs throughputs with different memory page sizes.\nThroughputs are normalized to vanilla Linux/KVM.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\nSwaptions\nRaytrace\nNormalized Throughput\nHugeGPT\nFig. 19: HUGEGPT‚Äôs overhead. Swaptions and Raytrace are page\nwalk non-intensive workloads. Throughputs are normalized to vanilla\nLinux/KVM.\nsize increases from 4KB to 1GB, HUGEGPT‚Äôs throughput\nimprovement relative to vanilla Linux/KVM degrades from\n15% to 4%. This is because huge pages (e.g., 1GB) can shorten\npage table walk. For instance, the page table for 1GB huge\npages does not need the last two levels that are present in page\ntables for 4KB pages. As a result, HUGEGPT cannot obtain\nmore benefits when the page size becomes very large. On the\nother hand, 1GB huge pages are not widely used as they incur\nlarge overhead such as memory fragmentation and CPU waste\nfor defragmentation [6].\nTo evaluate HUGEGPT‚Äôs overhead, we test the performance\nof HUGEGPT and vanilla Linux/KVM with two page walk\nnon-intensive workloads, i.e., Swaptions and Raytrace. We\nshow the performance results in Figure 19. When the workload\nis page walk non-intensive, there is almost no space for\nHUGEGPT to improve application performance compared to\nvanilla Linux/KVM, and the performance difference between\nHUGEGPT and vanilla Linux/KVM shows HUGEGPT‚Äôs over-\nhead. Figure 19 shows that HUGEGPT does not introduce\nmuch performance overhead (3% on average). HUGEGPT may\nintroduce overhead as it needs to identify guest page table\npages and allocate huge pages in the host OS.\nVII. Discussion\nLive Migration. HUGEGPT can support live migration and\nrestore from a snapshot. It needs the destination host OS to\nconduct system initialization as described in ¬ßV-A.\nMemory Consumption. HUGEGPT consumes negligible ex-\ntra memory space to store page table data compared to vanilla\nLinux/KVM. In our evaluation, for 100GB of application data,\nvanilla Linux/KVM needs around 217MB of memory space to\nstore page table data, and HUGEGPT needs around 221MB.",
        "semantic_similarity": 0.4523983597755432,
        "keyword_overlap": 0.044290657439446365,
        "combined_score": 0.32996604907471416,
        "paragraph_id": 8,
        "source_title": "Pages to Accelerate Address Translation"
      },
      {
        "text_fragment": "References\n[1] A. Margaritov, D. Ustiugov, E. Bugnion, and B. Grot, ‚ÄúPrefetched\naddress translation,‚Äù in Proceedings of the 52nd Annual IEEE/ACM\nInternational Symposium on Microarchitecture, 2019, pp. 1023‚Äì1036.\n[2] M. Ferdman, A. Adileh, O. Kocberber, S. Volos, M. Alisafaee, D. Jevd-\njic, C. Kaynak, A. D. Popescu, A. Ailamaki, and B. Falsafi, ‚ÄúClearing the\nclouds: a study of emerging scale-out workloads on modern hardware,‚Äù\nAcm sigplan notices, vol. 47, no. 4, pp. 37‚Äì48, 2012.\n[3] J. Navarro, S. Iyer, P. Druschel, and A. Cox, ‚ÄúPractical, transparent\noperating system support for superpages,‚Äù ACM SIGOPS Operating\nSystems Review, vol. 36, no. SI, pp. 89‚Äì104, 2002.\n[4] R. Achermann, A. Panwar, A. Bhattacharjee, T. Roscoe, and J. Gandhi,\n‚ÄúMitosis: Transparently self-replicating page-tables for large-memory\nmachines,‚Äù in Proceedings of the Twenty-Fifth International Conference\non Architectural Support for Programming Languages and Operating\nSystems, 2020, pp. 283‚Äì300.\n[5] A. Panwar, R. Achermann, A. Basu, A. Bhattacharjee, K. Gopinath,\nand J. Gandhi, ‚ÄúFast local page-tables for virtualized numa servers with\nvmitosis,‚Äù in Proceedings of the Twenty-Sixth International Conference\non Architectural Support for Programming Languages and Operating\nSystems, 2021.\n[6] A. Panwar, S. Bansal, and K. Gopinath, ‚ÄúHawkeye: Efficient fine-\ngrained os support for huge pages,‚Äù in Proceedings of the Twenty-Fourth\nInternational Conference on Architectural Support for Programming\nLanguages and Operating Systems, 2019, pp. 347‚Äì360.\n[7] F. Guo, S. Kim, Y. Baskakov, and I. Banerjee, ‚ÄúProactively break-\ning large pages to improve memory overcommitment performance in\nvmware esxi,‚Äù in Proceedings of the 11th ACM SIGPLAN/SIGOPS\nInternational Conference on Virtual Execution Environments, 2015, pp.\n39‚Äì51.\n[8] F. Guo, Y. Li, Y. Xu, S. Jiang, and J. C. Lui, ‚ÄúSmartmd: A high per-\nformance deduplication engine with mixed pages,‚Äù in 2017 {USENIX}\nAnnual Technical Conference ({USENIX}{ATC} 17), 2017, pp. 733‚Äì\n744.\n[9] A. Margaritov, D. Ustiugov, A. Shahab, and B. Grot, ‚ÄúPtemagnet: Fine-\ngrained physical memory reservation for faster page walks in public\nclouds,‚Äù in The 26th International Conference on Architectural Support\nfor Programming Languages and Operating Systems, ASPLOS 2021,\n2021.\n[10] T. Merrifield and H. R. Taheri, ‚ÄúPerformance implications of extended\npage tables on virtualized x86 processors,‚Äù in Proceedings of the12th\nACM SIGPLAN/SIGOPS International Conference on Virtual Execution\nEnvironments, 2016, pp. 25‚Äì35.\n[11] B. Pham, J. Vesel`y, G. H. Loh, and A. Bhattacharjee, ‚ÄúLarge pages and\nlightweight memory management in virtualized environments: Can you\nhave it both ways?‚Äù in Proceedings of the 48th International Symposium\non Microarchitecture, 2015, pp. 1‚Äì12.\n[12] B. Pham, J. Vesely, G. H. Loh, and A. Bhattacharjee, ‚ÄúUsing tlb\nspeculation to overcome page splintering in virtual machines,‚Äù 2015.\n[13] A. Panwar, A. Prasad, and K. Gopinath, ‚ÄúMaking huge pages actually\nuseful,‚Äù in Proceedings of the Twenty-Third International Conference\non Architectural Support for Programming Languages and Operating\nSystems, 2018, pp. 679‚Äì692.\n[14] W. Zhu, A. L. Cox, and S. Rixner, ‚ÄúA comprehensive analysis of\nsuperpage management mechanisms and policies,‚Äù in 2020 {USENIX}\nAnnual Technical Conference ({USENIX}{ATC} 20), 2020, pp. 829‚Äì\n842.\n[15] R. Kadekodi, S. Kadekodi, S. Ponnapalli, H. Shirwadkar, G. R. Ganger,\nA. Kolli, and V. Chidambaram, ‚ÄúWinefs: a hugepage-aware file system\nfor persistent memory that ages gracefully,‚Äù in Proceedings of the ACM\nSIGOPS 28th Symposium on Operating Systems Principles CD-ROM,\n2021, pp. 804‚Äì818.\n[16] A. Hunter, C. Kennelly, P. Turner, D. Gove, T. Moseley, and P. Ran-\nganathan, ‚ÄúBeyond malloc efficiency to fleet efficiency: a hugepage-\naware memory allocator,‚Äù in 15th {USENIX} Symposium on Operating\nSystems Design and Implementation ({OSDI} 21), 2021, pp. 257‚Äì273.\n[17] M. Maas, C. Kennelly, K. Nguyen, D. Gove, K. S. McKinley, and\nP. Turner, ‚ÄúAdaptive huge-page subrelease for non-moving memory\nallocators in warehouse-scale computers,‚Äù in Proceedings of the 2021\nACM SIGPLAN International Symposium on Memory Management,\n2021, pp. 28‚Äì38.\n[18] J. Gandhi, M. D. Hill, and M. M. Swift, ‚ÄúAgile paging: Exceeding the\nbest of nested and shadow paging,‚Äù in 2016 ACM/IEEE 43rd Annual\nInternational Symposium on Computer Architecture (ISCA).\nIEEE,\n2016, pp. 707‚Äì718.\n[19] J. Stojkovic, D. Skarlatos, A. Kokolis, T. Xu, and J. Torrellas, ‚ÄúParallel\nvirtualized memory translation with nested elastic cuckoo page tables,‚Äù\nin Proceedings of the 27th ACM International Conference on Archi-\ntectural Support for Programming Languages and Operating Systems,\n2022, pp. 84‚Äì97.\n[20] ‚ÄúIntel\n64\nand\nia-32\narchitectures\ndeveloper‚Äôs\nmanual,‚Äù\nhttps://www.intel.com/content/www/us/en/architecture-and-technology/\n64-ia-32-architectures-software-developer-manual-325462.html.\n[21] ‚ÄúAmd64 architecture programmer‚Äôs manual,‚Äù https://developer.amd.com/\nresources/developer-guides-manuals/.\n[22] ‚ÄúIntel five level paging,‚Äù https://en.wikipedia.org/wiki/Intel 5-level\npaging.\n[23] ‚ÄúFive\nlevel\npaging\nand\nfive\nlevel\nEPT\nwhite\npaper,‚Äù\nhttps://software.intel.com/content/www/us/en/develop/download/\n5-level-paging-and-5-level-ept-white-paper.html.\n[24] J. H. Ryoo, N. Gulur, S. Song, and L. K. John, ‚ÄúRethinking tlb designs\nin virtualized environments: A very large part-of-memory tlb,‚Äù ACM\nSIGARCH Computer Architecture News, vol. 45, no. 2, pp. 469‚Äì480,\n2017.\n[25] V. Karakostas, J. Gandhi, F. Ayar, A. Cristal, M. D. Hill, K. S. McKinley,\nM. Nemirovsky, M. M. Swift, and O. ¬®Unsal, ‚ÄúRedundant memory\nmappings for fast access to large memories,‚Äù ACM SIGARCH Computer\nArchitecture News, vol. 43, no. 3S, pp. 66‚Äì78, 2015.\n[26] S. Gupta, A. Bhattacharyya, Y. Oh, A. Bhattacharjee, B. Falsafi,\nand M. Payer, ‚ÄúRebooting virtual memory with midgard,‚Äù in 2021\nACM/IEEE 48th Annual International Symposium on Computer Archi-\ntecture (ISCA).\nIEEE, 2021, pp. 512‚Äì525.\n[27] B. Pham, V. Vaidyanathan, A. Jaleel, and A. Bhattacharjee, ‚ÄúColt: Coa-\nlesced large-reach tlbs,‚Äù in 2012 45th Annual IEEE/ACM International\nSymposium on Microarchitecture.\nIEEE, 2012, pp. 258‚Äì269.\n[28] B. Pham, A. Bhattacharjee, Y. Eckert, and G. H. Loh, ‚ÄúIncreasing tlb\nreach by exploiting clustering in page translations,‚Äù in 2014 IEEE 20th\nInternational Symposium on High Performance Computer Architecture\n(HPCA).\nIEEE, 2014, pp. 558‚Äì567.\n[29] C. H. Park, T. Heo, J. Jeong, and J. Huh, ‚ÄúHybrid tlb coalescing:\nImproving tlb translation coverage under diverse fragmented memory\nallocations,‚Äù in Proceedings of the 44th Annual International Symposium\non Computer Architecture, 2017, pp. 444‚Äì456.\n[30] T. W. Barr, A. L. Cox, and S. Rixner, ‚ÄúTranslation caching: skip, don‚Äôt\nwalk (the page table),‚Äù ACM SIGARCH Computer Architecture News,\nvol. 38, no. 3, pp. 48‚Äì59, 2010.\n[31] I. Yaniv and D. Tsafrir, ‚ÄúHash, don‚Äôt cache (the page table),‚Äù ACM\nSIGMETRICS Performance Evaluation Review, vol. 44, no. 1, pp. 337‚Äì\n350, 2016.\n[32] Y. Kwon, H. Yu, S. Peter, C. J. Rossbach, and E. Witchel, ‚ÄúCoordinated\nand efficient huge page management with ingens,‚Äù in 12th {USENIX}\nSymposium on Operating Systems Design and Implementation ({OSDI}\n16), 2016, pp. 705‚Äì721.\n[33] I. Subramanian, C. Mather, K. Peterson, and B. Raghunath, ‚ÄúImple-\nmentation of multiple pagesize support in hp-ux.‚Äù in USENIX Annual\nTechnical Conference, 1998, pp. 105‚Äì119.\n[34] N. Ganapathy and C. Schimmel, ‚ÄúGeneral purpose operating system sup-\nport for multiple page sizes.‚Äù in USENIX Annual Technical Conference,\nno. 98, 1998, pp. 91‚Äì104.\n[35] M. Talluri, S. Kong, M. D. Hill, and D. A. Patterson, ‚ÄúTradeoffs\nin supporting two page sizes,‚Äù in Proceedings of the 19th annual\ninternational symposium on Computer architecture, 1992, pp. 415‚Äì424.\n[36] M. Dashti, A. Fedorova, J. Funston, F. Gaud, R. Lachaize, B. Lepers,\nV. Quema, and M. Roth, ‚ÄúTraffic management: a holistic approach to\nmemory placement on numa systems,‚Äù ACM SIGPLAN Notices, vol. 48,\nno. 4, pp. 381‚Äì394, 2013.\n[37] F. Gaud, B. Lepers, J. Decouchant, J. Funston, A. Fedorova, and\nV. Qu¬¥ema, ‚ÄúLarge pages may be harmful on {NUMA} systems,‚Äù in\n2014 {USENIX} Annual Technical Conference ({USENIX}{ATC} 14),\n2014, pp. 231‚Äì242.\n[38] C. H. Park, I. Vougioukas, A. Sandberg, and D. Black-Schaffer, ‚ÄúEvery\nwalk‚Äôs a hit: making page walks single-access cache hits,‚Äù in Proceed-\nings of the 27th ACM International Conference on Architectural Support\nfor Programming Languages and Operating Systems, 2022, pp. 128‚Äì141.\n[39] F. Gaud, B. Lepers, J. Decouchant, J. Funston, A. Fedorova, and\nV.\nQuema,\n‚ÄúLarge\npages\nmay\nbe\nharmful\non\nnuma\nsystems,‚Äù\nin\n2014\nUSENIX\nAnnual\nTechnical\nConference\n(USENIX\nATC",
        "semantic_similarity": 0.43028974533081055,
        "keyword_overlap": 0.024213075060532687,
        "combined_score": 0.3084667442497272,
        "paragraph_id": 10,
        "source_title": "Pages to Accelerate Address Translation"
      },
      {
        "text_fragment": "0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\nRedis\nMemcached\nSpecjbb\nMasstree\nMoses\nSphinx\nShore\nNormalized\nAverage Latency\nNative\nHugeGPT\nFig. 7: Average latencies of latency sensitive workloads. Average latencies\nare normalized to vanilla Linux/KVM.\nsystem on average. This confirms HUGEGPT‚Äôs effectiveness\non improving application throughput by reducing the overhead\nof two dimensional page walks in vanilla Linux/KVM.\nFigure 5 also shows that HUGEGPT increases the through-\nput by the largest percentage (16%) for the Memcached\nworkload and the smallest percentage (5%) for the BTree and\nGUPS workloads. For the Memcached workload, it strides the\nmemory with weak memory access locality so more page table\nentries may be cached by TLB and page walk caches compared\nto random memory accesses. Therefore, reducing the leaf page\ntable entries of the nested page table in HUGEGPT shows\nmore performance improvement. This is consistent with the\nperformance observation in ¬ßII-B. Since GUPS and BTree\nworkloads conduct randomly memory accesses, HUGEGPT‚Äôs\nperformance improvement on these workloads is less. For\ninstance, GUPS is calculated by identifying the number of\nmemory locations that can be randomly updated in one second,\nso it shows almost no memory access locality such that it may\nbe hard to cache lower level page table entries.\nC. Experiments with Latency Sensitive Workloads\nFigure 7 shows the average latencies of different systems\nwhen they are tested with latency sensitive workloads. On\naverage, native system shows the lowest average latency as\nmost page table entries can be cached while walking the one\nlevel page table. In the worst case, native system only incurs\nfour memory references. Relative to native system, HUGEGPT\nincreases the average latency by 16% on average. Compared to\nvanilla Linux/KVM, HUGEGPT reduces the average latency\nby 8% on average. This is because HUGEGPT reduces the\naverage page walk latency of the two dimensional page walks\nby up to about 50% as explained in ¬ßII-B. HUGEGPT reduces\npage walk cache misses and the number of memory references\nin two dimensional page walks from 24 to 20 in the worst case.\nTo further pinpoint why HUGEGPT increases the average\nlatency compared to native system and reduces the average la-\ntency compared to vanilla Linux/KVM, we profile the average\npage walk latency when the latency sensitive workloads are\ntested with the three systems. We show the profiling results in\nFigure 10. On average, HUGEGPT increases the average page\nwalk latency by 62% compared to native system and decreases\nthe average page walk latency by 8% compared to vanilla\nLinux/KVM. This is consistent with the average latency results\nand also shows HUGEGPT‚Äôs effectiveness on reducing the\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\nRedis\nMemcached\nSpecjbb\nMasstree\nMoses\nSphinx\nShore\nNormalized\nTail Latency (95th)\nNative\nHugeGPT\nFig. 8: 95th percentile tail latencies of latency sensitive workloads. Tail\nlatencies are normalized to vanilla Linux/KVM.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\nRedis\nMemcached\nSpecjbb\nMasstree\nMoses\nSphinx\nShore\nNormalized\nTail Latency (99th)\nNative\nHugeGPT\nFig. 9: 99th percentile tail latencies of latency sensitive workloads. Tail\nlatencies are normalized to vanilla Linux/KVM.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\nRedis\nMemcached\nSpecjbb\nMasstree\nMoses\nSphinx\nShore\nNormalized Average\nPage Walk Latency\nNative\nHugeGPT\nFig. 10: Average page walk latencies of latency sensitive workloads.\nAverage page walk latencies are normalized to vanilla Linux/KVM.\noverhead of two dimensional page walks for latency sensitive\nworkloads in comparison to vanilla Linux/KVM.\nFigure 8 and Figure 9 show the 95th percentile tail latencies\nand the 99th percentile tail latencies, respectively, when the\nlatency sensitive workloads are tested with the three systems.\nOn average, HUGEGPT provides 8% lower 95th percentile\ntail latency 8% lower 99th percentile tail latency compared\nto vanilla Linux/KVM, and 32% higher 95th percentile tail\nlatency and 30% higher 99th percentile tail latency relative to\nnative system. The tail latency test results are consistent with\nthe average page walk latency of the three systems as shown\nin Figure 10.\nFigure 7, Figure 8, and Figure 9 also show that HUGEGPT\nshows small performance advantage for some workloads (e.g.,\nMoses and Masstree) and large performance advantage for\nsome other workloads (e.g., Specjbb and Sphinx). This is\nbecause Specjbb and Sphinx show weak memory access\nlocality. HUGEGPT performs better on these workloads as\nexplained in ¬ßII-B. Memory access patterns in Moses and\nMasstree workloads are more random than Specjbb and\nSphinx. HUGEGPT does not show good performance with",
        "semantic_similarity": 0.41610074043273926,
        "keyword_overlap": 0.035725576983876066,
        "combined_score": 0.30198819139808025,
        "paragraph_id": 6,
        "source_title": "Pages to Accelerate Address Translation"
      }
    ],
    "resolved_title": "Pages to Accelerate Address Translation"
  },
  {
    "citation_marker": "[9]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: YES. The statement accurately reflects Shfllock's NUMA-awareness (as the cited paragraph states Shfllock \"achieve[s] NUMA-awareness\" via queue re-ordering) and mentions borrowing a queue traversal optimization, which aligns with Shfllock's \"shuffling\" technique of re-ordering the wait queue off the critical path to optimize traversal. No misrepresentation is evident."
    ],
    "evidence": [
      {
        "text_fragment": "Scalable and Practical Locking with Shuffling\nSanidhya Kashyap Irina Calciu‚àó\nXiaohe Cheng‚Ä°\nChangwoo Min‚Ä†\nTaesoo Kim\nGeorgia Institute of Technology\n‚àóVMware Research\n‚Ä°HKUST\n‚Ä†Virginia Tech\nAbstract\nLocks are an essential building block for high-performance\nmulticore system software. To meet performance goals, lock\nalgorithms have evolved towards specialized solutions for ar-\nchitectural characteristics (e.g., NUMA). However, in practice,\napplications run on different server platforms and exhibit\nwidely diverse behaviors that evolve with time (e.g., num-\nber of threads, number of locks). This creates performance\nand scalability problems for locks optimized for a single sce-\nnario and platform. For example, popular spinlocks suffer\nfrom excessive cache-line bouncing in NUMA systems, while\nscalable, NUMA-aware locks exhibit sub-par single-thread\nperformance.\nIn this paper, we identify four dominating factors that im-\npact the performance of lock algorithms. We then propose a\nnew technique, shuffling, that can dynamically accommodate\nall these factors, without slowing down the critical path of\nthe lock. The key idea of shuffling is to re-order the queue\nof threads waiting to acquire the lock in accordance with\nsome pre-established policy. For best performance, this work\nis done off the critical path, by the waiter threads. Using\nshuffling, we demonstrate how to achieve NUMA-awareness\nand implement an efficient parking/wake-up strategy, with-\nout any auxiliary data structure, mostly off the critical path.\nThe evaluation shows that our family of locks based on shuf-\nfling improves the throughput of real-world applications\nup to 12.5√ó, with impressive memory footprint reduction\ncompared with the recent lock algorithms.\nCCS Concepts\n‚Ä¢ Software and its engineering ‚ÜíMu-\ntual exclusion.\nKeywords\nmutual exclusion, memory footprint, Linux.\nACM Reference Format:\nSanidhya Kashyap Irina Calciu Xiaohe Cheng Changwoo Min\nTaesoo Kim. 2019. Scalable and Practical Locking with Shuffling. In\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Request permissions from permissions@acm.org.\nSOSP‚Äô19, October 27‚Äì30, 2019, Huntsville, ON, Canada\n¬© 2019 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 978-1-4503-6873-5/19/10...$15.00\nhttps://doi.org/10.1145/3341301.3359629\n0k\n100k\n200k\n300k\n400k\n500k\n24 48 72 96 120144168192\n1\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\n0\n24 48 72 96 120144168192\nOps/sec\n#threads\n(a) File creation in a shared directory\nStock\nCST\nCohort\nShflLock\nMemory (MB)\n#threads\n(b) Memory used by lock instances\nFigure 1. Impact of locks on a file-system micro-benchmark that\nspawns threads to create new files in a shared directory (MWCM [39]).\nA process stresses the writer side of the readers-writer lock. We eval-\nuate the Linux baseline version (Stock), CST [27], Cohort lock [18],\nand our proposed ShflLock. (a) File creation throughput on an\n8-socket 192-core machine. (b) Total memory consumed by locks\nthat are part of the inode structure.\nACM SIGOPS 27th Symposium on Operating Systems and Principles\n(SOSP‚Äô19). ACM, New York, NY, USA. 14 pages. https://doi.org/10.\n1145/3341301.3359629\n1\nIntroduction\nThe introduction of multicore machines marked the end of\nthe ‚Äúfree lunch‚Äù[47], making concurrent programming, es-\npecially lock-based mutual exclusion, a critical approach to\nimprove the performance of applications. Lock algorithms\ndetermine the scalability of applications in multicore ma-\nchines [3, 5, 21].\nSince the invention of concurrent programming, lock de-\nsign has been influenced by hardware evolution. For instance,\nMCS [37] was proposed to address excessive cache-line traffic\nresulting from an increasing number of threads trying to ac-\nquire the lock at the same time, while Cohort locks [18] were\nproposed in response to the emergence of the non-uniform\nmemory access (NUMA) architecture. NUMA machines con-\nsist of multiple nodes (or sockets), each with multiple cores,\nlocally attached memories, and fast caches. In such machines,\nthe access from a socket to its local memory is faster than\nremote access to memory on a different socket [44] and each\nsocket has a shared last-level-cache. Cohort locks exploit\nthis characteristic to improve application throughput.\nUnfortunately, the influence of hardware evolution on\nlock design has resulted in a tight coupling between hard-\nware characteristics and lock algorithms. Meanwhile, other\nfactors have been neglected, such as memory footprint [10],\nlow thread counts, and core over-subscription. For exam-\nple, Cohort locks can achieve high throughput at high core\ncounts, but also require memory proportional to the num-\nber of sockets. The extra memory is unacceptable for some\napplications, such as databases and OSes, which can have\n586",
        "semantic_similarity": 0.7842199206352234,
        "keyword_overlap": 0.0580991474581623,
        "combined_score": 0.566383688682105,
        "paragraph_id": 0,
        "source_title": "Scalable and Practical Locking with Shuffling"
      }
    ],
    "resolved_title": "Scalable and Practical Locking with Shuffling"
  }
]