[
  {
    "citation_marker": "[10]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: **Answer:** YES  \n**Reason:** The statement \"[10] Sanidhya Kashyap, Changwoo Min, and Taesoo Kim.\" **accurately reflects the authorship** listed in the cited document's header (\"Sanidhya Kashyap, Changwoo Min, Taesoo Kim, Georgia Institute of Technology\"). The names, order, and attribution are identical, with no distortion or fabrication.  \n\nThe statement solely cites authorship, and the provided paragraph explicitly confirms these three authors. The content of the abstract/introduction is irrelevant to this specific attribution, which is correctly supported."
    ],
    "evidence": [
      {
        "text_fragment": "Scalable NUMA-aware Blocking Synchronization Primitives\nSanidhya Kashyap\nChangwoo Min\nTaesoo Kim\nGeorgia Institute of Technology\nAbstract\nApplication scalability is a critical aspect to efficiently\nuse NUMA machines with many cores. To achieve that,\nvarious techniques ranging from task placement to data\nsharding are used in practice. However, from an operat-\ning system‚Äôs perspective, these techniques often do not\nwork as expected because various subsystems in the OS\ninteract and share data structures among themselves, re-\nsulting in scalability bottlenecks. Although current OSes\nattempt to tackle this problem by introducing a wide range\nof synchronization primitives such as spinlock and mu-\ntex, the widely-used synchronization mechanisms are not\ndesigned to handle both under- and over-subscribed sce-\nnarios in a scalable manner. In particular, the current\nblocking synchronization primitives that are designed to\naddress both scenarios are NUMA oblivious, meaning\nthat they suffer from cache line contention in an under-\nsubscribed situation, and even worse, inherently spur long\nscheduler intervention, which leads to sub-optimal perfor-\nmance in an over-subscribed situation.\nIn this work, we present several design choices to im-\nplement scalable blocking synchronization primitives that\ncan address both under- and over-subscribed scenarios.\nSuch design decisions include memory-efficient NUMA-\naware locks (favorable for deployment) and scheduling-\naware, scalable parking and wake-up strategies. To vali-\ndate our design choices, we implement two new blocking\nsynchronization primitives, which are variants of mutex\nand reader-writer semaphore in the Linux kernel. Our\nevaluation results show that the new locks can improve\nthe application performance by 1.2‚Äì1.6√ó, and some of\nthe file system operations by as much as 4.7√ó, in both\nunder- and over-subscribed scenarios. These new locks\nuse 1.5‚Äì10√ó less memory than state-of-the-art NUMA-\naware locks on 120-core machine.\n1\nIntroduction\nOver the last decade, microprocessor vendors have been\npursuing the direction of bigger multi-core and multi-\nsocket machines [19, 33]. For example, a single sys-\ntem can have up to 4096 hardware threads that are or-\nganized into sockets, known as NUMA (Non-Uniform\nMemory Access) domains [33]. They address a key prob-\nlem of removing the memory access latency bottleneck\nby directly attaching multiple CPUs to a large chunk of\nmemory (DRAM). Furthermore, these machines have be-\ncome a norm to further scale applications such as large\nin-memory databases (Microsoft SQL server [28]) and\nprocessing engines [34, 41].\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\n120.0\n140.0\n0\n20\n40\n60\n80 100 120\nM ops/sec\n#thread\n(a) Directory read\n0.0\n5.0\n10.0\n15.0\n20.0\n25.0\n0\n20\n40\n60\n80 100 120\nGB\n#thread\n(b) Memory usage\nVanilla\nCohort\nCST\nFigure 1: Impact of NUMA-aware locks on a file-system micro-\nbenchmark that spawns threads to enumerate files in a shared\ndirectory [29], which stresses the reader side of the reader-writer\nsemaphore (rwsem). Figure (a) shows the impact of locks on\nthe throughput till 120 threads on a 120 core machine, where\nVanilla is Linux‚Äôs native version, Cohort is an in-kernel ported\nversion of NUMA-aware lock [17], and our NUMA-aware lock\nimplementation (CST). Figure (b) shows memory usage that\nuse these locks before and after the experiments.\nA NUMA machine consists of multiple sockets, where\neach node has a locally attached memory, a last-level\ncache and multiple CPUs. It only exposes a flat cache-\ncoherent architecture to software by hiding the underlying\nhardware topology from the applications. Unfortunately,\nthis flat architecture hinders the scalability of applications\nas the applications may either suffer remote memory ac-\ncess or from the contended memory access from multiple\nCPUs, thereby degrading their performance [2, 4].\nTo achieve scalability in NUMA machines, various\napplications such as databases, and OS rely on NUMA\npartitioning to mitigate the cost of remote memory access\neither by data placement or via task placement to achieve\nhigh performance. However, this approach does not solve\nthe problem of how to efficiently modify shared data struc-\ntures such as inodes, dentry cache or even the structures of\nmemory allocator that are shared among multiple threads.\nAs a result, synchronization primitives become the basic\nbuilding blocks of such multi-threaded applications, and\nare critical in determining their scalability [4]. Hence, the\nstate-of-the-art NUMA-aware locks [6, 7, 12, 13, 17, 25]\nare the apt choice to efficiently exploit the NUMA be-\nhavior, and also achieve scalability on these architectures.\nUnfortunately, they are difficult to adopt in practice due\nto their inherent memory overhead.\nFor non-blocking locks, Wickizier et al. [4] showed that\na Ticket lock suffers from cache-line contention with in-\ncreasing core count. They replace it with the MCS lock to\nmitigate such effect, improving the system performance.\nHowever, its adoption faced several challenges due to\nthe memory constraint of the spinlock [23]. Similarly,\nfor blocking synchronization primitives, there are various\n1",
        "semantic_similarity": 0.08546900749206543,
        "keyword_overlap": 0.005573248407643312,
        "combined_score": 0.06150027976673879,
        "paragraph_id": 0,
        "source_title": "Scalable NUMA-aware Blocking Synchronization Primitives"
      }
    ],
    "resolved_title": "Scalable NUMA-aware Blocking Synchronization Primitives"
  },
  {
    "citation_marker": "[11]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.67, overlap: 0.02): **NO**  \nThe statement from the article claims that the cited document ([11]) discusses \"work conservation\" in resource scheduling, where resources should be occupied unless no waiter exists. However, the provided content from the cited document (\"OS Scheduling with Nest\") **does not mention \"work conservation\" at all**. Instead, it focuses on:  \n- Reducing task dispersal via \"nests\" (sets of cores) to maintain high core frequency.  \n- Heuristics for core placement (e.g., searching primary/reserve nests before falling back to CFS).  \n- Avoiding frequency inversion and optimizing energy/performance trade-offs.  \n\nThe cited document describes scheduling strategies to keep tasks on \"warm\" cores for efficiency but **never addresses the work-conservation principle or lock overhead related to waiting threads** as asserted in the article. Thus, the article misrepresents the cited source by attributing unrelated concepts to it.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.65, overlap: 0.05): **NO**  \nThe cited paragraph discusses work conservation in the context of OS task scheduling (allocating tasks to cores), stating it requires that tasks should not be placed on non-idle cores if idle cores are available. However, the article's statement applies work conservation to **locking mechanisms**, specifically linking it to lock idle time overhead increasing with waiting threads. The cited source does not mention locking, synchronization, or thread wait times at all‚Äîit focuses solely on task/core scheduling for performance and energy efficiency. Thus, the article distorts the original context by misattributing the concept to locking.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.64, overlap: 0.02): **NO**  \nThe statement claims work conservation is a significant requirement in locking to avoid overhead from waiting threads, implying it is universally desirable. However, the cited paragraph explicitly describes how Linux's CFS scheduler **violates work conservation** during task wakeup:  \n- CFS restricts idle-core searches to a single die (ignoring other dies).  \n- It makes only \"a limited effort\" to find idle cores on that die.  \n- The source states: \"**Wakeup is not work conserving**\" directly.  \n\nThus, the article distorts the cited content by presenting work conservation as essential, while the source highlights a deliberate non-work-conserving design in Linux for efficiency.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "OS Scheduling with Nest: Keeping Tasks Close Together on Warm Cores\nEuroSys ‚Äô22, April 5‚Äì8, 2022, RENNES, France\n2.2\nThe ùëÜmove scheduler, targeting core frequency\nGouicem et al. [7] identified the problem of frequency inver-\nsion, in the common case where a task ùëáparent forks or wakes\nanother task ùëáchild and then immediately sleeps to wait for\nùëáchild‚Äôs results. ùëáparent‚Äôs core is likely running at a high fre-\nquency, while CFS will place ùëáchild on an idle core if one is\navailable.ùëáparent will thus be delayed untilùëáchild completes on\nan initially low-frequency core, while ùëáparent‚Äôs former high\nfrequency core is available. Gouicem et al. showed that this\nproblem causes a slowdown on a variety of applications.\nGouicem et al. proposed the scheduler ùëÜmove that places\nùëáchild on the core ofùëáparent, allowingùëáchild to benefit from that\ncore‚Äôs high frequency. If ùëáparent does not immediately block\nor if other tasks are waiting, such a strategy may cause ùëáchild\nto incur high latency. Thus, ùëÜmove only makes this placement\nwhen the core chosen by CFS has a low frequency and sets\na timer for ùëáchild, to move ùëáchild to the core chosen by CFS\nif ùëáchild is not scheduled on ùëáparent‚Äôs core within a brief de-\nlay. When the timer expires, however, ùëÜmove does nothing to\nensure that ùëáchild ends up on a core with a high frequency.\n2.3\nLinux‚Äôs power governors\nThe scheduler has no control over core frequencies. Instead,\ncore frequency results from an interplay between the Linux\npower governor and the hardware. The governor sets the\nbounds in which the frequency should vary and can make\nsuggestions about what frequency should be used. The hard-\nware combines the information from the governor with its\nobservations about the current activity on the core and the\ncore‚Äôs socket, and chooses a frequency for the core accord-\ningly. The power governor has a significant impact on per-\nformance for many applications. Thus, when we refer to a\nscheduler, we refer to the used governor as well. We consider\nthe performance and schedutil governors, which are available\non most Linux systems and represent distinct strategies.\nPerformance requests that the hardware use the nomi-\nnal frequency of the machine. The hardware can still freely\nchoose between the nominal frequency and the turbo fre-\nquencies. Performance gives tasks high performance, but\nmisses the potential energy savings that can be obtained by\nrunning non-demanding tasks at lower frequencies.\nSchedutil takes into account information from the sched-\nuler about recent task activity, to attempt to reconcile perfor-\nmance and energy usage. It allows the machine to use its full\nrange of frequencies. When schedutil observes that the tasks\non a core have a high recent CPU utilization, it suggests to\nthe hardware to increase the frequency.\n3\nThe Nest Approach\nThe key idea behind Nest is the use of a nest, defining a\nlimited set of recently used cores to consider in high priority\nwhen placing a task. By limiting activity to a small number of\ncores, Nest encourages the hardware to choose a high core\nprimary\nnest\nSearch\nstrategy\nNest management\non core selection\nnothing idle\n)\ntimeout\n5\nreserve\nnest\nselection\nj\nnothing idle\n&\nCFS\nnormal\nselection\ni\nimpatient selection\nO\nFigure 1. Core-search path through the nests (top) and core\nmovement between nests (bottom).\nfrequency. The challenge in creating a scheduler around this\nidea is to design heuristics that properly dimension the nest\naccording to applications‚Äô current needs. A nest that is too\nlarge will result in task dispersal, replicating the dispersal\nproblem of CFS. A nest that is too small will result in tasks\ncompeting for the same cores, inducing overloads.\n3.1\nBuilding the nest\nAs shown in Figure 1, Nest keeps track of two sets of cores\n(nests), to consider in high priority for task placement. Cores\nin the primary nest are currently in use or have been used\nrecently and are expected to be useful in the near future.\nCores in the reserve nest were previously in the primary nest\nbut have not been used recently and thus are considered\nto be less likely to be used in the near future, or they have\nrecently been selected by CFS and have not yet proved their\nnecessity for the current set of tasks.\nThe top of Figure 1 (red arrows) describes the core-search\nheuristic. For a forking or waking task, Nest first searches\nfor an idle core in the primary nest, then if none is found\nit searches for an idle core in the reserve nest. If that also\nfails, then it falls back on CFS. The search in the primary\nnest starts at the task‚Äôs previous core (or the parent‚Äôs core,\nfor a fork), to reduce the risk of collision with concurrent\nforks and wakeups on other cores. The search in the reserve\nnest, which is expected to be accessed less often, starts from\na fixed core, chosen arbitrarily as the core on which the\nsystem call that started Nest was executed, to reduce task\ndispersal. In both cases, the search first considers cores on\nthe same die as the task‚Äôs previous core (or the parent‚Äôs core,\nfor a fork), before considering cores on the other dies. This\nheuristic reduces the number of used dies, thus increasing\nthe chance of leaving some dies completely idle and saving\nenergy. Unlike CFS, Nest selects any core that is found to be\ncurrently idle, independent of recent load, in order to favor\ncore reuse. Also unlike CFS, Nest does not take into account\nactivity on hyperthreads. Nevertheless, all cores that newly\nenter the nests are initially chosen by CFS when there are no\nidle cores in the nests, and thus they inherit CFS‚Äôs strategy\nof selecting cores where the hyperthread is idle.\nThe bottom of Figure 1 (blue arrows) indicates how cores\nmove between the nests, to allow the nest size to adapt to\n3",
        "semantic_similarity": 0.6711508333683014,
        "keyword_overlap": 0.02436795613767895,
        "combined_score": 0.4771159701991146,
        "paragraph_id": 2,
        "source_title": "OS Scheduling with Nest: Keeping Tasks Close Together on Warm Cores"
      },
      {
        "text_fragment": "OS Scheduling with Nest: Keeping Tasks Close\nTogether on Warm Cores\nJulia Lawall\nInria\nParis, France\nHimadri Chhaya-Shailesh\nInria\nParis, France\nJean-Pierre Lozi\nOracle Labs\nZurich, Switzerland\nBaptiste Lepers\nUniversity of Sydney\nSydney, Australia\nWilly Zwaenepoel\nUniversity of Sydney\nSydney, Australia\nGilles Muller\nInria\nParis, France\nAbstract\nTo best support highly parallel applications, Linux‚Äôs CFS\nscheduler tends to spread tasks across the machine on task\ncreation and wakeup. It has been observed, however, that in\na server environment, such a strategy leads to tasks being\nunnecessarily placed on long-idle cores that are running at\nlower frequencies, reducing performance, and to tasks being\nunnecessarily distributed across sockets, consuming more\nenergy. In this paper, we propose to exploit the principle of\ncore reuse, by constructing a nest of cores to be used in pri-\nority for task scheduling, thus obtaining higher frequencies\nand using fewer sockets. We implement the Nest scheduler\nin the Linux kernel. While performance and energy usage\nare comparable to CFS for highly parallel applications, for a\nrange of applications using fewer tasks than cores, Nest im-\nproves performance 10%‚Äì2√ó and can reduce energy usage.\nCCS Concepts: ‚Ä¢ Computer systems organization;\nKeywords: Scheduling, Linux kernel\nACM Reference Format:\nJulia Lawall, Himadri Chhaya-Shailesh, Jean-Pierre Lozi, Baptiste\nLepers, Willy Zwaenepoel, and Gilles Muller. 2022. OS Schedul-\ning with Nest: Keeping Tasks Close Together on Warm Cores. In\nSeventeenth European Conference on Computer Systems (EuroSys\n‚Äô22), April 5‚Äì8, 2022, RENNES, France. ACM, New York, NY, USA,\n16 pages. https://doi.org/10.1145/3492321.3519585\n1\nIntroduction\nThe primary goal of an operating system (OS) task scheduler\nis to allocate tasks to cores in a way that maximizes applica-\ntion performance. A well-known desirable property is work\nconservation, i.e., if a task is placed on a core that is not idle,\nthen no idle core should be available [10, 11]. However, in\nchoosing a core for a task, it is also important to consider\nwhether the chosen core will allow the task to access needed\nEuroSys ‚Äô22, April 5‚Äì8, 2022, RENNES, France\n¬© 2022 Association for Computing Machinery.\nThis is the author‚Äôs version of the work. It is posted here for your personal\nuse. Not for redistribution. The definitive Version of Record was published\nin Seventeenth European Conference on Computer Systems (EuroSys ‚Äô22), April\n5‚Äì8, 2022, RENNES, France, https://doi.org/10.1145/3492321.3519585.\n(hardware) resources efficiently. The performance that a task\ncan achieve is determined in part by the frequency of the cho-\nsen core [7]. On modern CPUs, core frequencies may vary\nsignificantly, as individual cores can adjust their frequency\nindependently. Nevertheless, the Linux kernel‚Äôs default CFS\nscheduler does not take core frequency into account. Placing\ntasks on cores in a way that causes higher frequencies to be\nused can improve performance.\nWe consider scheduling on large multicore servers. Such\nservers are today becoming more accessible and affordable.\nThey can be used for traditional high-performance comput-\ning, where applications are often designed to decompose to\nthe number of cores available, so that tasks can be pinned to\ncores, making scheduling irrelevant. But multicore servers\ncan also be used as computing resources for applications\nthat are demanding in terms of compute cycles, memory,\nor disk requirements. Such applications rely on the OS task\nscheduler for task placement. The number of cores required\nmay vary from few to many, and back, across the course of\nthe application. To get the best performance, the OS task\nscheduler must optimally adapt to all of these situations.\nModern servers offer ‚Äúturbo‚Äù frequencies [1, 8] allowing\ncores to run at a frequency higher than the nominal fre-\nquency. Various turbo frequencies are available, depending\non the number of active cores on the socket, to respect ther-\nmal constraints. The frequency is determined jointly by the\nsoftware and the hardware. The software, typically an OS\nkernel-level power governor, suggests boundaries, and then\nthe hardware chooses a frequency for a core within these\nboundaries according to the number of cores on the same\nsocket and their current degree of activity. To obtain the\nhighest possible frequencies, it is necessary to minimize the\nnumber of cores used (‚Äúkeeping tasks close together‚Äù) and\nensure a sustained activity (‚Äúkeeping cores warm‚Äù).\nIn this paper, we propose the task scheduler Nest, de-\nsigned according to the principles reuse cores and keep cores\nwarm. To increase core reuse, Nest tries to place tasks within\na set of recently used cores (the nest). To keep cores warm,\nwith Nest, the idle process spins on a newly idle core for\na short period, to encourage the hardware to keep the fre-\nquency high. Finally, when a task must be placed outside the\n1",
        "semantic_similarity": 0.6453209519386292,
        "keyword_overlap": 0.053804765564950036,
        "combined_score": 0.4678660960265254,
        "paragraph_id": 0,
        "source_title": "OS Scheduling with Nest: Keeping Tasks Close Together on Warm Cores"
      },
      {
        "text_fragment": "EuroSys ‚Äô22, April 5‚Äì8, 2022, RENNES, France\nJ. Lawall, H. Chhaya-Shailesh, J.-P. Lozi, B. Lepers, W. Zwaenepoel, and G. Muller\nnest because another task is using its previous core, Nest re-\nmembers this previous core, and attempts to return the task\nto the same core in the nest the next time a core is chosen for\nthe task. On purely sequential applications and applications\nwith as many or more tasks than cores, Nest performs simi-\nlarly to CFS. Nest is particularly beneficial for applications\nwith a moderate number of effective concurrent tasks, and\nwhere tasks fork, block, and terminate frequently, resulting\nin many task placements. We implement Nest within the\nLinux kernel, by modifying the CFS scheduler.\nOur contributions are as follows:\n‚Ä¢ We motivate two new principles for task schedulers:\nreuse cores and keep cores warm, and implement these\nprinciples in the Linux kernel v5.9.\n‚Ä¢ We show performance improvements with Nest on\na wide range of multicore benchmarks, on four 2- or\n4-socket multicore machines, including improvements\non a 4-socket Intel Xeon 6130 of more than 20% on 8%\nof our more than 200 Phoronix multicore tests.\n‚Ä¢ We show that these performance improvements can\nalso reduce CPU energy usage by up to 20%, on our\nsoftware configuration benchmark.\n‚Ä¢ Nest often achieves comparable or better performance\nthan the Linux kernel v5.9‚Äôs performance power gov-\nernor, which requests use of at least the nominal fre-\nquency, while using the schedutil power governor, which\nallows lower frequencies in periods of light activity.\nThe rest of this paper is organized as follows. Section 2\npresents background about the Linux kernel‚Äôs CFS scheduler\nand power management. Section 3 presents Nest and Section\n4 presents its implementation in the Linux kernel. Section 5\nevaluates Nest on multicore benchmarks. Finally, Section 6\npresents some related work and Section 7 concludes.\nTerminology. We say concurrent tasks for the set of tasks\nthat are running at a given point in time. This does not\ninclude tasks that are sleeping or waiting to run. We evaluate\nNest on Intel servers, where two hardware threads share\na single physical core (simultaneous multithreading). For\nsimplicity, we refer to the number of cores on a machine as\nthe number of hardware threads. We say that one core is a\nhyperthread of another core if both share the same physical\ncore. A key concept in Linux‚Äôs Completely Fair scheduler is\nthe set of cores that share a last-level cache. We refer to such\ncores as being on the same die.\n2\nBackground\nWe present Linux‚Äôs Completely Fair (CFS) scheduler (as of\nLinux v5.9), that Nest extends, as well as a recent scheduler,\nùëÜmove, that also targets better use of core frequencies. We\nthen present Linux‚Äôs power governors that influence the\nfrequency changes at the hardware level.\n2.1\nLinux‚Äôs CFS scheduler\nCFS uses a collection of heuristics to place tasks on cores\non task fork, task exec, task wakeup, and load balancing.\nFork and wakeup are most relevant to Nest. We present the\nmost commonly used heuristics, which rely on the Linux\nscheduling domains. Fork and wakeup represent around 1300\nlines of code, out of the more than 11K lines of code in\nfair.c, the main file implementing CFS.\nScheduling domains. The Linux kernel scheduler views\nthe available CPUs according to a hierarchy of domains,\norganized into levels. On our multi-socket Intel servers, the\ndomains are NUMA (all cores on the machine), SMP (cores\non the same die), and SMT (hyperthreads sharing the same\nphysical core), from highest to lowest. Each core is associated\nwith the sequence of domains that contain it. Each domain\nrefers to a list of groups, which comprise the list of cores\nassociated with each of its child domains.\nFork. Starting from the highest domain, CFS searches for the\nleast loaded associated group, and then for the least loaded\ncore within that group. The load, of a group or a core, is\ncharacterized by various criteria, including the number of\nidle cores, the recent load on the cores, the expected time to\nwake from idle states, and the new task‚Äôs NUMA preferences.\nCores are numbered by successive integers. The search for a\ncore within a group is carried out in numerical order, modulo\nthe number of cores available, starting from the core per-\nforming the fork. When a core is selected, the search repeats\nwith the child domain containing the chosen core.\nSearch for a core across the entire machine favors work\nconservation. On the other hand, searching in a fixed order\nmeans that recently used cores may be overlooked, thus\nhindering reuse, if other idle cores come earlier in the order.\nTaking recent load into account also hinders reuse, as it\ndisfavors idle cores that have been recently used.\nWakeup. CFS first selects a target core, that is either the\nwoken task‚Äôs previous core or the core performing the wake-\nup. Heuristics are used to choose between them, combining\ninformation about core idleness, the type of wakeup, and the\nrecent load on the core. CFS then searches for an idle core on\nthe target‚Äôs die. First, it searches for a core where both the\ncore and its hyperthread are idle. If none is found, it searches\nthrough a few cores to find one that is idle. If this search also\nfails, CFS checks whether the hyperthread of the target is\nidle. If all previous searches fail, CFS selects the target.\nWakeup is not work conserving, as it only considers a\nsingle die, and it only makes a limited effort to find an idle\ncore on that die. It traverses the cores in a fixed order, and\nthus it may overlook recently used idle cores. On the other\nhand, it does not consider recent load in the final core choice,\nand thus recently used idle cores are not disfavored.\n2",
        "semantic_similarity": 0.6438806056976318,
        "keyword_overlap": 0.020826952526799388,
        "combined_score": 0.4569645097463821,
        "paragraph_id": 1,
        "source_title": "OS Scheduling with Nest: Keeping Tasks Close Together on Warm Cores"
      }
    ],
    "resolved_title": "OS Scheduling with Nest: Keeping Tasks Close Together on Warm Cores"
  },
  {
    "citation_marker": "[12]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.11, overlap: 0.04): **NO**  \n\n**Reason:** The statement in the article claims that citation [12] (SMURF-THP) discusses Linux spinlocks, MCS locks, NUMA-aware locks (e.g., CNA, Shfllock), and lock design optimizations. However, the cited SMURF-THP paragraph exclusively focuses on **uncertainty quantification for event prediction using a Transformer Hawkes process**, including:  \n- Accuracy metrics (Table 5, Figure 7)  \n- Training data sensitivity (Figure 6)  \n- Hyperparameter tuning (Figure 5)  \n- Comparisons with non-likelihood-based estimators.  \n\nThere is **no mention of locks, NUMA architectures, operating systems, or scheduling mechanisms** in the provided SMURF-THP content. The article misattributes research on lock optimizations to SMURF-THP, which is unrelated. This constitutes a **distortion/fabrication** of the cited source.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.10, overlap: 0.03): **Answer:** NO  \n**Reason:** The cited paragraph from \"[12]\" (SMURF-THP) exclusively discusses uncertainty quantification methods for Transformer Hawkes Processes, including calibration scores, interval lengths, and ablation studies on time-series datasets. It contains **no mention** of mutex locks, scheduler cooperation, thread suspension, CPU fairness, or NUMA architecture. The article‚Äôs claim that \"[12] proposed a scheduler-cooperative mutex (SCL)\" is **unsupported and misattributes** the cited work, which belongs to an unrelated domain (machine learning for event prediction).",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.09, overlap: 0.03): **NO**  \nThe statement misrepresents the cited source. The article claims [12] proposed \"SCL\" (a scheduler-cooperative mutex for thread suspension and CPU fairness), but the cited paragraph from [12] (\"SMURF-THP\") exclusively discusses a statistical method for uncertainty quantification in event modeling (score matching, Hawkes processes, and Transformer networks). There is no mention of locks, thread scheduling, NUMA, or any OS-related mechanisms in the cited text.  \n\n**Reason**:  \n- **Article statement**: Describes OS scheduling (SCL mutex, thread suspension, CPU fairness, NUMA).  \n- **Cited source [12]**: Focuses on probabilistic event modeling (score matching, Hawkes processes, Langevin Dynamics).  \nThe content is entirely unrelated; the citation is either incorrect or misattributed.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process\nTable 5. Event type prediction accuracy of SMURF-THP given\nground truth event time as the inputs.\nSMURF-THP\nDataset\nAcc(%)\nStackOverflow\n48.38\nRetweet\n61.65\nMIMIC-II\n83.84\nFinancial\n61.62\nWe also train SMURF-THP with different volumes of train-\ning data to study its generalization ability. We train the\nmodel on different ratios of the dataset and present the per-\nformance in Figure 6 and Figure 7. As shown, all metrics\ngo better as we feed more training data. Compared to the\nRetweet dataset, SMURF-THP is more sensitive to training\nratio on the StackOverflow dataset. This is due to that the\nStackOverflow dataset contains less events than the Retweet,\nthereby SMURF-THP requires larger proportion to learn the\ndistribution.\nSMURF-THP(CS)\nSMURF-THP(CRPS)\nCalibration Score (%)\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\nCRPS\n0.440\n0.445\n0.450\n0.455\nHyperparameter Œ±\n1\n10\n(a) Calibration Score and CRPS\nSMURF-THP(Acc)\nAccuracy (%)\n46.15\n46.20\n46.25\n46.30\nHyperparameter Œ±\n1\n10\n(b) Accuracy\nFigure 5. Sensitivity of the hyperparameter Œ± in the training objec-\ntive to prediction performance.\nSMURF-THP(CS)\nSMURF-THP(CRPS)\nCalibration Score (%)\n1\n2\n3\nCRPS\n0.44\n0.45\n0.46\n0.47\nTraining data ratio\n0\n0.5\n1.0\n(a) StackOverflow\nCalibration Score (%)\n0.7\n0.8\n0.9\nCRPS\n0.85\n0.90\n0.95\n1.00\n1.05\nTraining data ratio\n0\n0.5\n1.0\n(b) Retweet\nFigure 6. Calibration Score and CRPS of SMURF-THP trained\nwith different ratios of the StackOverflow and Retweet datasets.\n5. Discussion and Conclusion\nWe acknowledge the existence of several studies that\nadopt non-likelihood-based estimators to circumvent the\nintractable integral within the log-likelihood computation.\nWe present our discussions on these works below. Xiao et al.\nAcc(SO)\nAcc(RT)\nAccuracy-SO (%)\n45.0\n45.5\n46.0\nAccuracy-RT (%)\n59.6\n59.8\n60.0\n60.2\n60.4\nTraining data ratio\n0\n0.5\n1.0\nFigure 7. Prediction Accuracy of SMURF-THP trained with differ-\nent ratio of the StackOverflow and Retweet datasets.\n(2017) trains the model to directly predict the next event‚Äôs\ntime and type through a summation of Mean Squared Error\n(MSE) and cross-entropy loss. However, their model does\nnot construct an explicit intensity function and hence doesn‚Äôt\nsupport flexible sampling and uncertainty quantification. TP-\nPRL (Li et al., 2018) employs reinforcement learning (RL)\nfor learning an events generation policy, but they concentrate\non the temporal point process (TPP) rather than the marked\ntemporal point process (MTPP), which is the focus of our\nwork. Furthermore, they presume the intensity function to\nbe constant between timestamps, a limitation that hampers\nthe accurate capture of the point process‚Äôs temporal dynam-\nics. Upadhyay et al. (2018) applies RL for MTPP which\ntrains a policy to maximize feedback from the environment.\nSimilar to TPPRL, it assumes a very stringent intensity func-\ntion, e.g., in exponential forms, which is oversimplified to\ncapture the complex point process in real-world applications.\nINITIATOR (Guo et al., 2018) and NCE-TPP (Mei et al.,\n2020) are both based on noise-contrastive estimations for\nMTPP. However, they utilize the likelihood objective for\ntraining the noise generation network, which consequently\nreintroduces the intractable integral. In our experiments,\nwe include NCE-TPP‚Äôs performance, as its authors have\ndemonstrated it outperforms INITIATOR.\nSeveral other works (Wang et al., 2020; Fox et al., 2016)\nexplore different scopes of uncertainty quantification for the\nHawkes process. That is, they provide uncertainty quantifi-\ncation for the parameters in conventional Hawkes process\nmodels, whereas we focus on uncertainty quantification for\nthe predicted arrival time.\nIn this work, we present SMURF-THP, a score-based\nmethod for training Transformer Hawkes process models\nand quantifying prediction uncertainty. Our proposed model\nadopts score matching as the training objective to avoid\nintractable computations in conventional Hawkes process\nmodels. Moreover, with the learnt score function, we can\nsample arrival time of events using the Langevin Dynam-\nics. This enables uncertainty quantification by calculating\nthe associated confidence interval. Experiments on various\nreal-world datasets demonstrate that SMURF-THP achieves\nstate-of-the-art performance in terms of Calibration Score,\nCRPS and Interval Length.\n9",
        "semantic_similarity": 0.10659301280975342,
        "keyword_overlap": 0.04172560113154172,
        "combined_score": 0.0871327893062899,
        "paragraph_id": 8,
        "source_title": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process"
      },
      {
        "text_fragment": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process\nTable 2. Comparison of different methods‚Äô performance on four real-world datasets in terms of Calibration Score (CS), CRPS, Coverage\nError (CER) and Interval Length (IL). CER and IL are both calculated at confidence level 0.5.\nStackOverflow\nRetweet\nMethods\nCS(%)(‚Üì)\nCRPS(‚Üì)\nCER(%)(‚Üì)\nIL(‚Üì)\nAcc(%)(‚Üë)\nCS(%)(‚Üì)\nCRPS(‚Üì)\nCER(%)(‚Üì)\nIL(‚Üì)\nAcc(%)(‚Üë)\nNHP\n6.64¬±0.55\n0.69¬±0.01\n5.78¬±0.26\n0.67¬±0.04\n46.20¬±0.13\n14.35¬±0.22\n1.65¬±0.05\n16.98¬±0.24\n0.077¬±0.005\n60.28¬±0.08\nNCE-TPP\n5.64¬±0.46\n0.62¬±0.01\n5.99¬±0.39\n0.63¬±0.04\n46.20¬±0.15\n13.55¬±0.10\n1.59¬±0.05\n10.30¬±0.16\n0.055¬±0.002\n60.30¬±0.06\nSAHP\n4.48¬±0.24\n0.48¬±0.02\n7.73¬±0.47\n0.55¬±0.01\n46.22¬±0.06\n10.06¬±0.35\n1.12¬±0.02\n15.65¬±0.03\n0.061¬±0.001\n60.32¬±0.08\nTHP\n4.13¬±0.25\n0.46¬±0.01\n5.90¬±0.33\n0.56¬±0.02\n46.48¬±0.05\n4.12¬±0.13\n1.08¬±0.05\n3.00¬±0.02\n0.059¬±0.002\n60.63¬±0.13\nSMURF-THP\n0.65¬±0.12\n0.44¬±0.01\n0.48¬±0.09\n0.52¬±0.01\n46.26¬±0.08\n0.71¬±0.16\n0.86¬±0.04\n0.76¬±0.03\n0.031¬±0.001\n60.34¬±0.12\nMIMIC-II\nFinancial\nMethods\nCS(%)(‚Üì)\nCRPS(‚Üì)\nCER(%)(‚Üì)\nIL(‚Üì)\nAcc(%)(‚Üë)\nCS(%)(‚Üì)\nCRPS(‚Üì)\nCER(%)(‚Üì)\nIL(‚Üì)\nAcc(%)(‚Üë)\nNHP\n9.87¬±0.35\n0.74¬±0.03\n8.85¬±0.29\n0.69¬±0.04\n83.25¬±0.27\n4.64¬±0.42\n1.03¬±0.01\n3.85¬±0.30\n0.053¬±0.002\n60.23¬±0.05\nNCE-TPP\n6.52¬±0.24\n0.65¬±0.01\n9.23¬±0.38\n0.77¬±0.04\n83.60¬±0.32\n3.55¬±0.12\n1.35¬±0.05\n4.09¬±0.04\n0.055¬±0.04\n60.28¬±0.09\nSAHP\n7.49¬±0.32\n0.61¬±0.01\n15.70¬±0.52\n0.35¬±0.01\n83.76¬±0.13\n3.50¬±0.33\n1.09¬±0.05\n3.19¬±0.06\n0.048¬±0.003\n60.37¬±0.08\nTHP\n3.89¬±0.14\n0.75¬±0.05\n5.81¬±0.12\n0.84¬±0.03\n84.78¬±0.13\n3.53¬±0.19\n1.57¬±0.02\n3.19¬±0.13\n0.042¬±0.006\n60.51¬±0.06\nSMURF-THP\n2.87¬±0.11\n0.55¬±0.04\n2.33¬±0.09\n0.43¬±0.03\n84.02¬±0.31\n2.49¬±0.13\n0.84¬±0.01\n2.61¬±0.06\n0.042¬±0.003\n61.02¬±0.09\nnecessary for modeling scores on low-density regions.\nSAHP\nTHP\nSMURF-THP\nCoverage\n0\n0.5\n1.0\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(a) StackOverflow\nCoverage\n0\n0.5\n1.0\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(b) Retweet\nSAHP\nTHP\nSMURF-THP\nCoverage\n0\n0.5\n1.0\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(c) MIMIC-II\nCoverage\n0\n0.5\n1.0\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(d) Financial\nFigure 1. Coverage of different confidence levels on four datasets.\nDistribution of Samples. We visualize the distribution of\nsamples generated by SMURF-THP for several events to\nstudy the predicted intensity and present them in Figure 3.\nA large proportion of the samples stay close to value 0,\nwhich is reasonable since most of the events occur within\na short time after the previous one. Distributions vary as\ntime goes further. Figure 3(c) and Figure 3(d) exhibit that\nSMURF-THP can still present tiny peaks around the ground\ntruth value, indicating that our model can still capture the\neffect of historical events. Yet, the generated samples may\nbe inaccurate due to the huge randomness around the large\nvalues of arrival time.\nSAHP\nTHP\nSMURF-THP\nInterval LengthŒî\n‚àí0.4\n‚àí0.2\n0\n0.2\n0.4\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(a) StackOverflow\nInterval LengthŒî\n0\n1\n2\n3\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(b) Retweet\nSAHP\nTHP\nSMURF-THP\nInterval LengthŒî\n0\n0.5\n1.0\n1.5\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(c) MIMIC-II\nInterval LengthŒî\n‚àí2\n0\n2\n4\nConÔ¨Ådence Level\n0\n0.5\n1.0\n(d) Financial\nFigure 2. Interval Length of different confidence levels on four\ndatasets. We minus the interval length of all methods with the\nlength of SMURF-THP (denoted as Interval Length‚àÜ) to better\nshow the differences.\n4.3. Ablation Study\nParametrization. In addition to parametrizing the inten-\nsity function, we can also parametrize the score function\ndirectly as was done in conventional score-based methods.\nThat is, we can mask the intensity function and directly\nparametrize the corresponding score function with the neu-\nral network. Table 3 summarizes the results on the Stack-\nOverflow and Retweet datasets, where SMURF-THPs in-\ndicates SMURF-THP with the score function parametrized.\nResults imply that parametrizing the intensity function fits\nthe sequence better than parametrizing the score function.\n7",
        "semantic_similarity": 0.10252058506011963,
        "keyword_overlap": 0.029420594416091263,
        "combined_score": 0.08059058786691112,
        "paragraph_id": 6,
        "source_title": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process"
      },
      {
        "text_fragment": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process\ndomains. Early work (Sahani et al., 2016) has adopted score\nmatching in modeling Poisson point process, but it fails\nto model complicated event dependencies induced in the\nmodern event data due to the simplified assumption that the\nintensity function is independent of the historical events.\nA suitable sampling algorithm for score matching-based\nmodels is Langevin Dynamics (LD), which can produce\nsamples from a probability density using only its score func-\ntion. Hsieh et al. (2018) propose Mirror Langevin Dynamics\n(MLD) as a variant of Langevin Dynamics that focuses on\nsampling from a constrained domain. We employs both LD\nand MLD for generating event samples in our experiments.\n3. Method\n3.1. Score Matching Objective of Hawkes Process\nLet S = {(ti, ki)}L\ni=1 denote an event sequence of length\nL, where each pair (ti, ki) corresponds to an event of type\nki ‚àà{1, ¬∑ ¬∑ ¬∑ , M} happened at time ti ‚àà[0, tmax]. Also, we\ndenote the history events up to time t as Ht = {(tj, kj) :\ntj < t}. Our goal is to learn pT,K(t, k | Ht), the joint\nconditional probability of the event proceeding time t given\nthe history Ht.\nTo employ score matching, we decompose the joint con-\nditional pdf pT,K by conditioning on the event time. By\ndoing so, the partial likelihood of the discrete event types\ncan be maximized by minimizing the cross entropy loss;\nand the marginal likelihood of the continuous event time\ncan be substituted by a score-matching objective. Such a\nsubstitution avoids the intractable integral of the intensity.\nSpecifically, we condition pT,K on the event time and we\nhave:\npT,K(t, k | Ht) = pT (t | Ht) ¬∑ pK | T (k | t, Ht).\nCorrespondingly, we have the log-likelihood:\n‚Ñì(S)=\nL\nX\ni=1\nlog pT (ti|Hti)+\nL\nX\ni=1\nlog pK|T (ki|ti, Hti). (3)\nWe can use a neural network to parameterize the intensity\nfunction and directly train the model using Eq. (3). How-\never, such an approach inevitably faces computational chal-\nlenges, i.e., exact computation of the intensity‚Äôs integral is\nintractable. Therefore, we derive a score-matching objective\nto substitute the first term in Eq. (3).\nIn SMURF-THP, we use a Transformer model with parame-\nters Œ∏ to parameterize the intensity function. More details\nare presented in Section 3.3. A sample‚Äôs score is defined as\nthe gradient of its log-density. Then, using Eq. (1), we can\nwrite the score of the i-th event given its history Hti and the\nmodel parameters Œ∏ as:\nœà(ti | Hti; Œ∏) = ‚àÇt log pT (ti | Hti; Œ∏)\n= ‚àÇt log Œª(ti | Hti; Œ∏) ‚àíŒª(ti | Hti; Œ∏).\nThe original objective of score matching is to minimize the\nexpected squared distance between the score of the model\nœà(¬∑; Œ∏) and the score of the ground truth œà‚àó(¬∑). However,\nminimizing such an objective is infeasible since it relies\non the unknown score œà‚àó(¬∑). We can resolve this issue by\nfollowing the general derivation in Hyv¬®arinen (2005) and\narrive at an empirical score-matching objective for Hawkes\nprocess with single type:\nbJ(Œ∏) =\nL\nX\ni=1\n[1\n2œà(ti | Hti; Œ∏)2 + ‚àÇtœà(ti | Hti; Œ∏)],\n(4)\nwhere ‚àÇtœà(ti | Hti; Œ∏) = ‚àÇ2\nt log Œª(ti | Hti; Œ∏) ‚àí‚àÇtŒª(ti |\nHti; Œ∏). We state in the follow theorem that the score match-\ning objective in Eq. (4) satisfies local consistency: minimiz-\ning bJ(Œ∏) is as sufficient as maximizing the first term of Eq.\n(3) for estimating the model parameters.\nTheorem 3.1. Assume the event time in sequence S fol-\nlows the model: p‚àó\nT (t | Ht) = pT (t | Ht; Œ∏‚àó) for some Œ∏‚àó,\nand that no other parameter gives a pdf that is equal 1 to\npT (¬∑; Œ∏‚àó). Assume further that the optimization algorithm is\nable to find the global minimum and pT (t | Ht; Œ∏) is positive\nfor all t and Œ∏. Then the score matching estimator obtained\nby minimizing Eq. (4) is consistent, i.e., it converges in prob-\nability towards Œ∏‚àówhen sample size approaches infinity.\nProof. Let œà‚àó(¬∑) and œà(¬∑; Œ∏) be the associated score func-\ntion of p‚àó\nT (¬∑) and pT (¬∑; Œ∏), respectively. The objective in Eq.\n(4) is an empirical estimator of the following objective:\nJ(Œ∏)= 1\n2\n* L\nX\ni=1\n(œà(ti | Hti; Œ∏) ‚àíœà‚àó(ti | Hti))2\n+\n{ti}L\ni=1‚àºp‚àó\nT\n.\nWe first prove that J(Œ∏) = 0 ‚áíŒ∏ = Œ∏‚àó. Since p‚àó\nT (¬∑) is\npositive, we can infer from J(Œ∏‚Ä≤) = 0 that œà(¬∑; Œ∏‚Ä≤) and\nœà‚àó(¬∑) are equal, which implies log p‚àó\nT (¬∑) = log pT (¬∑; Œ∏‚Ä≤)+c\nfor some constant c. Because both p‚àó\nT and pT are pdf‚Äôs,\nthe constant c must be 0 and hence we have p‚àó\nT = pT .\nBy assumption, Œ∏‚àóis the only parameter that fulfills this\nequation, so necessarily Œ∏‚Ä≤ = Œ∏‚àó.\nThen according to the law of large numbers, bJ converges\nto J as the sample size approaches infinity. Thus, the esti-\nmator converges to a point where J is globally minimized.\n1Equality of pdf‚Äôs are taken in the sense of equal almost every-\nwhere with respect to the Lebesgue measure.\n3",
        "semantic_similarity": 0.09065592288970947,
        "keyword_overlap": 0.032621951219512196,
        "combined_score": 0.07324573138865029,
        "paragraph_id": 2,
        "source_title": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process"
      }
    ],
    "resolved_title": "SMURF-THP: Score Matching-based UnceRtainty quantiFication for Transformer Hawkes Process"
  },
  {
    "citation_marker": "[13]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.46, overlap: 0.02): **NO**  \n\n**Reason:** The statement describes \"ore matching-based uncertainty quantification for transformer Hawkes process,\" but the cited paragraph exclusively discusses **2DFQ (Two-Dimensional Fair Queuing)**, a scheduling algorithm for multi-tenant cloud services. The cited content focuses on challenges in fair resource allocation (e.g., burstiness, latency reduction) and makes no mention of:  \n- Hawkes processes,  \n- Uncertainty quantification,  \n- \"Transformer\" models, or  \n- Any machine learning methodology.  \n\nThe statement thus **fabricates a connection** to unrelated content in the cited source.  \n\n---\n**Key mismatches:**  \n1. **Topic:** The statement addresses ML/uncertainty quantification; the cited work addresses cloud resource scheduling.  \n2. **Terminology:** \"2DFQ,\" \"fair queuing,\" and \"multi-tenant cloud services\" in the cited text have no semantic alignment with the statement‚Äôs claims.  \n3. **Content:** The cited paragraph does not reference Hawkes processes, transformers, or uncertainty quantification.  \n\nThis appears to be a **citation error** (e.g., incorrect reference tag or source confusion).",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.44, overlap: 0.01): **NO**  \nThe statement is **not supported** by the cited paragraph.  \n\n**Reason:**  \nThe cited paragraph from the 2DFQ paper discusses cloud service scheduling algorithms (WFQE, WF¬≤QE, 2DFQE), tenant workload isolation, and request cost estimation. It contains **no mention** of \"ore matching,\" \"uncertainty quantification,\" \"transformer Hawkes process,\" or any related machine learning concepts referenced in the statement. The content is exclusively focused on fair queuing techniques for multi-tenant cloud systems.  \n\nThus, the statement **misattributes** unrelated content to the cited source.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.43, overlap: 0.01): **NO**  \n\n**Reason:** The statement from the article describes \"ore matching-based uncertainty quantification for transformer Hawkes process,\" but the cited document ([13]) exclusively discusses **2DFQ (Two-Dimensional Fair Queuing)**, a scheduling algorithm for cloud services. The cited paragraph details experiments on latency, tenant predictability, and comparisons between schedulers (WFQE, WF2QE, 2DFQE). There is **no mention** of \"transformer Hawkes process,\" \"uncertainty quantification,\" or \"ore matching\" in the cited content. The article's statement appears to misattribute an unrelated topic to this source.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "2DFQ: Two-Dimensional Fair Queuing for\nMulti-Tenant Cloud Services\nJonathan Mace1, Peter Bodik2, Madanlal Musuvathi2, Rodrigo Fonseca1,\nKrishnan Varadarajan2\n1Brown University, 2Microsoft\nABSTRACT\nIn many important cloud services, diÔ¨Äerent tenants execute\ntheir requests in the thread pool of the same process, requiring\nfair sharing of resources. However, using fair queue schedulers\nto provide fairness in this context is diÔ¨Écult because of high ex-\necution concurrency, and because request costs are unknown\nand have high variance. Using fair schedulers like WFQ and\nWF2Q in such settings leads to bursty schedules, where large re-\nquests block small ones for long periods of time. In this paper,\nwe propose Two-Dimensional Fair Queuing (2DFQ), which\nspreads requests of diÔ¨Äerent costs across diÔ¨Äerent threads and\nminimizes the impact of tenants with unpredictable requests.\nIn evaluation on production workloads from Azure Storage,\na large-scale cloud system at Microsof, we show that 2DFQ\nreduces the burstiness of service by 1-2 orders of magnitude.\nOn workloads where many large requests compete with small\nones, 2DFQ improves 99th percentile latencies by up to 2\norders of magnitude.\nCCS Concepts\n‚Ä¢Networks ‚ÜíCloud computing; Packet scheduling;\n‚Ä¢Computer systems organization ‚ÜíAvailability;\nKeywords\nFair Request Scheduling; Multi-Tenant Systems\n1.\nINTRODUCTION\nMany important distributed systems and cloud services exe-\ncute requests of multiple tenants simultaneously. Tese include\nstorage, conÔ¨Åguration management, database, queuing, and\nco-ordination services, such as Azure Storage [9], Amazon\nDynamo [16], HDFS [53], ZooKeeper [36], and many more.\nIn this context, it is crucial to provide resource isolation to\nensure that a single tenant cannot get more than its fair share\nof resources, to prevent aggressive tenants or unpredictable\nworkloads from causing starvation, high latencies, or reduced\nPermission to make digital or hard copies of all or part of this work for personal\nor classroom use is granted without fee provided that copies are not made or\ndistributed for proÔ¨Åt or commercial advantage and that copies bear this notice\nand the full citation on the Ô¨Årst page. Copyrights for components of this work\nowned by others than the author(s) must be honored. Abstracting with credit is\npermitted. To copy otherwise, or republish, to post on servers or to redistribute to\nlists, requires prior speciÔ¨Åc permission and/or a fee. Request permissions from\npermissions@acm.org.\nSIGCOMM ‚Äô16, August 22 - 26, 2016, Florianopolis , Brazil\n¬© 2016 Copyright held by the owner/author(s). Publication rights licensed to\nACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00\nDOI: http://dx.doi.org/10.1145/2934872.2934878\nthroughput for others. Systems in the past have suÔ¨Äered cas-\ncading failures [19,27], slowdown [14,20,27,28,33], and even\ncluster-wide outages [14,19,27] due to aggressive tenants and\ninsuÔ¨Écient resource isolation.\nHowever, it is diÔ¨Écult to provide isolation in these sys-\ntems because multiple tenants execute within the same process.\nConsider the HDFS NameNode process, which maintains\nmetadata related to locations of blocks in HDFS. Users invoke\nvarious APIs on the NameNode to create, rename, or delete\nÔ¨Åles, create or list directories, or look up Ô¨Åle block locations.\nAs in most shared systems, requests to the NameNode wait\nin an admission queue and are processed in FIFO order by a\nset of worker threads. In this setting tenant requests contend\nfor resources, such as CPU, disks, or even locks, from within\nthe shared process. As a result, traditional resource manage-\nment mechanisms in the operating system and hypervisor\nare unsuitable for providing resource isolation because of a\nmismatch in the management granularity.\nIn many domains, resource isolation is implemented using\na fair queue scheduler, which provides alternating service to\ncompeting tenants and achieves a fair allocation of resources\nover time. Fair schedulers such as Weighted Fair Queuing [46],\nwhich were originally studied in the context of packet schedul-\ning, can be applied to shared processes since the setting is\nsimilar: multiple tenants submit Ô¨Çows of short-lived requests\nthat are queued and eventually processed by a server of lim-\nited capacity. However, in shared processes there are three\nadditional challenges that must be addressed:\n‚óèResource concurrency: Tread pools in shared processes\nexecute many requests concurrently, ofen tens or even\nhundreds, whereas packet schedulers are only designed for\nsequential execution of requests (i.e. on a network link);\n‚óèLarge cost variance: Request costs vary by at least 4 orders\nof magnitude across diÔ¨Äerent tenants and API types, from\nsub-millisecond to many seconds. By contrast, network\npackets only vary in length by up to 1.5 orders of mag-\nnitude (between 40 and 1500 bytes). Unlike CPU thread\nschedulers, requests are not preemptible by the application;\n‚óèUnknown and unpredictable resource costs: Te execu-\ntion time and resource requirements of a request are not\nknown at schedule time, are diÔ¨Écult to estimate up front,\nand vary substantially based on API type, arguments, and\ntransient system state (e.g., caches). By contrast, the length\nof each network packet is known a priori and many packet\nschedulers rely on this information.",
        "semantic_similarity": 0.4648873209953308,
        "keyword_overlap": 0.017980107115531753,
        "combined_score": 0.33081515683139107,
        "paragraph_id": 0,
        "source_title": "2DFQ: Two-Dimensional Fair Queuing for Multi-Tenant Cloud Services"
      },
      {
        "text_fragment": "WFQE\nWF¬≤QE\n2DFQE\n0\n100\n200\n300\n0\n5\n10\n15\nx103\n0\n100\n200\n300\n0\n5\n10\n15\nx103\nx103\nService Received\n0\n100\n200\n300\n0\n5\n10\n15\nx103\nx103\nx103\nTime (s)\n(a) Service received by T1, a predictable tenant submitting small requests. Each\nÔ¨Ågure illustrates the service received by T1 under WFQE, WF2QE, and 2DFQE.\n2DFQ continues to provide stable service as the workload mix becomes unpre-\ndictable, with only a minor increase in oscillations.\n0123456789\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n0%\n0123456789\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n33%\n0\n5\n10\n15\nTime (s)\n0123456789\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n66%\n100\n103\n104\n105\n106\n107\nRequest Size\n(b) Tread occupancy over time for 2DFQE for increasingly unpredictable\nworkloads. Horizontal lines represent worker threads; Ô¨Åll color indicates cost\nof the running request at each instant in time. 2DFQE isolates predictable, small\nrequests even as increasingly many tenants are unpredictable.\nFigure 11: Time series as the overall server workload mix comprises 0% (top), 33% (middle), and 66% (bottom) unpredictable tenants.\naggregate, while 2DFQ and WF2Q are comparable. Figure 9b\nillustrates sizes of requests running on threads during the\nexperiment. Service spikes under WF2Q correlate with sev-\neral large requests occupying threads simultaneously. 2DFQ\npartitions requests across threads according to request size,\navoiding such spikes. Figure 10 plots a CDF of the service lag\nstandard deviations across all tenants included in the experi-\nment. A low standard deviation is desirable, as it corresponds\nto fewer oscillations in service. Te Ô¨Ågure shows that the Ô¨Årst\nquartile of tenants have approximately 50x lower standard\ndeviation under 2DFQ than WF2Q and 100x lower standard\ndeviation under 2DFQ than WFQ. Tese tenants are the ones\nwith primarily small requests.\nTo more precisely understand how 2DFQ impacts tenants\nbased on request sizes, we repeat the experiment and include\nan additional seven tenants, t1 . . . t7. Tese tenants submit\nrequests with Ô¨Åxed costs of 28, 210, 212, . . . , 220 respectively\n(from 256 to 1 million), spanning the range of costs in our\nworkload. Figure 10 (right) plots the distribution of service lag\nexperienced by t1 . . . t7 under WFQ, WF2Q and 2DFQ. Each\ndistribution shows how much the tenant deviates from its\nfair share. Under all schedulers, large requests (t7) experience\na wide range of service lag, because service is received in\nlarge, coarse-grained bursts. For progressively smaller requests\n(t6 . . . t1), WFQ reduces service lag to a range of 0.8 seconds;\nWF2Q reduces it to 0.5 seconds, while 2DFQ reduces it to\n0.01 seconds. Tese results illustrate how 2DFQ particularly\nimproves the service received by tenants with small requests.\n6.2\nUnknown Request Costs\nOur second set of experiments evaluates schedulers when\nrequest costs are not known a priori. We compare 2DFQE\n(Œ± = 0.99) to variants of WFQ and WF2Q that estimate re-\nquest costs using per-tenant per-API exponential moving av-\nerages (Œ± = 0.99). We refer to them, respectively, as WFQE\nand WF2QE. We also implemented both retroactive charging\nand refresh charging for WFQE and WF2QE. Without these\ntechniques, we found that the quality of schedules deteriorated\nby a surprising amount. It turned out to be relatively common\nfor workloads to have back-to-back requests that diÔ¨Äer by sev-\neral orders of magnitude; without retroactive charging it takes\ntoo long to incorporate measurements back into the moving\naverage to rectify estimation error. For the same reason, with-\nout refresh charging it would quickly lead to multiple large\nrequests taking over the thread pool. Since the bookkeeping\ntechniques are straightforward to implement, we applied them\nto all algorithms, and our experiment results only reÔ¨Çect the\ndiÔ¨Äerences between scheduling logic and estimation strategy.\nWe Ô¨Årst evaluate 2DFQE for workloads where an increasing\nproportion of the tenants are unpredictable, comparing to\nservice under WFQE and WF2QE. Second, we compare the\nschedulers across a suite of 150 workloads generated from\nproduction workload traces, and assess the overall eÔ¨Äect on\nservice lag and request latency.\n6.2.1\nUnpredictable Workloads\nIn this experiment we evaluate 2DFQE‚Äôs pessimistic cost\nestimation strategy, demonstrating how it co-locates unpre-\ndictable and expensive tenants, keeping them away from pre-\ndictable tenants with small requests. We examine a single\nworkload in detail; in ¬ß6.2.2 we give aggregate results across\na suite of experiments. We show that 2DFQE improves ser-\nvice for those tenants compared to WFQE and WF2QE, which\ndeteriorate under the same conditions.\nWe examine a workload of 300 randomly selected tenants\nplus T1 . . .T12 as in ¬ß6.1.2. We repeat the experiment three\ntimes. Initially, most tenants in the workload are predictable,\nand 2DFQE provides little, if any, improvement over WFQE",
        "semantic_similarity": 0.44032883644104004,
        "keyword_overlap": 0.006795016987542469,
        "combined_score": 0.31026869060499074,
        "paragraph_id": 10,
        "source_title": "2DFQ: Two-Dimensional Fair Queuing for Multi-Tenant Cloud Services"
      },
      {
        "text_fragment": "latencies converge towards the latency of the most expensive\nrequests in the system.\nFigure 12 (bottom lef) plots CDFs of the service lag stan-\ndard deviation across all tenants. It shows the successive in-\ncrease in the proportion of tenants with high standard devi-\nation ‚Äì this corresponds to the unpredictable tenants. Te\nremaining predictable tenants experience approximately 10 to\n15√ó reduced standard deviation under 2DFQE compared to\nWFQE and WF2QE.\n6.2.2\nProduction Workloads\nFinally, we run a suite of 150 experiments derived from pro-\nduction workloads of Azure Storage. We simulate the service\nreceived by tenants under WFQE, WF2QE, and 2DFQE, as\nwe randomly vary several parameters: the number of worker\nthreads (2 to 64); the number of tenants to replay (0 to 400);\nthe replay speed (0.5-4√ó); the number of continuously back-\nlogged tenants (0 to 100); the number of artiÔ¨Åcially expensive\ntenants (0 to 100); and the number of unpredictable tenants\n(0 to 100). To compare between experiments, we also include\nT1 . . .T12.\nWe measure the 99th percentile latency of tenants in each\nexperiment and calculate the relative speedup of 2DFQE com-\npared to WFQE and WF2QE. As an example, in the ‚Äú0% Unpre-\ndictable‚Äù experiment of ¬ß6.2.1, T1‚Äôs 99th percentile latency was\n3.3ms under 2DFQE, 4.5ms under WFQE, and 28ms under\nWF2QE, giving 2DFQE a speedup of 1.4√ó over WFQE and 8.5√ó\nover WF2QE.\nFigure 13a (lef) plots the distribution of 2DFQE‚Äôs speedup\nover WFQE and WF2QE. Across the experiments, 2DFQE sig-\nniÔ¨Åcantly improves 99th percentile latency for tenants such\nas T1, whose requests are small and predictable (illustrated in\n¬ß2). T1 has a median improvement of 3.8√ó over WFQE and\n142√ó over WF2QE. However, 2DFQE does not improve 99th\npercentile latency as much for tenants with large and/or un-\npredictable requests, such as T10 and T12. Figure 13a (right)\nplots the 99th percentile latencies across all experiments for\nT10, comparing 2DFQE to WFQE (top) and WF2QE (bottom).\n2DFQE resulted in worse 99th percentile latency for T10 in 64\nof the experiments for WFQE and 47 for WF2QE. However,\nwhen 2DFQE did improve latencies for T10, it was by signif-\nicantly larger factors (up to 61√ó) than when latencies were\nworse (up to 5√ó).\nTo better understand how 2DFQE improves latencies for\ntenants with smaller requests, we repeat the experiment suite\nto include the Ô¨Åxed-cost tenants t1 . . . t7 as described in ¬ß6.1.2.\nFigure 13b plots the distribution of 2DFQE‚Äôs speedup over\nWFQE and WF2QE for t1 . . . t7, and illustrates how latency\nis primarily improved for tenants with small requests (t1).\nConversely, tenants with very expensive requests such as t7\nand T12 see little, if any improvement.\nOverall, the experiments where WFQE and WFQE per-\nformed best correlated with low numbers of both unpredictable\nand expensive tenants, for which request cost estimates were\naccurate and there was little chance for thread pool blocking.\nOn the other hand, the experiments where 2DFQE performed\nbest correlated with high numbers of either unpredictable\nor expensive tenants, with most speedups occurring when\n-100x\n-10x\n0\n10x\n100x\n1000x\nT1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11 T12\nWFQE\n2DFQE 99% Speedup\n0.1\n1\n10\n100\n0.1\n1\n10\n100\nT10\nWFQE  99% Latency (s)\n2DFQE  99% Latency (s)\n-100x\n-10x\n0\n10x\n100x\n1000x\nT1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11 T12\nWF¬≤QE\n2DFQE 99% Speedup\n0.1\n1\n10\n100\n0.1\n1\n10\n100\nT10\nWF¬≤QE  99% Latency (s)\n2DFQE  99% Latency (s)\n(a) 99th percentile latency speedups for T1 . . .T12 comparing 2DFQE to\nWFQE (top row) and WF2QE (bottom row). Scatter plots show T10‚Äôs 99th\npercentile latencies in detail, each point representing one experiment.\n-100x\n-10x\n0\n10x\n100x\n1000x\nt1\nt2\nt3\nt4\nt5\nt6\nt7\n2DFQE 99% Speedup\nWFQ E\n-100x\n-10x\n0\n10x\n100x\n1000x\nt1\nt2\nt3\nt4\nt5\nt6\nt7\nE\n2DFQE 99% Speedup\nWF¬≤QE\n(b) Results for t1 . . . t7 from repeated experiment suite.\nFigure 13: Comparison of 99th percentile latencies across 150 experiments\nbased on production workloads. Whiskers extend to min and max values.\nboth were true. WFQE and WF2QE rapidly deteriorate as\nthe workload becomes unpredictable, increasing the relative\nimprovement of 2DFQE. For example, in ¬ß6.2.1, the initial\nworkload (0% Unpredictable) was more predictable than a\ntypical workload, and 2DFQ only improved 99th percentile\nlatency for T1 by a factor of approximately 1.5√ó over WFQE.\nOn the other hand, the Ô¨Ånal workload (66% Unpredictable)\nwas less predictable than a typical workload, and T1‚Äôs 99th\npercentile latency improvement over WFQE was more than\n100√ó. Te suite of 150 experiments presented here similarly\nvary in how predictable they are, and the range of 99th per-\ncentile latency speedups illustrated in Figure 13a (lef) reÔ¨Çect\nthis range.\n7.\nDISCUSSION\n2DFQ Intuition 2DFQ improves service compared to WFQ\nand WF2Q primarily due to the manageable mix of predictable,\nunpredictable, cheap and expensive requests in real workloads.\nConsider the illustration in Figure 14. At one extreme we have\npredictable workloads with low variation in request cost (¬å).\nTis scenario is representative of packet scheduling on net-\nwork links, and WFQ, WF2Q and 2DFQ would provide simi-\nlarly good quality of service because little, if any, blocking can\noccur. At the other extreme lies workloads with hugely varying\nrequest costs and completely unpredictable tenants (¬ç). In this\nscenario, all schedulers would behave poorly because blocking\nwould be unavoidable, even for 2DFQ. However, starting from",
        "semantic_similarity": 0.42639321088790894,
        "keyword_overlap": 0.005466347796378545,
        "combined_score": 0.3001151519604498,
        "paragraph_id": 12,
        "source_title": "2DFQ: Two-Dimensional Fair Queuing for Multi-Tenant Cloud Services"
      }
    ],
    "resolved_title": "2DFQ: Two-Dimensional Fair Queuing for Multi-Tenant Cloud Services"
  },
  {
    "citation_marker": "[14]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: **Answer:** YES  \n**Reason:** The article's citation of Mellor-Crummey and Scott (1991) is accurately supported by the provided paragraph. The cited paragraph explicitly lists the publication as:  \n> *\"ACM Transactions on Computer Systems, Vol. 9, No. 1, February 1991\"*  \nThis matches the article's statement:  \n> *\"ACM Trans. Comput. Syst. 9, 1 (Feb. 1991), 21‚Äì65\"*  \nNo distortion or fabrication is present. The citation details (journal, volume, issue, date, and page range) align exactly."
    ],
    "evidence": [
      {
        "text_fragment": "Algorithms\nfor Scalable\nSynchronizahon\non Shared\n‚Äî Memory\nMultiprocessors\n.\n65\n31.\nMAE~AWA,\nM,\nA\n~\nalgorithm\nfor\nmutual\nexclusion\nin\ndecentralized\nsystems.\nACM\nTrans.\nComput.\nSyst.\n3, 2 (May\n1985),\n145-159,\n32.\nMELLOR-CRUMMEY,\nJ.\nM.\nConcurrent\nqueues:\nPractical\nfetch-and-@\nalgorithms,\nTech.\nRep,\n229,\nComputer\nScience\nDept,,\nUniv.\nof Rochester,\nNov.\n1987.\n33.\nMELLOR-CRUMMEY,\nJ.\nM.,\nAND\nScorn,\nM.\nL.\nAlgorithms\nfor\nscalable\nsynchronization\non\nshared-memory\nmultiprocessors.\nTech.\nRep.\n342,\nComputer\nScience\nDept,,\nUniv.\nof Rochester,\nApr.\n1990.\nAlso\nCOMP\nTR90-114,\nDept.\nof Computer\nScience,\nRice\nUniv.,\nMay\n1990\n34.\nOUSTERHOUT,\nJ.\nK\n, SCELZA,\nD,\nA.,\nAND\nSINDHU,\nP.\nS,\nMeciusa:\nAn\nexperiment\nin\ndis-\ntributed\noperating\nsystem\nstructure.\nCornmun.\nACM\n23,\n2 (Feb.\n1980),\n92-105.\n35,\nP1596\nWorking\nGroup\nof the\nIEEE\nComputer\nSociety\nMicroprocessor\nStandards\nCommittee.\nSCI\n(scalable\ncoherent\ninterface):\nA.n overview\nof extended\ncache-coherence\nprotocols,\nFeb\n5,\n1990.\nDraft\n0.59\nP1596/Part\nIII-D.\n36.\nPETERSON,\nG,\nL.\nA\nnew\nsolution\nto\nLamport‚Äôs\nconcurrent\nprogramming\nproblem\nusing\nsmall\nshared\nvariables.\nACM\nTrans.\nProgram.\nLang,\nSyst.\n5, 1 [Jan,\n1983),\n56-65.\n37.\nPFISTER, G,, BRANTLEY,\nW.\nC., GEORGE, D. A,,\nHARVEY,\nS. L.,\nKLEINFELDER,\nW. J , MCAVLIFF~,\nK.\nP.,\nMELTON,\nT\nA.,\nNORTON,\nV\nA\n, AND WEISS,\nJ.\nThe\nIBM\nresearch\nparallel\nprocessor\nprototype\n(RP3):\nIntroduction\nand\narchitecture.\nIn\nProceedings\nof\nthe\n1985\nInternational\nConference\non Parallel\nProcessing\n(Aug.\n1985),\n764-771.\n38.\nPFISTER,\nG.,\nAND\nNORTON,\nV.\nA.\n‚ÄúHot\nspot‚Äù\ncontention\nand\ncombining\nin\nmultistage\ninterconnection\nnetworks.\nIEEE\nTrans.\nComput,\nC-34,\n10 (Ott,\n1985),\n943-948.\n39.\nRAYMOND,\nK.\nA tree-based\nalgorithm\nfor\ndistributed\nmutual\nexclusion.\nACM\nTrans.\nCom -\nput.\nSyst\n7, 1 (Feb.\n1989),\n61-77.\n40.\nRAYNAL,\nM.\nAlgorithms\nfor\nMutual\nExcluszon.\nMIT\nPress\nSeries\nin\nScientific\nComputation.\nMIT\nPress,\nCambridge,\nMass,,\n1986,\nTranslated\nfrom\nthe\nFrench\nby\nD.\nBeeson.\n41.\nREED,\nD.\nP.,\nAND\nKANODIA,\nR\nK.\nSynchronization\nwith\neventcounts\nand\nsequencers\nCommun.\nACM\n22,\n2 (Feb.\n1979),\n115-123.\n42,\nRETTBERG,\nR.\nD.,\nCROWJT~ER, W,\nR.,\nCARVEY,\nP.\nP.,\nAND TOMLINSON,\nR.\nS.\nThe\nMonarch\nparallel\nprocessor\nhardware\ndesign,\nComputer\n23,\n4 (Apr\n1990),\n18-30\n43.\nRIGART,\nG.,\nAND AGRAWALA,\nA K.\nAn\noptimal\nalgorithm\nfor\nmutual\nexclusion\nin\ncomputer\nnetworks.\nCommzm\nACM\n24, 1 (Jan\n1981)\n9-17.\n44.\nRUDOLPH,\nL.,\nAND\nSEGALL,\nZ.\nDynamic\ndecentralized\ncache\nschemes\nfor\nMIMD\nparallel\nprocessors.\nIn\nProceedings\nof the International\nSymposium\non Computer\nArchitecture\n(1984),\n340-347.\n45.\nSANDERS,\nEt,\nA.\nThe\ninformation\nstructure\nof\ndistributed\nmutual\nexclusion\nalgorithms.\nACM\nTrans.\nComput.\nSyst.\n5, 3 (Aug.\n1987),\n284-299\n46.\nSCHNEIDER,\nF.\nB.\nSynchronization\nin\ndistributed\nprograms,\nACM\nTrans.\nProgram\nLang.\nSyst.\n4, 2 (Apr.\n1982),\n179-195.\n47,\nSCOTT,\nM.\nL.,\nLEBLANC,\nT.\nJ.,\nAND\nMARSH,\nB.\nD.\nMulti-model\nparallel\nprogramming\nin\nPsyche.\nIn\nProceedings\nof the Second\nACM\nSymposium\non Principles\nand\nPractice\nof Parallel\nProgramming\n(Mar.\n1990),\n70-78.\n48.\nTANG,\nP.,\nAND\nYEW,\nP.-C,\nProcessor\nself-scheduling\nfor\nmultiple-nested\nparallel\nloops,\nIn\nProceedings\nof the 1986\nInternational\nConference\non Parallel\nProcessing\n(Aug\n1986),\n528-535.\n49.\nTANG,\nP.,\nAND\nYEW,\nP.-C.\nAlgorithms\nfor\ndistributing\nhot-spot\naddressing\nCSRD\nRep.\n617,\nCenter\nfor\nSupercomputing\nResearch\nand\nDevelopment,\nUniv.\nof\nIllinois\nat\nUrbana-\nChampaign,\nJan.\n1987.\n50.\nYEW,\nP.-C.\nArchitecture\nof the\nCedar\nparallel\nsupercomputer\nCSRD\nRep\n609,\nCenter\nfor\nSupercomputing\nResearch\nand\nDevelopment,\nUniv.\nof Illinois\nat\nUrbana-Champaign,\nAug.\n1986.\n51.\nYEW,\nP.-C.,\nTZENG, N.-F.,\nAND LAWRIE,\nD. H.\nDistributing\nhot-spot\naddressing\nin\nlarge-scale\nmultiprocessors.\nIEEE\nTrans.\nComput.\nC-36,\n4 (Apr.\n1987),\n388-395.\n52,\nZAHORIAN,\nJ.,\nLAZOWSKA,\nE.\nD.,\nAND\nEAGER,\nD,\nl..\nThe\neffect\nof\nscheduling\ndiscipline\non\nspin\noverhead\nin\nshared\nmemory\nparallel\nprocessors.\nTech.\nRep.\nTR-89-07-03,\nComputer\nScience\nDept.,\nUniv.\nof Washington,\nJuly\n1989.\nReceived\nJune\n1990;\nrevised\nJanuary\n1991;\naccepted\nJanuary\n1991\nACM\nTransactions\non Computer\nSystems,\nVol.\n9, No.\n1, February\n1991.",
        "semantic_similarity": 0.6820339858531952,
        "keyword_overlap": 0.029312830370014416,
        "combined_score": 0.48621763920824096,
        "paragraph_id": 44,
        "source_title": "Algorithms for scalable synchronization on shared-memory multiprocessors"
      }
    ],
    "resolved_title": "Algorithms for scalable synchronization on shared-memory multiprocessors"
  },
  {
    "citation_marker": "[15]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.52, overlap: 0.01): **NO**  \nThe statement in the article misrepresents the cited source ([15]).  \n\n**Reason:**  \nThe cited paragraph from Park and Eom‚Äôs work discusses **conventional resource scheduling** (e.g., CPU, I/O, page cache) and its limitations in multi-user systems, such as:  \n- Performance unfairness between users.  \n- Lack of prioritization for foreground applications.  \n- No mention of **intraprocess locks**, **priority inversion**, or **lock-based scheduling failures**.  \n\nThe article incorrectly attributes issues of lock unfairness and priority inversion to [15], which focuses on user-level resource allocation and cache management, not locks. This constitutes a distortion of the cited source.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.50, overlap: 0.02): **NO**  \nThe cited paragraph from Park and Eom (reference [15]) does **not** support the article's statement about intraprocess locks, priority inversion, or performance optimization failures. Here's why:  \n\n1. **Scope Mismatch**:  \n   The article discusses **intraprocess locks** causing priority inversion and undermining thread-priority optimizations.  \n   The cited paragraph describes **user-level CPU/I/O scheduling** (URS) for multi-user systems, focusing on \"vruntime,\" \"share\" values, and fairness/isolation among *users* (not threads or locks).  \n\n2. **No Mention of Locks or Priority Inversion**:  \n   Park and Eom's text exclusively details URS's hierarchical scheduling (user groups ‚Üí session groups ‚Üí processes). It never addresses:  \n   - Locks (intraprocess or otherwise),  \n   - Priority inversion,  \n   - Thread-priority optimizations,  \n   - Page faults or address-space protection.  \n\n3. **Irrelevant Citation**:  \n   The article implies [15] supports its claim about locks disrupting priority-based optimizations. However, the cited material discusses resource allocation *among users*, not thread synchronization or lock fairness.  \n\n**Conclusion**: The article misattributes its claim to [15]. The cited content does not discuss locks, priority inversion, or the described failure mechanism, making the citation unsupported and potentially distorted.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.48, overlap: 0.02): **NO**  \n\n**Reason:** The cited paragraph from [15] discusses *user-based resource scheduling* (URS) focusing on prioritizing foreground applications via page cache management and I/O scheduling. It does not mention **intraprocess locks**, **priority inversion**, or **lock fairness** at all. The article's statement incorrectly attributes these concepts to [15], fabricating a connection unsupported by the source.  \n\n**Key Discrepancies:**  \n1. **Scope Mismatch**: [15] addresses user-level resource allocation (CPU/I/O/page cache), not low-level locking mechanisms.  \n2. **Absence of Key Terms**: Terms like \"locks,\" \"priority inversion,\" or \"FIFO-based locks\" do not appear in the cited text.  \n3. **Misattributed Mechanism**: The performance optimizations in [15] involve user-group weights and cache prioritization, not thread-priority adjustments related to lock fairness.  \n\nThe citation [15] is used inaccurately to support claims about locks, constituting distortion.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "428\nIEEE TRANSACTIONS ON CONSUMER ELECTRONICS, VOL. 65, NO. 3, AUGUST 2019\nFig. 2.\nThe conventional page cache management.\nLike this, conventional resource scheduling schemes dis-\ntribute CPU and I/O resources only based on processes\nor sessions, without considering users. These conventional\nscheduling schemes can incur various problems in multi-\nuser surface computing systems. First, a user that runs more\nprocesses or sessions will use more system resources than\nother users, resulting in performance unfairness among the\nusers. Second, when a malicious user run a large number of\nprocesses, the entire system can halt because the malicious\nuser monopolizes almost all the system resources. Hence, con-\nventional scheduling schemes has a lack of consideration for\nmulti-user surface computing systems. To solve these prob-\nlems, a new resource scheduling scheme for multi-user surface\ncomputing systems need to be developed.\nC. Conventional Page Cache Management\nThe page cache is a specialized space residing in main\nmemory to bridge the performance gap between main memory\nand secondary storage. Since secondary storage is much slower\nthan main memory, issuing I/Os to the storage can severely\nincrease the latencies. To compensate for the low performance\nof the storage, the page cache retains frequently or recently\naccessed data in main memory. When the upcoming I/Os\naccess the cached pages, the I/Os are processed in the page\ncache without data transmission from/to the storage.\nAs depicted in Fig. 2, the modern page cache maintains two\ndifferent LRU queues, called the active list and the inactive list,\nto efÔ¨Åciently manage pages in the page cache [17]. (A) When\na new page is allocated in the page cache, it is Ô¨Årst inserted into\nthe inactive list. (B) After the page is accessed several times\n(by default, more than twice), it is promoted to the active\nlist. The eviction process is opposite to the allocation pro-\ncess. (C) When the system has insufÔ¨Åcient free pages in main\nmemory, the page cache management de-allocates pages at the\ntail of the inactive list to reclaim free pages. Additionally, (D)\nit demotes pages at the tail of the active list to the inactive\nlist, in order to balance the size of the lists.\nThe conventional page cache management does not distin-\nguish pages used by a foreground application from those used\nby background applications. Thus, the latency of a foreground\napplication can be increased, diminishing the quality of user\nexperience. In multi-user surface computing systems, there can\nbe multiple foreground applications, and therefore, a new page\ncache management scheme that prioritizes multiple foreground\napplications is necessary to enhance multi-user experiences.\nIII. DESIGN AND IMPLEMENTATION\nAs mentioned in Section II, conventional resource schedul-\ning, including CPU, I/O, and the page cache, has a lack of\nconsideration for the features of multi-user surface computing\nsystems. Therefore, conventional resource scheduling schemes\ncan cause severe degradation of user experience in multi-user\nsurface computing systems.\nIn this paper, we present a user-based resource scheduling\nscheme for multi-user surface computing systems, called URS,\nthat improves multi-user experience by providing performance\nfairness and isolation, and prioritizing each user‚Äôs foreground\napplication. For user-based scheduling and performance isola-\ntion among users, URS creates an individual resource group\nfor each user, called a user group, and inserts each applica-\ntion to its corresponding user group. To prioritize foreground\napplications, URS detects pages that are used by the fore-\nground applications, and tries to keep the pages in the page\ncache longer. We implement the user group using the Cgroups\nimplementation, which controls resources based on groups and\nprovides isolation among the groups. The details of Cgroups\nwill be discussed in Section V.\nA. Constructing User Groups for CPU and I/O Resources\nURS creates an individual user group per user for CPU\nand I/O resources, called ‚ÄúUser Task Group‚Äù and ‚ÄúUser CFQ\nGroup‚Äù, respectively. To create a user group for each user,\nURS needs to distinguish the ownership of applications. There\nhave been several research results [18], [19], [20] related to\nhow to pair applications and their owners. Since the scope\nof our paper does not include this subject, we suppose that\nusers log in to the system before they use it, and thus the\ncurrent version of URS simply utilizes the ‚Äòuid‚Äô (user id) of\napplications to deÔ¨Åne the owners of applications.\nAs described in Fig. 3, when a user logs in, URS checks\nif a user group with the ‚Äòuid‚Äô exists in the user group list\nby hooking system calls. If it exists, URS inserts the current\nprocess of the user into the user group. Otherwise, URS creates\na new user group linked to the user group list and inserts the\ncurrent (log-in) process of the user into the user group. A child\nprocess forked by a parent process belongs to the same user\ngroup because the child process copies the group information\nof the parent process. Therefore, if URS inserts a user‚Äôs current\nprocess into the user group on log-in, all the processes that\nthe user executes, belong to the same user group.\nB. CPU Scheduling for Performance Fairness and Isolation\nAmong Users\nIn CPU scheduling, URS constructs three layers of task\ngroups using the method explained in the previous subsec-\ntion. As described in Fig. 4, the Ô¨Årst layer is the root task\ngroup, of which runqueue contains all the user task groups\nas its scheduling entities. The second layer is the user task\ngroup, which URS provides to each user. Under user task\ngroups, there exist session task groups that the user owns.\nWe maintain the session task groups to beneÔ¨Åt from the con-\nventional CPU scheduling. Finally, a runqueue of each session\ntask group contains actual processes created on the session.\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:24:52 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.5237538516521454,
        "keyword_overlap": 0.011516314779270634,
        "combined_score": 0.3700825905902829,
        "paragraph_id": 2,
        "source_title": "URS: User-Based Resource Scheduling for Multi-User Surface Computing Systems"
      },
      {
        "text_fragment": "PARK AND EOM: URS\n429\nFig. 3.\nHow to construct user groups.\nFig. 4.\nCPU scheduling of URS for performance fairness and isolation\namong users.\nWhen URS schedules CPU resources, it Ô¨Årst chooses a user\ntask group with the lowest ‚Äòvruntime‚Äô. And then, URS chooses\na session task group with the lowest ‚Äòvruntime‚Äô in the user\ntask group. Finally, URS allocates CPU resources to the pro-\ncess with the lowest ‚Äòvruntime‚Äô in the session task group. In\nour CPU scheduling scheme, each user task group is an iso-\nlated scheduling entity from the other user task groups, and\nthe system allocates CPU resources to each user task group,\ndepending on its speciÔ¨Åed ‚Äòshare‚Äô. (The higher the ‚Äòshare‚Äô\nvalue is, the more CPU resources are allocated because the\n‚Äòvruntime‚Äô increases slowly.) Therefore, CPU scheduling of\nURS provides CPU performance isolation among users by cre-\nating a task group for each user. Additionally, it can provide\nCPU performance fairness by assigning the same ‚Äòshare‚Äô value\nFig. 5.\nComparison between the conventional and proposed CPU scheduling.\nFig. 6.\nI/O scheduling of URS for performance fairness and isolation\namong users.\nto all the user task groups, or it can prioritize applications of\na particular user by assigning a higher ‚Äòshare‚Äô value to the user.\nFor example, let us assume that there are four users who\nrun 1-4 applications, each of which is running on an individ-\nual session, as shown in Fig. 5. In the conventional system,\nCPU resources are distributed based on sessions. Therefore,\nthe system allocates quadruple the amount of resources to\nthe user who runs four applications, compared to the user\nwho runs only a single application. However, in case of the\nproposed scheme, all users have their own user group above\nthe session task groups. Therefore, the system can allocate the\nsame amount of CPU resources to each user regardless of the\nnumber of their applications, when all users have the same\n‚Äòshare‚Äô value.\nC. I/O Scheduling for Performance Fairness and Isolation\nAmong Users\nI/O scheduling of URS is similar to the CPU schedul-\ning. In conventional I/O scheduling, each process has its own\nCFQ queue to ensure fairness among the processes. On the\nother hand, the proposed scheme additionally creates a user\nCFQ group for each user above the process CFQ queue, as\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:24:52 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.5030742287635803,
        "keyword_overlap": 0.0228734810578985,
        "combined_score": 0.35901400445187576,
        "paragraph_id": 3,
        "source_title": "URS: User-Based Resource Scheduling for Multi-User Surface Computing Systems"
      },
      {
        "text_fragment": "432\nIEEE TRANSACTIONS ON CONSUMER ELECTRONICS, VOL. 65, NO. 3, AUGUST 2019\nFig. 10.\nUser-based scheduling experiments for I/O scheduling of URS.\nFig. 11.\nPerformance isolation experiments for I/O scheduling of URS.\nshows around two times higher read performance in total than\nthe other users because URS gives two times higher ‚Äòweight‚Äô\nvalue to the corresponding user CFQ group than the other CFQ\nuser groups.\nOn the isolation experiments in Fig. 11, the proposed\nscheme shows 1.66 times higher aggregate throughput than\nthe conventional scheme. As with the CPU scheduling exper-\niments, URS isolates each user from other users in terms\nof I/O performance by providing users with individual user\nCFQ groups. Accordingly, URS diminishes the inÔ¨Çuence of\nthe malicious user on the normal users. As a result, URS\nprovides user-based resource scheduling and performance iso-\nlation among multiple users for I/O resources, as well as CPU\nresources.\nC. Page Cache Management of URS\nTo verify the page cache management of URS, we sim-\nulate a situation where four users run one foreground and\none background application each. For both of the foreground\nand background applications, each user runs two FIO bench-\nmarks, each of which reads a 1GB Ô¨Åle with a request size\nof 128KB. To inform whether an application runs in the\nforeground or not, we explicitly set the different nice value\nto the foreground application. Also, we repeatedly run the\napplications for their data to be cached in the page cache layer.\nFig. 12.\nFIO-read experiments for verifying the page cache management\nof URS.\nThe experimental results in Fig. 12 are normalized to\nthe baseline where applications run with bypassing the page\ncache. In the baseline, all data should be read from the sec-\nondary storage. Only-FG in Fig. 12 shows the performance\nof the best case when only foreground applications are run-\nning without background ones. In Fig. 12, the conventional\nscheme shows 1.78 times higher average throughput than\nthe baseline, whereas URS shows 6.12 times higher aver-\nage throughput than the baseline. The conventional scheme\ndoes not distinguish the foreground applications, and thus their\ndata are frequently deallocated from the page cache because of\nbackground applications. However, URS differentiates data of\nforeground applications and prioritizes them by giving more\nopportunities to stay in the page cache. Eventually, the average\nthroughput of URS is only 11% less than that of Only-FG.\nV. RELATED WORK\nFair share scheduler [21] brings up the problem of schedul-\ning resources based on processes and tries to equally distribute\nCPU resources to users or groups. This work presents the\nconcept of ‚Äòshare‚Äô which indicates the entitlement to take\nresources. Fair share scheduler assigns the same amount of\n‚Äòshare‚Äô to each user and distributes resources based on the\n‚Äòshare‚Äô values, resulting in performance fairness among users\nor groups. However, the fair share scheduler only focuses on\nthe users‚Äô fairness without prioritizing the users according\nto the real-world priorities. Additionally, it does not accel-\nerate foreground applications which are critical to the user\nexperience of multi-user surface computing systems.\nAnother previous work related to the user-based resource\nscheduling is Cgroups. Cgroups is a user-space API to control\nand limit system resources, such as CPU, memory, and I/O,\nvia creating a resource group. By making a resource group per\nuser in the user-space, Cgroups can distribute system resources\nbased on users and isolate users from each other. However,\nsince Cgroups works in the user-space, it should keep track of\nthe ownership information of all applications by continuously\ninquiring it to the kernel layer. Besides, Cgroups cannot con-\ntrol pages for the page cache, and thus it cannot prioritize\nforeground applications in the page cache layer.\nFastTrack [12] mentions the importance of a foreground\napplication regarding user experience. It improves the I/O\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:24:52 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.48427003622055054,
        "keyword_overlap": 0.016129032258064516,
        "combined_score": 0.3438277350318047,
        "paragraph_id": 6,
        "source_title": "URS: User-Based Resource Scheduling for Multi-User Surface Computing Systems"
      }
    ],
    "resolved_title": "URS: User-Based Resource Scheduling for Multi-User Surface Computing Systems"
  },
  {
    "citation_marker": "[16]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.47, overlap: 0.04): **NO**  \n**Reason:** The article's statement claims that citation [16] (\"Automatic Code Generation for High-Performance Graph Algorithms\") prioritizes foreground applications on mobile environments by handling lock contentions in the page cache layer. However, the cited paragraph exclusively discusses a compiler framework for optimizing graph algorithms via sparse linear algebra and DSLs. It addresses challenges like irregular computations and portability but contains **no mention** of mobile environments, lock contentions, page cache layers, or foreground/background application prioritization. The cited work focuses on graph algorithm performance across architectures (e.g., CPUs/GPUs), not lock-related scheduling in mobile systems. Thus, the statement fabricates unsupported claims about the cited paper's content.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.46, overlap: 0.02): **NO**  \n\n**Reason:**  \nThe article's statement claims that citation [16] (\"Automatic Code Generation for High-Performance Graph Algorithms\") prioritizes foreground applications on mobile environments by handling lock contentions in the page cache layer. However, the cited paragraph exclusively discusses:  \n- A compiler framework for graph algorithms using sparse linear algebra.  \n- Optimizations like workspace transformation, two-phase computation, and parallelization.  \n- Performance comparisons with graph libraries (e.g., LAGraph).  \n\nThe cited work has **no mention** of mobile environments, lock contention, page cache layers, or foreground application prioritization. The article misattributes a mobile/lock-related solution to a paper focused on graph algorithm compilers, fabricating an unsupported connection.  \n\n**Key Discrepancies:**  \n1. The cited work targets **graph processing efficiency** on high-performance architectures, not mobile systems.  \n2. The cited contributions are **compiler optimizations** (e.g., semiring operations, parallelization), not lock management.  \n3. The evaluation compares **graph kernels** (e.g., BFS, SpGEMM) against libraries like LAGraph, with no reference to locks or scheduling failures.  \n\nThe article distorts the cited work's domain (graph algorithms vs. mobile scheduling) and fabricates a non-existent focus on lock contention.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.40, overlap: 0.03): **NO**  \nThe statement claims that citation [16] (\"Automatic Code Generation...\") addresses \"prioritiz[ing] a foreground application on mobile environments by handling lock contentions in the page cache layer.\" However, the cited paragraph from [16] exclusively discusses:  \n1. **Graph libraries** (e.g., LAGraph, NW-Graph) and their optimization approaches.  \n2. **Compilers for sparse computations** (e.g., GreenMarl, GraphIt, TACO) and their limitations.  \n3. The authors' own **compiler framework** for graph algorithms (COMET), including its optimizations and benchmarks against LAGraph.  \n\n**There is no mention of mobile environments, lock contentions, page cache layers, or foreground application prioritization** in the cited text. The work focuses solely on high-performance graph algorithm code generation, making the article's attribution to [16] a misrepresentation.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "Automatic Code Generation for High-Performance\nGraph Algorithms\nZhen Peng‚àó, Rizwan A. Ashraf‚àó, Luanzheng Guo‚àó, Ruiqin Tian‚Ä†¬ß and Gokcen Kestor‚àó‚Ä°\n‚àóPacific Northwest National Laboratory, Richland, Washington, USA\n‚Ä† Horizon Robotics, Shanghai, China\n‚Ä° University of California, Merced, Merced, California, USA.\nEmails: {zhen.peng, rizwan.ashraf, lenny.guo, gokcen.kestor}@pnnl.gov, ruiqin.tian@horizon.ai\nAbstract‚ÄîGraph problems are common across many fields,\nfrom scientific computing to social sciences. Despite their im-\nportance and the attention received, implementing graph al-\ngorithms effectively on modern computing systems remains a\nchallenging task that requires significant programming effort\nand generally results in customized implementations. Current\ncomputing and memory hierarchies are not architected for\nirregular computations, resulting performance that is far from\nthe theoretical architectural peak. In this paper, we propose\na compiler framework to simplify the development of graph\nalgorihtm implementations that can achieve high performance\non modern computing systems. We provide a high-level domain\nspecific language (DSL) to represent graph algorithms through\nsparse linear algebra expressions and graph primitives including\nsemiring and masking. The compiler leverages the semantics\ninformation expressed through the DSL during the optimization\nand code transformation passes, resulting in more efficient IR\npassed to the compiler backend. In particular, we introduce\nan Index Tree Dialect that preserves the semantic information\nof the graph algorithm to perform high-level, domain-specific\noptimizations, including workspace transformation, two-phase\ncomputation, and automatic parallelization. We demonstrate that\nthis work outperforms state-of-the-art graph libraries LAGraph\nby up to 3.7√ó speedup in semiring operations, 2.19√ó speedup in\nan important sparse computational kernel, and 9.05√ó speedup\nin graph processing algorithms.\nIndex Terms‚Äîcompiler, optimization, graph algorithms, tri-\nangle counting, breadth-first search, code generation, semiring,\nmasking, sparse linear algebra\nI. INTRODUCTION\nGraphs data structures are used in many domains, from\ncomputer security [1] and computational genomics [2] to\nnetwork sciences [3] and scientific computing [4], to represent\ncomplex interactions and casual relationships (edges) among\nentities (nodes). While graphs adapt well to solve problems at\ndifferent scales, real-life problems often produce graph data\nstructures that are highly irregular and extremely large. These\ntwo factors pose challenges while implementing efficient graph\nalgorithms on modern computer architectures, which have\nbeen developed and optimized mostly for regular computation.\nTo achieve high-performance, developers are often forced to\nwrite ad-hoc code specifically tailored for given architectures\nusing a fairly low-level programming language, such as C/C++\n¬ßWork was performed while the author was at Pacific Northwest National\nLaboratory\nor CUDA, which, then, impedes the portability of the imple-\nmentation on different systems, productivity, and reusability.\nWith the proliferation of modern computing systems, the\ncurrent practice of manually reimplementing graph algorithms\nfor each new architecture is simply not sustainable.\nIn this work, we seek a solution to develop graph algo-\nrithms that provide high performance on modern computer\narchitectures but do not hinder portability and productivity.\nIn this endover, we identify two main challenges: 1) find the\nright level of abstraction to represent graph algorithms and\n2) lower that abstraction to efficient machine code. The level\nof abstraction should be high enough to enable developers\nto express graph algorithms effectively and with notations\nthat make sense in the application domain, both of which\nincrease productivity. Hand-tuned, architecture-specific imple-\nmentations (e.g., CUDA) may achieve high performance but\ndeveloping such solutions is time-consuming and not portable\nacross systems. The abstraction should carry sufficient seman-\ntics information to be used during code optimizations and\nmachine code generation to increase performance on specific\narchitectures. Finally, the abstraction should be architecture-\nindependent and semantically rich to guarantee portability\nacross different systems. In fact, it is generally easier to\nport high-level operations (e.g., sparse matrix-sparse matrix\nmultiplication) than low-level constructs (e.g., nested loops)\nacross systems. In this work, we opt for (sparse) linear\nalgebra as a reasonable programming abstraction to develop\nefficient graph algorithms. Algebraic representations of graph\nalgorithms are architecture-independent, sufficiently high-level\nso that users can effectively implement graph applications\nin their domains, and carry enough semantics information\nto inform the underlying system about which architecture-\nindependent and architecture-specific optimizations should be\nemployed. Compared to vertex-based and edge-traversal im-\nplementations, algebraic representations provide a compact\nand expressive way to represent graph algorithms, carrying\nsemantic information through the characteristics of the matrix\nused to represent the graph [5], [6], are easier to develop,\nmore portable, and can leverage a large body of research and\noptimization.\nThe second challenge is represented by mapping (lowering)\nthe high-level abstraction to efficient code for specific com-\nputing systems. The inherent irregularity of graph processing",
        "semantic_similarity": 0.4697279930114746,
        "keyword_overlap": 0.03699421965317919,
        "combined_score": 0.33990786100398596,
        "paragraph_id": 0,
        "source_title": "Automatic Code Generation for High-Performance Graph Algorithms"
      },
      {
        "text_fragment": "algorithms and the size of real-life graphs pose considerable\nchallenges when performing this process. The sheer size of\nreal-life problems makes it difficult, if not impossible, to store\ngraphs (i.e., adjacency matrix) using dense data structures.\nGiven the intrinsic sparse nature of graph structures, storing\ngraphs in dense format would introduce excessive pressure on\nthe memory subsystem and unnecessary computation. Efficient\ngraph implementations generally prefer sparse representations\nof the graph to reduce memory requirements and use sparse\noperators to increase computing efficiency by eliminating\nunnecessary computation. However, modern computing archi-\ntectures and memory technologies have been designed and\noptimized for dense computation and do not perform as\nwell for sparse computation [7]. The process of lowering\nhigh-level abstraction to efficient machine code must employ\ndifferent kinds of optimizations, both architecture-independent\nand architecture-specific, and should be performed at all\nlevels of the lowering process. First, the language should\nprovide high-level, graph-oriented operators that carry enough\ninformation for efficient code generation. Second, architecture-\nindependent, graph-specific optimizations, such as fusion and\nautomatic parallelization, should be applied to the high-level\ncode. Next, generic architecture-independent optimizations\n(loop unrollling, dead code elimination, etc.) should be con-\nsidered. Finally, the resulting code should be optimized for\nthe target architecture. This process should be automated to\nincrease productivity and portability and, to the extend that\nthe abstraction carries enough semantics information, should\nhave the user out of the loop.\nIn this work, we propose a domain-specific compiler frame-\nwork to develop graph algorithm implementations that can\nachieve high performance, are portable across different sys-\ntems, and are easy to develop. We propose a high-level\nDomain-Specific Language (DSL) to represent graph algo-\nrithms through (sparse) linear algebra expressions and specific\ngraph-oriented operators. The DSL allows users to embed\ndomain-specific semantics that is leveraged internally during\ncode generation through a series of optimizations and lowering\nsteps to generate efficient Intermediate Representation (IR),\nsuch as specific graph primitives including semiring and\nmasking. The proposed compiler is based on a multi-level IR\nand progressive lowering from high-level IRs (or dialects) that\nencapsulate the semantics of the application to low-level IRs,\nwhich are closer to the architecture. The compiler leverages the\nsemantics information expressed through the DSL during the\noptimization and code transformation passes. This generally\nresults in more efficient IR that can be passed to the compiler\nbackend (e.g., LLVM) to generate machine code compared\nto general-purpose programming environments, such as C or\nC++. In particular, we introduce an Index Tree Dialect. This\ndialect preserves the semantic information of the graph al-\ngorithm to perform high-level, domain-specific optimizations.\nSeveral code optimizations and transformations are applied\nwhile lowering the index tree IR to lower-level dialects in\nthe compilation pipeline, including optimizations specifically\ndeveloped in this work: workspace transformation, two-phase\ncomputation, and automatic parallelization. Workspace trans-\nformation takes advantage of intermediate dense structure to\nimprove the data locality and reduce computation complexity\nwhile preserving the sparse format of the resulting outputs.\nThe two-phase computation employs symbolic computation\nto deduce the minimum size for the output‚Äôs sparse data\nstructure. We also introduce a novel optimization algorithm\nthat leverages the symbolic information to perform automatic\nparallelization of sparse linear algebra primitives.\nWe show that by combining our DSL, optimizations\n(workspace transformation, two-phase computation, and paral-\nlelization), and efficient graph primitives (semiring and mask-\ning), we are able to outperform state-of-the-art graph libraries\n(e.g., LAGraph [6], which implements the GraphBLAS stan-\ndard [8]) by a significant margin. We evaluate the performance\nof several graph primitives and graph processing algorithms.\nOur results show that our work outperforms LAGraph by up to\n3.7√ó for semiring operations, 2.19√ó for SpGEMM kernel, and\n9.05√ó for graph processing algorithms Breadth First Search\n(BFS) and Triangle Counting (TC). In summary, this work\nmakes the following contributions:\n‚Ä¢ a novel compiler framework and DSL, which enable users to\nproductively develop the algebraic implementation of graph\nalgorithms and achieve high-performance.\n‚Ä¢ important graph primitives (semirings and masking op-\nerations)\nand\ncode\noptimizations\nand\ntransformations\n(workspace transformation, two-phase computation, paral-\nlelization) for efficient execution;\n‚Ä¢ a performance evaluation of sparse linear algebra kernels\nand two prominent graph processing algorithms and com-\nparison with LAGraph.\nThe rest of this work is organized as follows: Section II\nprovides an overview of the compiler; Section III introduces\nthe code generation optimizations of graph computations;\nSection IV demonstrates extended linear algebra primitives\nfor graph algorithms; Section V provides an exhaustive perfor-\nmance evaluation; finally, Section VI and Section VII compare\nthis work to other efforts and draw conclusions, respectively.\nII. OVERVIEW\nThis work proposes a domain-specific compiler framework\nto develop efficient graph algorithms represented by linear\nalgebra operations. Our work adheres to the GraphBLAS stan-\ndard, which provides a comprehensive set of graph primitives\nfor sparse matrices and vectors of various types and extends\nthe traditional linear algebra operators with semirings and\nmasking to achieve higher performance. The GraphBLAS-\nbased approach provides a consistent way for graph algorithm\nimplementation through common graph primitives that can\nbe optimized using well-studied techniques, and it avoids the\ncomplexity of writing different ad-hoc implementations com-\nmon with traditional vertex- or edge-centric approaches [9]‚Äì\n[11]. In most cases, the algorithmic complexity of the graph\nalgorithms implemented using linear algebra is close to the\ncomplexity of the implementation based on vertex- or edge-\ntransverses [12]. Given a graph G(V, E), where V is the set of",
        "semantic_similarity": 0.45952165126800537,
        "keyword_overlap": 0.017424975798644726,
        "combined_score": 0.3268926486271972,
        "paragraph_id": 1,
        "source_title": "Automatic Code Generation for High-Performance Graph Algorithms"
      },
      {
        "text_fragment": "VI. RELATED WORK\nGraph Libraries. There are numerous graph libraries,\nsuch as [6], [9]‚Äì[11], [24]‚Äì[30], that aim to provide high-\nperformance implementations of graph kernels using differ-\nent sequential and parallel algorithms. LAGraph [6] is a\nlibrary that contains representative graph algorithms and is\nbased on sparse linear algebra operations from the SuiteS-\nparse:GraphBLAS package [24]. On the other hand, NW-\nGraph [10] is a high-performance header-only graph library\nthat leverages C++20 features. However, different libraries\nhave their own approach to optimization and are tied to specific\nprogramming models. In contrast, the compiler potentially\noffers a unified solution for sequential and parallel code gener-\nation through the MLIR back-end while being complementary\nto existing library-based approaches.\nCompilers for Sparse Computations. There are several\ndomain-specific compilers designed for generating code of\nsparse operations in graph algorithms, including Green-\nMarl [31], GraphIt [32], and TACO [33]. These compilers,\nsuch as Green-Marl and TACO, perform source-to-source\ntranslation, where TACO translates its DSL operations to\nC++ using computational templates. However, TACO does not\nsupport parallel sparse computation (e.g., parallel SpGEMM),\nand its optimizations mainly focus on sequential code. In\ncontrast, the compiler in this work proposes optimizations\nincluding two-phase computation and parallelization for sparse\nkernels.\nRecently, MLIR infrastructure added support for sparse ten-\nsors through the sparse-tensor dialect [19]. COMET precedes\nthis support and does not utilize the sparse-tensor dialect.\nAs a result of MLIR‚Äôs support for sparse tensors, we can\nexpect more MLIR-based compilers to include support for\ngraph algorithms in the future. One such example is the\nmlir-graphBLAS [34] effort that plans to lower to linalg\ndialect. Previously, it used to generate code at the loop level\n(SCF dialect) in a similar manner to this work albeit without\noptimizations such as workspace transforms.\nVII. CONCLUSIONS\nWe present a compiler framework to simplify the devel-\nopment of graph algorithms and generate efficient code for\ntarget computing architectures. Built on top of COMET, this\ncompiler consists of a DSL for developing graph algorithms\nusing algebraic operations, optimized graph operators (such\nas semiring and masking), and various optimizations and code\ntransformations (such as workspace transformation, two-phase\ncomputation, and automatic parallelization). We demonstrate\nthe performance benefits of code generation through our\ncompiler using common graph algorithms and compare it to a\nstate-of-the-art library-based approach LAGraph. Our results\nshow that compared to LAGraph, our compiler can achieve up\nto 3.7√ó speedup in semiring operations, 2.19√ó speedup in an\nimportant sparse computational kernel, and 9.05√ó speedup in\ngraph processing algorithms.\nACKNOWLEDGMENT\nThe authors thank anonymous reviewers for their con-\nstructive comments and helpful suggestions. The research\ndescribed in this paper is part of the Data Model Convergence\nInitiative at Pacific Northwest National Laboratory (PNNL).\nIt was conducted under the Laboratory Directed Research and\nDevelopment Program at PNNL, a multi-program national\nlaboratory operated by Battelle for the U.S. Department of\nEnergy (DOE). This work was also supported by the U.S. DOE\nOffice of Science, Office of Advanced Scientific Computing\nResearch, under award 66150: ‚ÄúCENATE ‚Äì Center for Ad-\nvanced Architecture Evaluation‚Äù project. This work was also\nsupported by the High-Performance Data Analytics (HPDA)\nprogram at PNNL. PNNL is operated by Battelle for the U.S.\nDOE under Contract DE-AC05-76RL01830.",
        "semantic_similarity": 0.4045248031616211,
        "keyword_overlap": 0.025487256371814093,
        "combined_score": 0.290813539124679,
        "paragraph_id": 10,
        "source_title": "Automatic Code Generation for High-Performance Graph Algorithms"
      }
    ],
    "resolved_title": "Automatic Code Generation for High-Performance Graph Algorithms"
  },
  {
    "citation_marker": "[17]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.65, overlap: 0.02): **NO**  \nThe statement from the article cites Shue et al. (2012) for their work on \"Performance isolation and fairness for multi-tenant cloud storage.\" However, the provided paragraph from the cited document focuses on the **Pisces system**, its mechanisms (partition placement, weight allocation, replica selection, fair queuing), and its results (achieving near-ideal fairness and isolation).  \n\n**Reason:**  \nThe article's statement is a generic citation placeholder (likely a reference entry) but **does not include any specific claims** about the content of Shue et al.'s work. Since there is no substantive assertion to validate (e.g., \"Shue et al. propose Pisces\" or \"Shue et al. achieve 0.99 Min-Max Ratio\"), it **cannot be evaluated for accuracy or distortion** relative to the source. The citation itself is formatted correctly (authors, title, venue, year), but the absence of contextual claims renders the question of \"support\" inapplicable.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.64, overlap: 0.03): **NO**  \nThe statement contains a **fabrication**.  \n\n**Reason:**  \nThe cited paragraph explicitly lists the publication venue as the \"**10th USENIX Symposium on Operating Systems Design and Implementation (OSDI ‚Äô12)**.\" However, the article incorrectly cites it as \"**Proceedings of USENIX Symposium on Operating Systems Design and Implemen[tation]**\" without specifying the conference series (OSDI) or edition (10th). This omission misrepresents the source by failing to acknowledge the specific symposium (OSDI) and edition, which are critical for accurate academic citation.  \n\n*(Note: The truncation \"Implemen\" appears to be a typographical error, but the core issue is the omission of \"OSDI\" and \"10th.\")*",
      "DeepSeek LLM confirms support for this match: Based on the provided information, the answer is **YES**.  \n\n**Reason:**  \nThe statement from the article (\"Performance isolation and fairness for multi-tenant cloud storage.\") is **accurately supported** by the cited paragraph. The paragraph explicitly discusses mechanisms (e.g., partition placement, weight allocation) to achieve \"system-wide fair service allocations\" and \"fairness\" in multi-tenant cloud storage, directly aligning with the article's statement. There is no evidence of misrepresentation, distortion, or fabrication‚Äîthe cited content substantiates the claim.  \n\n**Key alignment:**  \n- The source text focuses on \"fair service allocations\" (¬ß3) and \"fairness\" (e.g., DRF in ¬ß3.1).  \n- The title cited by the article (\"Performance isolation and fairness...\") matches the source's core objective.  \n\nNo further claims beyond the title are made in the article snippet, so no deeper technical accuracy requires validation."
    ],
    "evidence": [
      {
        "text_fragment": "362‚ÄÉ 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI ‚Äô12)\t\nUSENIX Association\ncould adapt these techniques for memory and disk I/O re-\nsources. Stout [20] exploits batch processing to minimize\nrequest latency, but does not address fairness.\nSeveral other systems focused on course-grained alloca-\ntion. Autocontrol [23] and Mesos [15] allocate per-node\nCPU and memory to schedule batch jobs and VMs, using\nutility-driven optimization and dominant resource fair-\nness, respectively. They operate on a coarse per-task or\nper-VM level, however, rather than on per-application\nrequests. In [10], the authors apply DRF to Ô¨Åne-grained\nmulti-resource allocation, speciÔ¨Åcally to enforce per-Ô¨Çow\nfairness in middleboxes. However, their DRF queuing\nalgorithm relies on virtual time, and it scans each per-Ô¨Çow\npacket queue for the lowest virtual start time.\n8.\nConclusion\nThis paper seeks to provide system-wide per-tenant\nweighted fair sharing and performance isolation in multi-\ntenant, key-value cloud storage services.\nBy decom-\nposing this problem into a novel combination of four\nmechanisms‚Äîpartition placement, weight allocation,\nreplica selection, and fair queuing‚Äîour Pisces system\ncan fairly share the aggregate system throughput, even\nwhen tenants contend for shared resources and demand\ndistributions vary across partitions and over time. Our pro-\ntotype implementation achieves near ideal fairness (0.99\nMin-Max Ratio) and strong performance isolation.\nAcknowledgments. We thank Jennifer Rexford for help-\nful discussions early in this project. Funding was provided\nthrough NSF CAREER Award #0953197.\nReferences\n[1] http://aws.amazon.com/dynamodb/faqs/, 2012.\n[2] http://docs.amazonwebservices.com/\namazondynamodb/latest/developerguide/\nLimits.html, 2012.\n[3] http://www.couchbase.org/, 2012.\n[4] http://code.google.com/p/spymemcached/, 2012.\n[5] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity,\ndata center network architecture. In SIGCOMM, 2008.\n[6] H. Ballani, P. Costa, T. Karagiannis, and A. Rowstron. Towards\npredictable datacenter networks. In SIGCOMM, 2011.\n[7] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and\nR. Sears. Benchmarking cloud serving systems with YCSB. In\nSOCC, 2010.\n[8] S. L. GarÔ¨Ånkel. An evaluation of Amazon‚Äôs grid computing ser-\nvices: EC2, S3 and SQS. Technical Report TR-08-07, Harvard\nUniv., 2007.\n[9] A. Ghodsi, M. Zaharia, B. Hindman, A. Konwinski, S. Shenker,\nand I. Stoica. Dominant resource fairness: Fair allocation of\nmultiple resource types. In NSDI, 2011.\n[10] A. Ghodsi, V. Sekar, M. Zaharia, and I. Stoica. Multi-resource\nscheduling for middleboxes. In SIGCOMM, 2012.\n[11] A. Gulati, I. Ahmad, and C. A. Waldspurger. PARDA: Proportional\nallocation of resources for distributed storage access. In FAST,\n2009.\n[12] A. Gulati, A. Merchant, and P. J. Varman. mClock: Handling\nthroughput variability for hypervisor IO scheduling. In OSDI,\n2010.\n[13] C. Guo, G. Lu, H. J. Wang, S. Yang, C. Kong, P. Sun, W. Wu,\nand Y. Zhang. SecondNet: A data center network virtualization\narchitecture with bandwidth guarantees. In CoNext, 2010.\n[14] J. He, R. Zhang-Shen, Y. Li, C.-Y. Lee, J. Rexford, and M. Chiang.\nDavinci: dynamically adaptive virtual networks for a customized\ninternet. In CoNext, 2008.\n[15] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. D. Joseph,\nR. Katz, S. Shenker, and I. Stoica. Mesos: A platform for Ô¨Åne-\ngrained resource sharing in the data center. In NSDI, 2011.\n[16] A. Iosup, N. Yigitbasi, and D. Epema. On the performance vari-\nability of production cloud services. In CCGrid, 2011.\n[17] T. Lam, S. Radhakrishnan, A. Vahdat, and G. Varghese. Netshare:\nVirtualizing data center networks across services. Technical Report\nCS2010-0957, UCSD, 2010.\n[18] T. Li, D. Baumberger, and S. Hahn. EÔ¨Écient and scalable multi-\nprocessor fair scheduling using distributed weighted round-robin.\nIn PPoPP, 2009.\n[19] Y. Mao, E. Kohler, and R. Morris.\nCache craftiness for fast\nmulticore key-value storage. In EuroSys, 2012.\n[20] J. C. McCullough, J. Dunagan, A. Wolman, and A. C. Snoeren.\nStout: an adaptive interface to scalable cloud storage. In USENIX\nAnnual, 2010.\n[21] M. M. Michael and M. L. Scott. Simple, fast, and practical non-\nblocking and blocking concurrent queue algorithms. In PODC,\n1996.\n[22] R. M. Nauss. Solving the generalized assignment problem: An\noptimizing and heuristic approach. INFORMS J. Computing, 15\n(Summer):249‚Äì266, 2003.\n[23] P. Padala, K.-Y. Hou, K. G. Shin, X. Zhu, M. Uysal, Z. Wang,\nS. Singhal, and A. Merchant. Automated control of multiple\nvirtualized resources. In EuroSys, 2009.\n[24] D. Palomar and M. Chiang. A tutorial on decomposition methods\nfor network utility maximization. JSAC, 24(8):1439‚Äì1451, 2006.\n[25] L. Peterson, A. Bavier, and S. Bhatia. VICCI: A programmable\ncloud-computing research testbed. Technical Report TR-912-11,\nPrinceton CS, 2011.\n[26] L. Popa, G. Kumar, M. Chowdhury, A. Krishnamurthy, S. Rat-\nnasamy, and I. Stoica. FairCloud: Sharing the network in cloud\ncomputing. In SIGCOMM, 2012.\n[27] H. Rodrigues, J. R. Santos, Y. Turner, P. Soares, and D. Guedes.\nGatekeeper: supporting bandwidth guarantees for multi-tenant\ndatacenter networks. In WIOV, 2011.\n[28] A. Shieh, S. Kandula, A. Greenberg, C. Kim, and B. Saha. Sharing\nthe data center network. In NSDI, 2011.\n[29] D. B. Shmoys and E. Tardos. An approximation algorithm for\nthe generalized assignment problem. Math. Prog., 62(1):461‚Äì474,\n1993.\n[30] M. Shreedhar and G. Varghese. EÔ¨Écient fair queuing using deÔ¨Åcit\nround-robin. Trans. Networking, 4(3):375‚Äì385, 1996.\n[31] M. Stonebraker. The case for shared nothing. IEEE Database Eng.\nBulletin, 9(1):4‚Äì9, 1986.\n[32] M. Wachs, M. Abd-el-malek, E. Thereska, and G. R. Ganger.\nArgon: Performance insulation for shared storage servers. In\nFAST, 2007.\n[33] J. Wang, P. Varman, and C. Xie. Optimizing storage performance\nin public cloud platforms. J. Zhejiang Univ. ‚Äì Science C, 11(12):\n951‚Äì964, 2011.\n[34] D. X. Wei, C. Jin, S. H. Low, and S. Hegde. Fast TCP: Motivation,\narchitecture, algorithms, performance. Trans. Networking, 14(6):\n1246‚Äì1259, 2006.\n[35] M. Zaharia, A. Konwinski, A. D. Joseph, R. Katz, and I. Stoica.\nImproving MapReduce performance in heterogeneous environ-\nments. In OSDI, 2008.",
        "semantic_similarity": 0.6548703610897064,
        "keyword_overlap": 0.01745877788554801,
        "combined_score": 0.4636468861284589,
        "paragraph_id": 24,
        "source_title": "Performance Isolation and Fairness for Multi-Tenant Cloud Storage"
      },
      {
        "text_fragment": "USENIX Association \t\n10th USENIX Symposium on Operating Systems Design and Implementation (OSDI ‚Äô12)‚ÄÉ 349\nPerformance Isolation and Fairness for\nMulti-Tenant Cloud Storage\nDavid Shue‚ãÜ, Michael J. Freedman‚ãÜ, and Anees Shaikh‚Ä†\n‚ãÜPrinceton University, ‚Ä†IBM TJ Watson Research Center\nAbstract\nShared storage services enjoy wide adoption in commer-\ncial clouds. But most systems today provide weak per-\nformance isolation and fairness between tenants, if at\nall. Misbehaving or high-demand tenants can overload\nthe shared service and disrupt other well-behaved tenants,\nleading to unpredictable performance and violating SLAs.\nThis paper presents Pisces, a system for achieving\ndatacenter-wide per-tenant performance isolation and fair-\nness in shared key-value storage. Today‚Äôs approaches for\nmulti-tenant resource allocation are based either on per-\nVM allocations or hard rate limits that assume uniform\nworkloads to achieve high utilization. Pisces achieves\nper-tenant weighted fair shares (or minimal rates) of the\naggregate resources of the shared service, even when dif-\nferent tenants‚Äô partitions are co-located and when demand\nfor diÔ¨Äerent partitions is skewed, time-varying, or bot-\ntlenecked by diÔ¨Äerent server resources. Pisces does so\nby decomposing the fair sharing problem into a combina-\ntion of four complementary mechanisms‚Äîpartition place-\nment, weight allocation, replica selection, and weighted\nfair queuing‚Äîthat operate on diÔ¨Äerent time-scales and\ncombine to provide system-wide max-min fairness.\nAn evaluation of our Pisces storage prototype achieves\nnearly ideal (0.99 Min-Max Ratio) weighted fair sharing,\nstrong performance isolation, and robustness to skew and\nshifts in tenant demand. These properties are achieved\nwith minimal overhead (<3%), even when running at high\nutilization (more than 400,000 requests/second/server for\n10B requests).\n1.\nIntroduction\nAn increasing number and variety of enterprises are mov-\ning workloads to cloud platforms. Whether serving exter-\nnal customers or internal business units, cloud platforms\ntypically allow multiple users, or tenants, to share the\nsame physical server and network infrastructure, as well\nas use common platform services. Examples of these\nshared, multi-tenant services include key-value stores,\nblock storage volumes, SQL databases, message queues,\nand notiÔ¨Åcation services. These leverage the expertise of\nthe cloud provider in building, managing, and improving\ncommon services, and enable the statistical multiplexing\nof resources between tenants for higher utilization.\nBecause they rely on shared infrastructure, however,\nthese services face two key, related issues:\n‚Ä¢ Multi-tenant interference and unfairness: Tenants\nsimultaneously accessing shared services contend\nfor resources and degrade performance.\n‚Ä¢ Variable and unpredictable performance: Tenants\noften experience signiÔ¨Åcant performance variations,\ne.g., in response time or throughput, even when they\ncan achieve their desired mean rate [8, 16, 33, 35].\nThese issues limit the types of applications that can mi-\ngrate to multi-tenant clouds and leverage shared services.\nThey also inhibit cloud providers from oÔ¨Äering diÔ¨Äer-\nentiated service levels, in which some tenants can pay\nfor performance isolation and predictability, while others\nchoose standard ‚Äúbest-eÔ¨Äort‚Äù behavior.\nShared back-end storage services face diÔ¨Äerent chal-\nlenges than sharing server resources at the virtual machine\n(VM) level. These stores divide tenant workloads into dis-\njoint partitions, which are then distributed (and replicated)\nacross diÔ¨Äerent service instances. Rather than managing\nindividual storage partitions, cloud tenants want to treat\nthe entire storage system as a single black box, in which\naggregate storage capacity and request rates can be elasti-\ncally scaled on demand. Resource contention arises when\ntenants‚Äô partitions are co-located, and the degree of re-\nsource sharing between tenants may be signiÔ¨Åcant higher\nand more Ô¨Çuid than with coarse VM resource allocation.\nParticularly, as tenants may use only a small fraction of a\nserver‚Äôs throughput and capacity,1 restricting nodes to a\nfew tenants may leave them highly underutilized.\nTo improve predictability for shared storage systems\nwith a high degree of resource sharing and contention,\nwe target global max-min fairness with high utilization.\nUnder max-min fairness, no tenant can gain an unfair\nadvantage over another when the system is loaded, i.e.,\neach tenant will receive its weighted fair share. Moreover,\ngiven its work-conserving nature, when some tenants use\nless than their full share, unconsumed resources are di-\nvided among the rest to ensure high utilization. While our\nmechanisms may be applicable to a range of services with\nshared-nothing architectures [31], we focus our design\nand evaluation on a replicated key-value storage service,\nwhich we call Pisces (Predictable Shared Cloud Storage).\n1Indeed, at today‚Äôs Amazon S3 prices, a single server handling\n50,000 GET reqs/second would cost $180/hour in request pricing alone.",
        "semantic_similarity": 0.6420774459838867,
        "keyword_overlap": 0.03136922424756253,
        "combined_score": 0.45886497946298943,
        "paragraph_id": 0,
        "source_title": "Performance Isolation and Fairness for Multi-Tenant Cloud Storage"
      },
      {
        "text_fragment": "USENIX Association \t\n10th USENIX Symposium on Operating Systems Design and Implementation (OSDI ‚Äô12)‚ÄÉ 353\ntenant B to receive a larger share (62.5%) of its dominant\nresource (requests). Instead, using dominant resource fair-\nness (DRF) [9] ensures that each tenant will receive a fair\nshare of its dominant resource relative to other tenants:\ntenant A receives 55% of out bytes and tenant B receives\n55% of requests, while out bytes remains the bottleneck.\n3.\nPisces Algorithms\nPisces implements four complementary mechanisms ac-\ncording to the design lessons discussed above. Each mech-\nanism operates on diÔ¨Äerent parts of the system at diÔ¨Äerent\ntimescales. Together, they deliver system-wide fair ser-\nvice allocations with minimal interference to each tenant.\n3.1\nPartition Placement\nThe partition placement mechanism ensures the feasibility\nof the system-wide fair shares. It assigns partitions so that\nthe load on each node (the aggregate of the tenants‚Äô per-\npartition fair-share demands) does not exceed the node‚Äôs\nrate capacity. Since our prototype currently only imple-\nments a simple greedy placement scheme, here we only\noutline the algorithm without providing details. The cen-\ntralized controller Ô¨Årst collects the request rate for each\ntenant partition, as measured at each server. It then com-\nputes the partition demand proportions by normalizing\nthe rates. Scaling each tenant‚Äôs global fair share by these\npartition proportions determines the per-partition demand\nshare that each tenant should receive. Next, the controller\nsupplies the demand and node capacity constraints into\na bin-packing solver to compute a new partition assign-\nment relative to the existing one. Finally, the controller\nmigrates any newly (re)assigned partitions and updates\nthe mapping tables in its request router(s).\nPartition placement typically runs on a long timescale\n(every few minutes or hours), since we assume that tenant\ndemand distributions (proportions) are relatively stable,\nthough demand intensity (load) can Ô¨Çuctuate more fre-\nquently. However, it can also be executed in response\nto large-scale demand shifts, severe fairness violations,\nor the addition or removal of tenants or service nodes.\nWhile the bin-packing problem is NP-hard, simple greedy\nheuristics may suÔ¨Éce in many settings, albeit not achieve\nas high a utilization. After all, to achieve fairness, we only\nneed to Ô¨Ånd a feasible solution, not necessarily an optimal\none. Further, many eÔ¨Écient approximation techniques\ncan Ô¨Ånd near-optimal solutions [22, 29]. We intend to\nfurther explore partition placement in future work.\n3.2\nWeight Allocation\nOnce the tenant partitions are properly placed, weight\nallocation iteratively adjusts the local tenant shares (R)\nto match the demand distributions. As sketched in Al-\ngorithm 1, the weight-allocation algorithm on the con-\ntroller (i) detects tenant demand-share mismatch and (ii)\ndecides which tenant share(s) (weights) to adjust and by\nwhat amount, even while it (iii) maintains the global fair\nshare. A key insight is that any adjustment to tenant\nshares requires a reciprocal swap: if tenant t takes some\nrate capacity from a tenant u on some server, t must give\nthe same capacity back to u on a diÔ¨Äerent server. This\nswap allows the tenants to maintain the same aggregate\nshares even while local shares change. Each tenant‚Äôs local\nweight is initially set to its global weight, wt, and then\nadapts over time. To adapt to distribution shifts yet allow\nreplica selection to adjust to the current allocation, weight\nallocation runs in the medium timescale (seconds).\nDetecting mismatch: While it is diÔ¨Écult to directly\nmeasure tenant demand, the central controller can monitor\nthe rate each tenant receives at the service nodes. Weight\nallocation uses this information (D), together with the\nallocated shares (R), to approximate the demand-share\nmismatch that each tenant experiences on each node. We\ntried both a latency-based cost function using the M/M/1\nqueuing model of request latency‚Äîi.e., lt\nn = 1 / (Rt\nn ‚àíDt\nn)\nfor tenant t on node n‚Äîas well as a more direct rate-based\ncost. Unfortunately, in a work-conserving system, it can\nbe diÔ¨Écult to determine how much rate R was actually\nallocated to t, as it can vary depending on others‚Äô de-\nmand. For example, if a tenant t has weight wt\nn = 1\n4 on a\nnode with capacity cn, then its local rate under load is just\nÀÜRt\nn = 1\n4cn, even though Rt\nn ‚â•1\n4cn if t has excess demand\nand other tenants are not fully using their shares. Instead,\nby using the diÔ¨Äerence between the consumed rate and\nthe conÔ¨Ågured local share, et\nn =\n\u001f\u001f\u001f Dt\nn ‚àíÀÜRt\nn\n\u001f\u001f\u001f, we can largely\nignore the variable allocation and instead focus on the ten-\nant‚Äôs desired rate under full load (i.e., ÀÜR). Fortunately, the\nallocation algorithm can easily accommodate any convex\ncost function to approximate demand mismatch.\nDetermining swap: Since the primary goal of Pisces\nis fairness, weight allocation seeks to minimize the max-\nimum demand-share mismatch (or cost). However, giv-\ning additional rate capacity to the tenant t that suÔ¨Äers\nmaximal latency necessarily means taking away capac-\nity from another tenant u at the same node n. If too\nlarge, this rate (weight) swap may cause u‚Äôs cost to\nexceed the original maximum. To ensure a valid rate\nswap, the algorithm uses the linear bisection for latency,\ntake(t, u, n) = \u001e(Ru\nn ‚àíDu\nn) ‚àí(Rt\nn ‚àíDt\nn)\u001d \u001c 2, or the min of\nthe diÔ¨Äerences for rate: min(et\nn, eu\nn).\nMaintaining fair share: Before committing to a Ô¨Ånal\nswap, weight allocation must Ô¨Årst Ô¨Ånd a reciprocal swap\nto maintain the global fair share: if tenant t takes from\ntenant u at node n, then it must reciprocate at a diÔ¨Äerent\nnode. Given a reciprocal node m, the controller computes\nthe rate swap as the minimum of the take and give swaps,\nswap = min \u001etake(t, u, n), give(u, t, m)\u001d, and translates the\nrates into the corresponding local weight settings.",
        "semantic_similarity": 0.5535020232200623,
        "keyword_overlap": 0.010974822466107165,
        "combined_score": 0.3907438629938757,
        "paragraph_id": 4,
        "source_title": "Performance Isolation and Fairness for Multi-Tenant Cloud Storage"
      }
    ],
    "resolved_title": "Performance Isolation and Fairness for Multi-Tenant Cloud Storage"
  },
  {
    "citation_marker": "[18]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.45, overlap: 0.01): Based on the provided texts, the answer is **NO**.  \n\n**Reason:**  \nThe article's statement claims that co-location addresses \"idleness of over-provisioned resources\" and that applications require \"discrete performance levels,\" implying a need for fairness. However, the cited paragraph from [18] focuses exclusively on **defining fairness metrics** (e.g., sharing benefit/loss degrees) and **modeling resource allocation structures** (single-level/hierarchical) in YARN. It does not:  \n- Mention \"over-provisioned resources,\" \"idleness,\" or \"co-location\" as motivations.  \n- Discuss \"discrete performance levels\" of applications.  \n- Validate the premise that co-location necessitates fairness due to resource idleness.  \n\nThe cited text assumes a pre-existing shared system and delves into fairness quantification, unrelated to the article‚Äôs cited context. Thus, the citation [18] is misapplied to support claims it does not address.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.43, overlap: 0.01): **NO**  \n\n**Reason:** The cited paragraph describes a *specific technical implementation* (LTYARN's long-term max-min fairness mechanism, configuration arguments, and experimental setup), while the article's statement claims the cited source supports a *broad premise* (\"data centers co-locate applications to overcome resource idleness,\" requiring \"fair resource allocation\"). The paragraph **does not substantiate** the article's assertion about the motivation (overcoming idleness of over-provisioned resources) or the general need for fairness in co-located environments. Instead, it details a scheduler's design and evaluation. The article distorts the focus of the citation.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.41, overlap: 0.01): **NO**  \nThe statement is **not accurately supported** by the cited paragraph.  \n\n**Reason:**  \nThe article's statement claims that co-location requires fair allocation because \"each application requires a discrete level of performance.\" However, the cited paragraph:  \n1. Focuses on **user-centric fairness** (resource-as-you-contributed fairness), where resource allocation is proportional to a user's *contribution* to the cloud.  \n2. Describes YARN's architecture and its fairness policies (e.g., max-min fairness, DRF), which prioritize *user/queue-level fairness*, not application-specific performance guarantees.  \n3. Defines properties (e.g., sharing incentive, strategy-proofness) tied to **user behavior and contributions**, not discrete application performance levels.  \n\nThe cited content addresses fairness in *user resource sharing* based on contributions, while the article misattributes it to *application performance heterogeneity* in co-location. This misrepresents the source‚Äôs core focus.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "of fairness, which is used to assess fair policies in the follow-\ning sections.\n4.1\nSystem Model\nThis paper considers the single-resource allocation fairness\n(e.g., memory) for YARN in the most commonly used homoge-\nneous environment [3]. Let M ¬º f1 . . . mg be the set of\nmachines (or instances) in the shared computing system. For\neach machine, we assume the amount of resources (e.g., mem-\nory) is Ri. Thus, the total resource capacity R of the system is\nR ¬º Pm\ni¬º1 Ri. Let N ¬º f1 . . . ng be the set of users in the\nshared computing systems. Assume that the resource contri-\nbutions (i.e., shared weights) for n users are W ¬º fw1 . . . wng.\nAccording to the practical needs of resource allocation, these\nusers can be grouped into multiple queues of either single-\nlevel resource allocation structure or hierarchical resource\nallocation structure in YARN.\nWithout loss of generality, for example, there is a cloud\nsystem consisting of 100 instances of t2.medium type on the\ncloud, contributed by four users A; B; C and D with diverse\ndata-intensive workloads (e.g., MapReduce, Tez, HIVE, and\nSpark) equally. In that case, we can establish a shared com-\nputing system with YARN. According to practical needs,\nthe four users can be organized either into a single group\nfor single-level resource allocation (e.g., Fig. 1a) or multiple\ngroups for hierarchical resource allocation (e.g., Fig. 1b ).\nIn our work below, we focus on the fairness for these two\ntypes of resource allocation structures for n users, namely,\nsingle-level resource allocation (Section 5.1) and hierarchical\nresource allocation (Section 5.2).\n4.2\nFairness DeÔ¨Ånition\nWe consider the fairness from the resource allocation\nperspective. In a shared cloud environment, ideally,\nevery user wants to get more resources or at least the\nsame amount of resources in a shared computing system\nthan the ones of exclusively using her partition of the\nsystem. We call it fair for a user (i.e., sharing beneÔ¨Åt)\nwhen that can be achieved. In contrast, due to the\nresource contention in the shared system, it is also possi-\nble for the total resources a user received are less than\nthat without sharing, which we call unfair (i.e., sharing\nloss). To ensure resource-as-you-contributed fairness and\nthe maximization of sharing incentive property in the\nshared system, it is important to minimize sharing loss\nÔ¨Årstly and then maximize sharing beneÔ¨Åt.\nIn the remainder of this paper, we refer to the total\nresources as accumulated resources along the time. Let gi√∞t√û\nbe the currently allocated resources for the ith user at time t.\nLet fi√∞t√û denote the accumulated resources for the ith user at\ntime t. Thus,\nfi√∞t√û ¬º\nZ t\n0\ngi√∞t√û dt:\n(1)\nLet di√∞t√û and Si√∞t√û denote the current demand and current\nresource share for the ith user at time t, respectively. Given\nthe total resource capacity R of the system and the shared\nweight wi for the ith user, there is\nSi√∞t√û ¬º R \u0003 wi\n\u0002 X\nn\nk¬º1\nwk:\n(2)\nThe fairness degree ri√∞t√û for the ith user at time t is\ndeÔ¨Åned as the normalization result of the amount of resour-\nces a user obtained in a shared environment with respect to\nthe non-shared environment, i.e.,\nri√∞t√û ¬º\nAllocationResultWithSharing\nAllocationResultWithoutSharing ¬º\nR t\n0 gi√∞t√û dt\nR t\n0 minfdi√∞t√û; Si√∞t√ûgdt\n:\n(3)\nri√∞t√û \u0004 1 implies the absolute resource fairness for the ith\nuser at time t. In contrast, ri√∞t√û < 1 indicates unfair. We can\neasily see that for a user i in a non-shared partition of the\nsystem, it always holds ri√∞t√û ¬º 1, since it has gi√∞t√û ¬º\nminfdi√∞t√û; Si√∞t√ûg at any time t in this scenario. To measure\nhow much better or worse for sharing with a fair policy\nthan without sharing (i.e., ri√∞t√û \u0005 1), we propose two con-\ncepts sharing beneÔ¨Åt degree and sharing loss degree to quantify\nit, respectively. Let C√∞t√û be sharing beneÔ¨Åt degree, as a sum of\nall √∞ri√∞t√û \u0005 1√û subject to ri√∞t√û \u0004 1, i.e.,\nC√∞t√û ¬º\nX\nn\ni¬º1\nmaxfri√∞t√û \u0005 1; 0g;\n(4)\nand let V√∞t√û denote sharing loss degree, as a sum of all\n√∞ri√∞t√û \u0005 1√û subject to ri√∞t√û < 1, i.e.,\nV√∞t√û ¬º\nX\nn\ni¬º1\nminfri√∞t√û \u0005 1; 0g:\n(5)\nThereby, it always holds that C√∞t√û \u0004 0 \u0004 V√∞t√û. Moreover,\nwe see that in a non-shared partition of the computing sys-\ntem, it always holds C√∞t√û ¬º V√∞t√û ¬º 0, indicating that there\nare neither sharing beneÔ¨Åt nor sharing loss. In contrast, in a\nshared cloud computing system, either of them could be\nnonzero. For a good fair policy, it should be able to maxi-\nmize V√∞t√û Ô¨Årst (e.g., V√∞t√û ! 0) and next try to maximize\nC√∞t√û as much as possible. Finally, we can use this two met-\nrics to compare the fairness among different policies.\n5\nFAIR POLICY DESIGN AND PRINCIPLE FOR YARN\nIn this section, we give our design and principle of fair poli-\ncies for YARN under cloud computing environment. Both\nsingle-level resource allocation and hierarchical resource\nallocation are considered.\n5.1\nSingle-Level Resource Allocation\nFor single-level resource allocation, we Ô¨Årst give a motiva-\ntion example to show that MemoryLess Resource Fairness\nis not suitable for cloud computing system. Then we\nFig. 1. A comparison example of the long-term resource fairness\nbetween single-level and multi-level resource allocations for four users\nA; B; C and D with equal share of the whole resource service, where Gi\ndenotes the ith group and G0 represents the total resource service of\nthe whole system.\nTANG ET AL.: FAIR RESOURCE ALLOCATION FOR DATA-INTENSIVE COMPUTING IN THE CLOUD\n23\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:32:54 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.45003825426101685,
        "keyword_overlap": 0.011627906976744186,
        "combined_score": 0.31851515007573505,
        "paragraph_id": 3,
        "source_title": "Fair Resource Allocation for Data-Intensive Computing in the Cloud"
      },
      {
        "text_fragment": "RC Ô¨Årst estimates the assumed execution time for each\nrunning/completed task with the updated quantum value\nfrom QU. Next, it computes and updates the accumulated\nresource for each application/queue.\n6.2.3\nResource Allocator\nResource Allocator locates at each queue of different levels,\nas shown in Fig. 4. It is triggered whenever there are pend-\ning tasks and idle resources. RA can now support FIFO,\nmemoryless max-min fairness and long-term max-min fair-\nness for each queue. Users can choose either of them\naccordingly. For long-term max-min fairness, it performs\nfair resource allocation for each application/queue with\nthe provided resource information from RC, based on For-\nmula (6). We provide two important conÔ¨Åguration argu-\nments for each queue, e.g., time quantum Q and round\nlength L in the default conÔ¨Åguration Ô¨Åle, to meet different\nrequirements for different queues. Moreover, we also sup-\nport minimum (maximum) resource share for queues\nunder long-term max-min fairness.\n7\nEVALUATION\nWe ran our experiments in a local cloud environment,\nwhich is established in a cluster consisting of 10 compute\nnodes, each with two Intel X5675 CPUs (six CPU cores per\nCPU with 3.07 GHz), 24 GB DDR3 memory. We emulate\nthe t2:medium instances of Amazon EC2 by conÔ¨Åguring h2\ncores, 4 GBi per VM and thereby create 6 VMs per node.\nThe Hadoop-2.2.0 is chosen in our experiment. We conÔ¨Åg-\nure 1 instance as master, and the remaining 59 instances\nas slaves.\nOur evaluation methodology is as follows. We Ô¨Årst con-\nstruct a single-level hierarchy for LTYARN with the four\nmacro-workloads below. We compare LTYARN with the\ndefault Hadoop Fair Scheduler (HFS). Second, we construct\na two-level hierarchy for LTYARN by grouping four macro-\nworkloads into groups to assess our design on hierarchical\nresource allocations.\n7.1\nMacro-Benchmarks\nTo evaluate our long-term fair scheduler LTYARN for\nYARN, we ran a macro-benchmark consisting of four differ-\nent workloads:\n\u0001\nA MapReduce instance with a mix of small and large\njobs based on the workload at the Facebook.\n\u0001\nA MapReduce instance running a set of large-size\nbatch\njobs\ngenerated\nwith\nPurdue\nMapReduce\nBenchmarks Suite [33].\n\u0001\nHive [25] running a series of TPC-H queries.\n\u0001\nSpark [13] running a series of machine learning\napplications.\n7.2\nMacro-Workloads\nSynthetic facebook workload. We synthesize our Facebook work-\nload based on the distribution of jobs sizes and inter-arrival\ntime at Facebook in Oct. 2009 provided by Zaharia et. al. [20].\nThe workload consists of 100 jobs. We categorize them into\nnine bins of job types and sizes, as listed in Table 3. It is a mix\nof large number of small-sized jobs (1 \u0007 15 tasks) and small\nnumber of large-sized jobs (e.g., 800 tasks3). The job submis-\nsion time is derived from one of SWIM‚Äôs Facebook workload\ntraces (e.g., FB-2009_samples_24_times_1hr_1.tsv) [34]. The\njobs are from Hive benchmark [35], containing four types of\napplications, i.e., rankings selection, grep search (selection),\nuservisits aggregation and rankings-uservisits join.\nPurdue workload. We select Ô¨Åve benchmarks (e.g., Word-\nCount, TeraSort, Grep, InvertedIndex, HistogramMovices)\nrandomly from Purdue MapReduce Benchmarks Suite [33].\nWe use 40 G wikipedia data [36] for WordCount, Inverte-\ndIndex and Grep, 40G generated data for TeraSort and His-\ntogramMovices with their provided tools. To emulate a\nseries of regular job submissions in a data warehouse, we\nsubmit these Ô¨Åve jobs sequentially at a Ô¨Åxed interval of 3\nmins to the system.\nTPC-H. To emulate continuous analytic query, such as\nanalysis of users‚Äô behavior logs, we ran TPC-H benchmark\nqueries on hive [14]. 40 GB data are generated with pro-\nvided data tools. Four representative queries Q1, Q9, Q12,\nQ17 are chosen, each of which we create Ô¨Åve instances. We\nwrite a script to launch each one after the previous one Ô¨Ån-\nished in a round robin fashion.\nSpark. Latest version of Spark has supported its job to run\non the YARN system. We consider two CPU-intensive\nmachine learning algorithms, namely, kmeans and alternat-\ning least squares (ALS) with provided example bench-\nmarks. We ran 10 instances of each algorithm, which are\nlaunched by a script that waits 2 minutes after each job com-\npleted to submit the next.\nWe conÔ¨Ågure a single-level hierarchy with the four work-\nloads, as shown in Fig. 5. Each leaf queue corresponding\nto a workload. We use it for the following sections, i.e., Sec-\ntions 7.3, 7.4, 7.5, and Appendix D of the supplemental\nmaterial, available online. Besides, the hierarchy of more\nlevels is given and used in Section 7.6.\nTABLE 3\nJob Types and Sizes for Each Bin in Our Synthetic\nFacebook Workloads\nBin\nJob Type\n# Maps\n# Reduces\n# Jobs\n1\nrankings selection\n1\nNA\n38\n2\ngrep search\n2\nNA\n18\n3\nuservisits aggregation\n10\n2\n14\n4\nrankings selection\n50\nNA\n10\n5\nuservisits aggregation\n100\n10\n6\n6\nrankings selection\n200\nNA\n6\n7\ngrep search\n400\nNA\n4\n8\nrankings-uservisits join\n400\n30\n2\n9\ngrep search\n800\n60\n2\nFig. 5. Single-level hierarchical for LTYARN. Four different leaf queues\nare conÔ¨Ågured, i.e., Facebook, Purdue, Spark, TPC-H, corresponding to\neach workload, respectively.\n3. We reduce the size of the largest jobs in [20] to have the workload\nÔ¨Åt our cluster size.\n28\nIEEE TRANSACTIONS ON SERVICES COMPUTING,\nVOL. 11,\nNO. 1,\nJANUARY/FEBRUARY 2018\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:32:54 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.42848527431488037,
        "keyword_overlap": 0.010388739946380697,
        "combined_score": 0.3030563140043305,
        "paragraph_id": 8,
        "source_title": "Fair Resource Allocation for Data-Intensive Computing in the Cloud"
      },
      {
        "text_fragment": "single user‚Äôs application is changing over time, implying\nthat it is hard to keep the high resource utilization all the\ntime. More effective resource sharing is critical for improv-\ning the resource utilization in the cloud.\nIn this paper, let‚Äôs consider a cloud-based computing sys-\ntem shared by n users, where user i has a resource contribu-\ntion of ki to the pool of cloud resources. To enable resource\nsharing sustainable between users in the long run, we\nshould guarantee the proportional relationship between\nthe amount of total resources a user used over a period of\ntime and the amount of resources contributed by the user to\nthe shared cloud (i.e., resource-as-you-contributed fairness).\nOur aim thus turns to explore a fair resource allocation pol-\nicy that can meet all the aforementioned good properties\nlisted in Section 3.\n2.2\nResource Management in YARN\nYARN, as the next generation of Hadoop (i.e., Hadoop\nMRv2), has evolved to be a large-scale data operating plat-\nform and cluster resource management system. There is a\nnew architecture for YARN, which separates the resource\nmanagement from the computation model. Such a separa-\ntion enables YARN to support a number of diverse data-\nintensive computing frameworks including Dryad [6],\nGiraph, Hoya, Spark [13], Storm [12] and Tez. In YARN‚Äôs\narchitecture, there is a global master named ResourceManager\n(RM) and a set of per-node slaves called NodeManagers(NM),\nwhich forms a generic system for managing applications in a\ndistributed manner. The RM is responsible for tracking and\narbitrating resources among applications. In contrast, the\nNM has responsibility for launching tasks and monitoring\nthe resource usage per slave node. Moreover, there is another\ncomponent called ApplicationMaster(AM), which is a frame-\nwork-speciÔ¨Åc entity. It is responsible for negotiating resour-\nces from the RM and working with the NM to execute and\nmonitor the progress of tasks. Particularly, all resources of\nYARN are requested in the form of ‚Äòcontainer‚Äô, which is a logi-\ncal bundle of resources (e.g., <1 CPUs, 2G memory> ).\nAs a multi-tenant platform, YARN organizes users‚Äô sub-\nmitted\napplications\ninto\nqueues\nand\nshare\nresources\nbetween these queues. Users can set their own queues in a\nconÔ¨Åguration Ô¨Åle provided by YARN. When all users‚Äô\nqueues are conÔ¨Ågured at the same level, the cluster resour-\nces will then be allocated at one level, which we call the\nsingle-level resource allocation. Moreover, to reÔ¨Çect the hier-\narchical tree structure for organizations of users in practice,\nYARN also supports hierarchical queues of tree topology.\nEach queue can represent an organization or a user. In the\ntree topology, there is a root node called Root Queue. It dis-\ntributes the resources of the whole system to the intermedi-\nate nodes called Parent Queues. Each parent queue further\nre-distributes resources into its sub-queues (parent queues\nor leaf queues) recursively until to the bottom nodes called\nLeaf Queues. Finally, users‚Äô submitted applications within\nthe same leaf queue share the resources. We call this alloca-\ntion as hierarchical resource allocation.\nThere is a Fair Scheduler [22] inside YARN, which can\nsupport both single-level and hierarchical resource alloca-\ntions. Moreover, both single-resource and multi-resource\nallocations are also supported. For the single-resource allo-\ncation, current version of YARN adopts the max-min fair\npolicy and focuses only on the memory resources. With\nregard to the multi-resource allocation, it takes the Domi-\nnant Resource Fairness (DRF) [23] and considers both CPU\nand memory resources.\nIn our paper, we focus on the single-resource allocation\nfor YARN by considering both single-level and hierarchical\nresource allocation in cloud computing. We remain the con-\nsideration for multi-resource allocation as future work.\n3\nCLOUD-ORIENTED RESOURCE ALLOCATION\nPROPERTIES\nWe present a set of desirable properties for cloud comput-\ning. Based on these properties, we design our fair allocation\npolicies for YARN. We have found the following Ô¨Åve impor-\ntant properties:\n\u0001\nSharing incentive: Each user should be better off shar-\ning the resources with others, than exclusively using\nthe resources individually. Consider a cloud system\nequally shared by n users over t period time. Then\neach user should get at least t \u0003 1\nn resources in the\nshared system.\n\u0001\nCost-efÔ¨Åcient workload incentive: Resources in the\ncloud are priced (i.e., not free). In a shared cloud sys-\ntem, we should encourage users to submit work-\nloads that generate positive utility to them (i.e., cost-\nefÔ¨Åcient workload) for cost efÔ¨Åciency and avoid\nthose spam workloads with no positive utility (i.e.,\ncost-inefÔ¨Åcient workload). That is, a user should be\nbetter off submitting cost-efÔ¨Åcient workload and\nyielding unused resources to others when not\nneeded. Otherwise, she may be selÔ¨Åsh and possesses\nall unneeded resources under her share by submit-\nting cost-inefÔ¨Åcient tasks in a shared computing\nenvironment.\n\u0001\nResource-as-you-contributed\nfairness:\nIn\nthe\ncloud,\nassume that each user contributes a certain number\nof machines (resources) to its common pool of\nmachines\n(resources).\nThen,\nthe\naccumulated\nresource that each user used received over time\nshould be in proportion to her contribution in the\nshared environment. This property is important as it\nis a Service-Level Agreement guarantee for users.\n\u0001\nStrategy-proofness: Users should not be able to get\nbeneÔ¨Åts by lying about their resource demands. This\nproperty is compatible with sharing incentive and\nresource-as-you-contributed fairness, since no user\ncan obtain more resources by lying.\n\u0001\nPareto efÔ¨Åciency: In a shared resource environment, it\nis impossible for a user to get more resources with-\nout decreasing the resource of at least one user. This\nproperty can ensure the system resource utilization\nto be maximized.\nAlthough this paper is focused on YARN, it is worth\nmentioning that our methodology can be applied to other\nresource management systems such as Mesos [10].\n4\nSYSTEM MODEL AND FAIRNESS DEFINITION\nIn this section, we Ô¨Årst model the single-resource allocation\nfor YARN in cloud environment. Next, we give the deÔ¨Ånition\n22\nIEEE TRANSACTIONS ON SERVICES COMPUTING,\nVOL. 11,\nNO. 1,\nJANUARY/FEBRUARY 2018\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:32:54 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.4125661253929138,
        "keyword_overlap": 0.010968921389396709,
        "combined_score": 0.29208696419185864,
        "paragraph_id": 2,
        "source_title": "Fair Resource Allocation for Data-Intensive Computing in the Cloud"
      }
    ],
    "resolved_title": "Fair Resource Allocation for Data-Intensive Computing in the Cloud"
  },
  {
    "citation_marker": "[19]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: **Answer:** YES  \n\n**Reason:** The article's statement (\"FLIN: Enabling Fairness and Enhancin\") accurately reflects the cited paragraph's core contribution. The paragraph explicitly states that FLIN is a fairness-enhancing scheduler designed for NVMe SSDs, addressing interference-induced unfairness caused by the removal of OS-level mechanisms. Key supporting points from the cited text include:  \n- FLIN is introduced to \"alleviate unfairness\" (directly matching \"Enabling Fairness\").  \n- It \"provides fairness among requests from different applications\" and improves \"fairness and performance\" (aligning with the statement).  \n- The context (modern SSDs lacking fairness mechanisms) is faithfully represented.  \n\nThere is no misrepresentation, distortion, or fabrication of the cited work's purpose or claims."
    ],
    "evidence": [
      {
        "text_fragment": "FLIN: Enabling Fairness and Enhancing Performance\nin Modern NVMe Solid State Drives\nArash Tavakkol‚Ä†\nMohammad Sadrosadati‚Ä†\nSaugata Ghose‚Ä°\nJeremie S. Kim‚Ä°‚Ä†\nYixin Luo‚Ä°\nYaohua Wang‚Ä†¬ß\nNika Mansouri Ghiasi‚Ä†\nLois Orosa‚Ä†‚àó\nJuan G√≥mez-Luna‚Ä†\nOnur Mutlu‚Ä†‚Ä°\n‚Ä†ETH Z√ºrich\n‚Ä°Carnegie Mellon University\n¬ßNUDT\n‚àóUnicamp\nModern solid-state drives (SSDs) use new host‚Äìinterface pro-\ntocols, such as NVMe, to provide applications with fast access\nto storage. These new protocols make use of a concept known\nas the multi-queue SSD (MQ-SSD), where the SSD has direct\naccess to the application-level I/O request queues. This removes\nmost of the OS software stack that was used in older protocols\nto control how and when the I/O requests were dispatched to\nstorage devices. Unfortunately, while the elimination of the OS\nsoftware stack leads to a significant performance improvement,\nwe show in this paper that it introduces a new problem: unfair-\nness. This is because the elimination of the OS software stack\neliminates the mechanisms that were used to provide fairness\namong applications in older SSDs.\nTo study application-level unfairness, we perform experiments\nusing four real state-of-the-art MQ-SSDs. We demonstrate that\nthe lack of fair scheduling mechanisms leads to high unfairness\namong concurrently-executing applications due to the interfer-\nence among them. For instance, when one of these applications\nissues many more I/O requests than others, the other applications\nare slowed down significantly. We perform a comprehensive\nanalysis of interference in real MQ-SSDs, and find four major\ninterference sources: (1) the intensity of requests sent by each\napplication, (2) differences in request access patterns, (3) the\nratio of reads to writes, and (4) garbage collection.\nTo alleviate unfairness in MQ-SSDs, we propose the Flash-\nLevel INterference-aware scheduler (FLIN). FLIN is a lightweight\nI/O request scheduling mechanism that provides fairness among\nrequests from different applications. FLIN uses a three-stage\nscheduling algorithm that protects against all four major sources\nof interference, while respecting the application-level priorities\nassigned by the host. FLIN is implemented fully within the\nSSD controller firmware, requiring no new hardware, and has\nnegligible (<0.06%) storage cost. Compared to a state-of-the-art\nI/O scheduler, FLIN improves the fairness and performance of a\nwide range of enterprise and datacenter storage workloads, with\nan average improvement of 70% and 47%, respectively.\n1. Introduction\nSolid-state drives (SSDs) are widely used as a storage\nmedium today due to their high throughput, low response\ntime, and low power consumption compared to conventional\nhard disk drives (HDDs). As more SSDs are deployed in data-\ncenters and enterprise platforms, there has been a continued\nneed to improve SSD performance. One area where SSD man-\nufacturers have innovated on SSD performance is the host‚Äì\ninterface protocol, which coordinates communication between\napplications and the SSD. SSDs initially adopted existing host‚Äì\ninterface protocols (e.g., SATA [88]) that were originally de-\nsigned for lower-performance HDDs. As the performance of\nthe underlying storage technology (e.g., NAND flash memory)\nused by the SSD increased, these host‚Äìinterface protocols\nbecame a significant performance bottleneck [114], mainly\nbecause these protocols rely on the OS to manage I/O requests\nand data transfers between the host system and the SSD.\nTo overcome this bottleneck, modern enterprise SSDs (e.g.,\n[30‚Äì33,65,66,87,103,104,110,111]) use new high-performance\nprotocols, such as the Non-Volatile Memory Express (NVMe)\nprotocol [21,83]. These new protocols make use of the multi-\nqueue SSD (MQ-SSD) [8,48,101] concept, where multiple host-\nside I/O request queues (in software) are directly exposed to\nthe SSD controller. There are two benefits to directly expos-\ning the request queues to the SSD controller: (1) there is no\nlonger any need for the OS software stack to manage the I/O\nrequests; and (2) the SSD can make more effective I/O request\nscheduling decisions than the OS, since the SSD knows ex-\nactly which of its internal resources are available or are busy\nserving other requests. Thus, the protocols eliminate the OS\nsoftware stack, enabling MQ-SSDs to provide significantly\nhigher performance than traditional SSDs [101,114].\nUnfortunately, eliminating the OS software stack also elim-\ninates critical mechanisms that were previously implemented\nas part of the stack, such as fairness control [6,8,35,55,56,84,\n89,106,118]. Fairness control mechanisms work to equalize\nthe effects of interference across applications when multiple\napplications concurrently access a shared device. Fairness is\na critical requirement in multiprogrammed computers and\nmulti-tenant cloud environments, where multiple I/O flows\n(i.e., series of I/O requests) from different applications concur-\nrently access a single, shared SSD [36,43,84,91,94,101].\nFor older host‚Äìinterface protocols, the OS software stack\nprovides fairness by limiting the number of requests that each\napplication can dispatch from the host to the SSD. Since fair-\nness was handled by the OS software stack, the vast majority\nof state-of-the-art SSD I/O request schedulers did not con-\nsider request fairness [20,36,37,40,77,90,112]. Surprisingly,\neven though new host‚Äìinterface protocols, such as NVMe, do\nnot use the OS software stack, modern MQ-SSDs still do not\ncontain any fairness mechanisms. To demonstrate this, we\nperform experiments using four real state-of-the-art enter-\nprise MQ-SSDs. We make two major findings. First, when two\napplications share the same SSD, one of the applications typi-\ncally slows down (i.e., it takes more time to execute compared\nto if it were accessing the SSD alone) significantly. When we\nrun a representative workload on our four SSDs, we observe\nthat such slowdowns range from 2x to 106x. Second, with the\nremoval of the fairness control mechanisms that were in the\nsoftware stack, an application that makes a large number of\nI/O requests to an MQ-SSD can starve requests from other\napplications, which can hurt overall system performance and\nlead to denial-of-service issues [8,84,101]. Therefore, we con-\nclude that there is a pressing need to introduce fairness control\nmechanisms within modern SSD I/O request schedulers.\nIn order to understand how to control fairness in a modern\nSSD, we experimentally analyze the sources of interference\namong I/O flows from different applications in an SSD using\nthe detailed MQSim simulator [101]. Our experimental results\nenable us to identify four major types of interference:\n1. I/O Intensity: The SSD controller breaks down each I/O\nrequest into multiple page-size (e.g., 4 kB) transactions. An\n397\n2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture\n2575-713X/18/$31.00 ¬©2018 IEEE\nDOI 10.1109/ISCA.2018.00041\nAuthorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on June 10,2025 at 11:34:02 UTC from IEEE Xplore.  Restrictions apply.",
        "semantic_similarity": 0.5644833445549011,
        "keyword_overlap": 0.024266365688487584,
        "combined_score": 0.40241825089497707,
        "paragraph_id": 0,
        "source_title": "FLIN: Enabling Fairness and Enhancing Performance in Modern NVMe Solid State Drives"
      }
    ],
    "resolved_title": "FLIN: Enabling Fairness and Enhancing Performance in Modern NVMe Solid State Drives"
  },
  {
    "citation_marker": "[1]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.68, overlap: 0.03): **NO**  \n\n**Reason:** The article statement claims FIFO-based locks exacerbate the convoy effect, but the cited paragraph explicitly states:  \n> *\"Spin locks eliminate convoys (as explained below convoys are caused by first-come first-served scheduling, spin locks don't have FCFS the discipline).\"*  \n\nThe cited source attributes convoys to FCFS/FIFO scheduling **and states that spin locks (non-FIFO) eliminate convoys**. This directly supports the article's claim that FIFO locks worsen convoys. The source further explains that FIFO order causes convoys and proposes random order as a solution, aligning with the article's assertion. No distortion or fabrication is present.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.65, overlap: 0.04): **NO**  \n*Reason:* The cited paragraph describes FIFO locks (explicitly \"first-come first-served order\") as causing convoys when a slow process holds a high-traffic lock, forcing faster processes to queue behind it. The article's statement accurately reflects this:  \n- **FIFO-based locks** ‚Üí Explicitly named in the source.  \n- **Exacerbate convoy effect** ‚Üí Source confirms FIFO queuing causes backups when slow holders block others.  \n- **Short threads suspended by long threads** ‚Üí Source uses the \"slow car\" analogy and quantifies delays (e.g., 100-instruction hold time blocking others).  \n\nThe source directly supports the article‚Äôs claims about FIFO locks and convoy dynamics. No distortion or fabrication is present.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.60, overlap: 0.02): **NO**\n\n**Reason:**  \nThe article's statement attributes convoy effect exacerbation specifically to locks \"without scheduling abilities, such as FIFO-based locks.\" However, the cited paragraph:  \n1. **Identifies pre-emptive scheduling** (e.g., quantum runout/page faults) while holding high-traffic locks as the primary cause of convoys.  \n2. **Proposes solutions** like avoiding pre-emption for high-traffic locks, lock granularity refinement, and non-exclusive modes‚Äî*not* scheduling-aware locks.  \n3. **Never mentions FIFO-based locks** or \"scheduling abilities\" as causal factors. The convoy mechanism described is tied to pre-emption during lock-holding, not inherent lock design.  \n\nThe article distorts the source by incorrectly ascribing the convoy problem to FIFO locks' lack of scheduling capabilities, while the source exclusively blames pre-emptive scheduling and high-traffic locks.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "for the same lock (decreasing the traffic on a particular lock). \nThese techniques make convoys less likely and less stable. \nBut we suspect that convoys will \ncontinue to occur. In particular, there was no easy way to fix system entry-exit lock convoys \nusing mode or granularity techniques (we had to resort to special logic for this problem). \nWe also considered two strategies which seem to have few virtues: spin locks and integration of \nthe dispatcher and lock manager. \nSpin locks come in two flavors: \no \nBusy wait: holds the CPU until quantum runout. \n¬∞ \nLazy wait: branches to the dispatcher and tests the lock the next time it is dispatched. \nSpin locks eliminate convoys (as explained below convoys are caused by first-come first-served \nscheduling, spin locks don't have FCFS the discipline). In one set of experiments we performed, \nbusy wait locks increased system execution time (elapsed) by 75%. Lazy wait locks increased \nexecution time by 20%. That is, the CPU time wasted by spinning is greater than the cost of \nconvoys. \nAnother way to solve the convoy problem is to involve the dispatcher. \nNotice in the example \nthat P1 stupidly gave up the lock to P2. If P1 had hung on to the lock until it waited, and P2 \ndid the same then the convoy would flush itself immediately. The obvious solution is to have the \ndispatcher know about locks and have the dispatcher grant locks when tasks are switched. The \narguments against this approach are: \no \nThe book-keeping associated with giving up a lock at task switch is intimidating. \no \nFor reasons of modularity, the dispatcher should not know about locks, they are a higher \nlevel notion. \n‚Ä¢ \nThe solution does not generalize to multiple processors. \n¬∞ \nThe solution does not address pre-emption due to page faults. \nA SOLUTION \nThe key issue of convoys is associated with the granting of locks in first-come first-served order. \nSo we elect to grant all lock requests in random order in the hope that eventually everyone will \nget service. In theory, some process might ' starve' (never be granted its request) but in fact \nthe underlying scheduler and the stochastic nature of real systems cause each process to \neventually get service. \nThe proposed solution is: \nWhen releasing a lock, broadcast to all waiters that the lock is free: \nsp \nDO; \nCONVOY=LATCH.QUEUE; \n,/* atomic pair */ \nLATCH=FREE; \n/* atomic pair */ \nDO WHILE(CONVOY -,= NIL); \nWAKEUP FIRST OF CONVOY; /* CAR of list */ \nCONVOY = REST OF CONVOY;/* CDR of list */ \nEND; \nEND; \n23",
        "semantic_similarity": 0.6800855994224548,
        "keyword_overlap": 0.02567313713212273,
        "combined_score": 0.48376186073535515,
        "paragraph_id": 3,
        "source_title": "The convoy phenomenon"
      },
      {
        "text_fragment": "THE CONVOY PHENOMENON \nMike Blasgen, \nJim Gray, \nMike Mitoma, \nTom Price \nIBM San Jose Research Laboratory \nSan Jose, California, 95193 \nMay 1977, (Revised Janruary 1979) \nCONVOYS DESCRIBED \nWhen driving on a two-lane road with no passing one often encounters clusters of cars. This is \nbecause a fast-moving car will soon bump into a slow one. \nThe equilibrium state of such a \nsystem is for everyone to be in a convoy behind the slowest car. \nIn System R [Astrahan], transactions often bump into one another when contending for shared \nresources. This contention appears as conflicting requests for locks. (You may know locks by \nthe names semaphore, monitor, latch or queue.) Typically, access to these resources follows the \nprotocol: \nLOCK <resource>; \n<operate on resource>; \nUNLOCK <resource>; \nIf other processes request the lock while it is granted then they are placed in a queue of waiters \nand suspended. When the lock becomes available, requests are granted in first-come first-served \norder. Setting and clearing a lock costs ten instuctions if no waiting is involved. If waiting is \ninvolved,it costs 50 instructions plus two process dispatches (i.e. several thousand instructions). \n(Note: the System R lock manager is much fancier than this [Gray], [Nauman].) \nIn what follows three statistics about a particular lock will be of interest: The execution interval \nof a lock is the average number of instructions executed between successive requests for that \nlock by a process. The duration of a lock is the average number of instructions executed while \nthe lock is held. The collision cross section of a lock is the fraction of time it is granted. In a \nuni-processor the collision cross section is (duration/(duration+interval)) ignoring the wait time \nand task switching time if a request must wait. There are three high-traffic locks in System R \nregulating access to the buffer pool, recovery log and to system entry/exit. Estimates of these \nstatistics for the high traffic resources of System R are shown below. \nRESOURCE \nl EXECUTION \n[ DURATION \nI CROSS \nI INTERVAL \nI HELD \nI SECTION \nI (ins%ruc%ions) \nI (ins%ruc¬±ions) \nI \nBUFFER. POOL \nENTRY-EXIT \nLOG \nI \n1000 \nl \n60 \nI \n6z \nI \n1500 \nI \n70 \nI \n5z \nl \nzoooo \n[ \n300~ \nl \n1.5z \n-+ ................ \n~ \n--+ ......... \n+ \nt The log lock is sometimes held during a log write to disk so this is an average. \nIn the remainder consider a hypothetical lock L with an execution interval of 1000 instructions, \na duration of 100 instructions and hence a cross section of 10%. \nConsider what happens if a process P1 stops (goes into wait state) while it holds high traffic lock \nL: \n20",
        "semantic_similarity": 0.653072863817215,
        "keyword_overlap": 0.037827943868212324,
        "combined_score": 0.46849938783251416,
        "paragraph_id": 0,
        "source_title": "The convoy phenomenon"
      },
      {
        "text_fragment": "If these rules are followed and if I/O is frequent enough so that almost every quantum ends with \nan I/O wait or a low-traffic lock wait then the high traffic locks should almost always be free \nwhen a task sleeps. \nWith probability P a process ends its quantum with an I/O wait or a low-traffic lock wait. P is \nvery close to 1. However, 1-P is not zero if the scheduler is pre-emptive (because the process \nmay not do any I/O for a long time thereby getting quantum runout or it may page fault.) So if \nthe lock is held 10% of the time, some one will sleep holding the lock with probability 0.t0(1-P). \nThis probability is small, but its consequences are so disasterous as to make it a real problem. \nNamely, such an event will create a convoy on that lock. \nHence, the dispatcher should never interrupt a process holding a high-traffic lock (low traffic \nlocks do not create convoys). Put another way pre-emptive scheduling is bad for a transaction \nsystem. \nThe consequence of pre-empting a process which holds a high traffic lock is that a \nconvoy will immediately form on the lock and that it will persist for a very long time. \n(Note \nthat page faults are a source of pre-emption). \nMost of us are stuck with a pre-emptive scheduler (i.e. general purpose operating system with \nvirtual memory). Hence convoys will occur. The problem is to make them evaporate quickly \nwhen they do occur rather than have them persist forever. \nBefore adopting the solutions outined below, 92% of lock waits were for the three high traffic \nlocks. \nAfter adopting the solutions described below, lock waits on high traffic locks were \nreduced by a factor of ten and only 40% of lock waits were for the high traffic locks. \nPOSSIBLE SOLUTIONS \nThe simplest solution of all is to run one transaction at a time. \nThen no locking is required. \nHowever, our experiments indicate that even with convoys the response and throughput of \nSystem R is improved by multi-programming. This is because I/O and computation can overlap \nand because short transactions are not delayed by very long ones. \nThe next simplest solution is to avoid locks. One can go a very long way with shrewd use of \natomic machine instructions (compare and swap) and other programming tricks. \nFor example, \nthe system entry-exit lock was eliminated by such techniques. We have done a lot of this, but \nhave been unable to completely eliminate high traffic locks from our programs. \nAnother strategy is to reduce the traffic on locks by refining: \n‚Ä¢ \nthe lock granularity (how much is locked), \n‚Ä¢ \nthe lock mode (non-exclusive requests). \nTo give an example of finer granularity, IMS was convoying on a lock which controlled the \nOSAM buffer pool. By partitioning the buffer pool into disjoint sub-pools and associating a lock \nwith each sub-pool the execution interval was increased so that buffer pool convoys disappeared. \nIt was replaced by convoys on the ' PI ' and log locks. This demonstrates that eliminating one \nbottleneck simply exposes the next one [Obermark]. \nTo give an example of non-exclusive lock modes, observe processes adding to the System R log \nneed not acquire it in exclusive mode. \nRather, adders can get it in shared mode and only \nprocesses which want to write the log to disk (a relatively rare event) need acquire the log lock \nin exclusive mode. \nUsing non-exclusive modes reduces the probability P that one will wait for a request, and \nrefining the granularity (more different locks) increases the execution interval between requests \n22",
        "semantic_similarity": 0.6005890667438507,
        "keyword_overlap": 0.020192748967416244,
        "combined_score": 0.42647017141092036,
        "paragraph_id": 2,
        "source_title": "The convoy phenomenon"
      }
    ],
    "resolved_title": "The convoy phenomenon"
  },
  {
    "citation_marker": "[20]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.39, overlap: 0.02): **NO**  \n\n**Reason:** The statement from the article discusses a **kernel mutex benchmark** (e.g., cross-directory file renaming, thread groups, throughput, fairness index) comparing locking mechanisms (CFL vs. Syncord). The cited paragraph, however, focuses on **adversarial machine learning** (e.g., coreset selection, batch sizes, robust accuracy, AdvCL) for contrastive learning efficiency. There is **no overlap in subject matter, methodologies, or results**. The cited text does not mention locks, file systems, mutexes, renaming operations, or fairness benchmarks, making the article's citation unsupported and likely erroneous.  \n\n**Key mismatches:**  \n- Article topic: **OS/kernel locking performance**.  \n- Cited text topic: **ML efficiency/robustness via coreset selection**.  \n- Metrics: Throughput/fairness (article) vs. accuracy/runtime (cited).  \n- References: Article cites \"[8], [20]\" for benchmarks; cited text uses tables for ML hyperparameters.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.34, overlap: 0.02): **NO**  \n\n**Reason:** The cited paragraph from [20] discusses adversarial machine learning techniques (e.g., Fast-AT, RCS) applied to image classification (CIFAR-10), focusing on training efficiency, standard accuracy (SA), and adversarial robustness (RA). It does **not** mention:  \n- File system operations (e.g., cross-directory rename benchmarks).  \n- Kernel mutex locks, CFL, Syncord, or fairness indices (Jain's index).  \n- Throughput comparisons for file renaming workloads.  \n\nThe article‚Äôs statement about file renaming benchmarks and lock performance (CFL vs. Syncord) cites [20] inaccurately, as the content of [20] is unrelated to file systems or locking mechanisms. The citation appears to be misplaced or misattributed.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.33, overlap: 0.03): **NO**  \n\n**Reason:**  \nThe statement from the article references a **filesystem operation benchmark** (cross-directory rename, kernel mutex evaluation, lock fairness) citing \"[20]\". However, the provided content from \"[20]\" (the adversarial contrastive learning paper) exclusively discusses **image classification tasks** (CIFAR-10/CIFAR-100 transfer learning, adversarial robustness, coreset selection). There is **no mention** of filesystems, mutex locks, renaming operations, or fairness benchmarks in the cited text.  \n\nThe article misattributes the filesystem experiment to [20], while [20]'s content is entirely unrelated (machine learning transferability). This constitutes **fabrication or severe misrepresentation** of the cited source.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "Table 10: Impact of I for RCS evaluated on the CIFAR-10 task.\nI\nRuning time\n(hours)\nSLF\nSA (%)\nRA (%)\n10\n15.6\n76.31\n38.17\n20\n13.0\n75.96\n37.21\n50\n12.2\n75.87\n35.54\nTable 11: Impact of batch size for coreset selection evaluated on the CIFAR-10 task.\nPre-training\nBatch size for RCS\nRuning time\n(hours)\nSLF\nSA (%)\nRA (%)\nACL-Entire\n-\n42.8\n78.87\n39.19\nACL with Random\n-\n11.8\n72.01\n29.87\nACL with RCS\n64\n13.6\n76.21\n37.48\nACL with RCS\n128\n13.3\n76.15\n37.41\nACL with RCS\n256\n13.1\n75.89\n37.17\nACL with RCS\n512\n13.0\n75.96\n37.21\nTable 12: Compatibility with AdvCL [15] evaluated on the CIFAR-10 task via SLF.\nPre-training\nRuning time\n(hours)\nSLF\nSA (%)\nRA (%)\nAdvCL-Entire\n57.8\n80.89\n42.36\nAdvCL with Random\n11.0\n73.67\n33.61\nAdvCL with RCS\n13.5\n77.93\n38.89\nB.11\nEfficient ACL via RCS with Various Batch Sizes\nIn this subsection, we show the impact of the batch size during coreset selection. We trained ResNet-\n18 via ACL with RCS on CIFAR-10 and then linearly finetuned ResNet-18 models on CIFAR-10.\nThe batch size for RCS is selected from {64, 128, 256, 512} and the batch size for ACL keeps as 512.\nThe subset fraction keeps as 0.2. Other training settings exactly follow Section 4.1. We report the\nstandard and robust test accuracy in Table 11.\nWe can find that as the batch size for RCS decreases, the running time becomes larger. It is because\nthere is a larger number of batches needed to calculate the loss gradients during RCS when the batch\nsize becomes smaller. Besides, we observe that the test accuracy on the downstream tasks seems to\ngradually increase as the batch size decreases. Especially, ACL with RCS using 64 batch size gains\nconsistent improvement compared with ACL with RCS using 512 batch size, which indicates that a\nsmaller batch size for RCS is useful to improve the performance but consumes more running time.\nTherefore, there could be a trade-off between the running time and the transferability. We leave how\nto further improve the efficiency and effectiveness in maintaining the transferability of RCS as the\nfuture work.\nB.12\nRCS for Accelerating Another Variant AdvCL [15]\nFan et al. [15] proposed a variant of ACL method, called ‚ÄúAdvCL‚Äù, that leverages a standardly pre-\ntrained model on ImageNet-1K to generate pseudo-labels for CIFAR-10 training data via K-means\nclustering. Based on ACL [14], the loss function of AdvCL is composed of a weighted sum of the\nadversarial contrastive loss and an ensemble of the CE loss between the adversarial data and its\npseudo label over different choices of cluster number. By simply replacing the loss function of the\nACL LACL(¬∑) with that of AdvCL in Algorithm 1 and 2, we can apply RCS for efficient AdvCL.\nWe pre-trained ResNet-18 via AdvCL with RCS on CIFAR-10 using SGD for 1000 epochs with an\ninitial learning rate 0.5 and a cosine annealing scheduler. We set Œ≤ = 512, TACL = 5, œµACL = 8/255,\nœÅACL = 2/255, and TRCS = 3. The pre-training settings of AdvCL exactly follow Fan et al. [15].\nWe take the same configuration of RCS as that of RCS for ACL in Section 4.1, i.e., warmup epoch\nW = 100, the epoch interval for conducting RCS I = 20, and the subset fraction k = 0.1. Then, we\nevaluate the performance on the CIFAR-10 task via SLF.\nTable 12 shows that RCS can speed up AdvCL [15] as well. Besides, RCS is a principled method\nthat helps AdvCL to obtain effective robust representations since AdvCL with RCS always achieves\na higher test accuracy compared with AdvCL with Random. Therefore, the experimental results\n23",
        "semantic_similarity": 0.38545799255371094,
        "keyword_overlap": 0.02349979018044482,
        "combined_score": 0.27687053184173105,
        "paragraph_id": 22,
        "source_title": "Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection"
      },
      {
        "text_fragment": "Table 13: Comparison between Fast-AT [24] with ACS [33] and Fast-AT with RCS on CIFAR-10.\nTraining method\nRunning time (minutes)\nSA (%)\nRA under PGD-20 (%)\nRA under AutoAttack (%)\nFast-AT on the entire set\n25.0\n86.20\n45.80\n41.04\nFast-AT with ACS\n12.8\n82.71\n45.71\n40.92\nFast-AT with RCS\n12.0\n83.47\n45.89\n41.06\nTable 14: Compatibility with Free-AT [25].\nTraining method\nRunning time (minutes)\nSA (%)\nRA under PGD-20 (%)\nRA under AutoAttack (%)\nFree-AT on the entire set\n116.5\n84.18\n49.05\n45.14\nFast-AT with RCS\n60.6\n82.39\n49.24\n45.15\nthe loss function of SAT is\nLSAT(D; Œ∏) =\nN\nX\ni=1\n\u001a\nmax\nÀúxi‚ààBœµ[xi] ‚Ñì(g ‚ó¶fŒ∏(Àúxi), yi)\n\u001b\n,\n(28)\nwhere ‚Ñìis the Cross-Entropy (CE) loss and Àúxi is adversarial training data generated by PGD within\nthe œµ-ball centered at xi.\nThe loss function of TRADES is\nLTRADES(D; Œ∏) =\nN\nX\ni=1\n\u001a\n‚Ñì(g ‚ó¶fŒ∏(xi), yi) + c ¬∑\nmax\nÀúxi‚ààBœµ[xi] KL(g ‚ó¶fŒ∏(Àúxi), g ‚ó¶fŒ∏(xi))\n\u001b\n,\n(29)\nwhere ‚Ñìis the CE loss, KL(¬∑, ¬∑) is the KL divergence, c > 0 is a trade-off parameter, and Àúxi\nis adversarial training data generated by PGD within the œµ-ball centered at xi. We set c = 6,\nfollowing Zhang et al. [35]. Note that the parameters of g are updated during supervised AT. Here we\nomit the parameters of g since we only use the parameters of the feature extractor fŒ∏ in downstream\ntasks.\nThe RCS problem for supervised AT is formulated as follows:\nS‚àó=\narg max\nS‚äÜD,|S|/|D|=k\n‚àíLRD(U; Œ∏ ‚àíŒ∑‚àáŒ∏Lsupervised(S; Œ∏)),\n(30)\nin which we replace the ACL loss LACL(¬∑) in Eq. (5) with the supervised AT loss Lsupervised(¬∑)\n(e.g., LSAT(¬∑) and LTRADES(¬∑)). Due to that LRD(¬∑) only needs data and does not need any label,\nRCS is applicable to supervised AT, no matter if the validation set is unlabeled or labeled. By\nleveraging greedy search, we show the algorithm of RCS for supervised AT in Algorithm 3 and\nefficient supervised AT via RCS in Algorithm 4.\nB.13.1\nComparison Between RCS and ACS [33] in Speeding Up Supervised AT including\nFast-AT [24], SAT [34] and TRADES [35]\nComparison between Fast-AT with RCS and Fast-AT [24] with ACS [33].\nWe conducted Fast-\nAT, Fast-AT with ACS, and Fast-AT with RCS on CIFAR-10. The experimental setting of Fast-AT\nand Fast-AT with ACS exactly follows that in Section 4.3 of Dolatabadi et al. [33]. That is, we trained\nResNet-18 on CIFAR-10 via Fast-AT (œµ = 8/255, Œ± = 10/255) using SGD with 0.9 momentum for\n60 epochs with the initial learning rate of 0.1 and divided by 10 at Epoch 40 and 50. As for the RCS\nfor speeding up Fast-AT, we set the subset fraction as k = 0.5, warmup epoch as W = 10, and epoch\ninterval for executing RCS I = 10. We report the standard test accuracy and robust test accuracy\nevaluated by PGD-20 (œµ = 8/255, Œ± = 8/2550) and AutoAttack in Table 13. All experiments are\nconducted on one RTX A5000 GPU. Table 13 validate that RCS, without using labels, is more\nefficient and effective in speeding up Fast-AT.\nCompatibility with Free-AT [25].\nFurther, we conducted Free-AT [25] and Free-AT with RCS\non CIFAR-10. We trained ResNet-18 on CIFAR-10 via Free-AT (œµ = 8/255) using SGD with 0.9\nmomentum for 60 epochs with the initial learning rate of 0.01 and divided by 10 at Epoch 40 and\n50. We keep the configurations of RCS for speeding up Free-AT the same as above. We report the\nstandard test accuracy and robust test accuracy evaluated by PGD-20 and AutoAttack in Table 14.\nTherefore, RCS, without using label information, can further speed up both Fast-AT [24] and Free-\nAT [25] while almost maintaining the adversarial robustness. In addition, RCS without using labels\nis more efficient than ACS while achieving similar adversarial robustness compared with ACS.\nTherefore, it validates the effectiveness of RCS in efficient supervised AT.\n25",
        "semantic_similarity": 0.335759699344635,
        "keyword_overlap": 0.020776166209329674,
        "combined_score": 0.24126463940404339,
        "paragraph_id": 24,
        "source_title": "Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection"
      },
      {
        "text_fragment": "Table 19: Cross-task standard transferability [48] of adversarially pre-trained ResNet-50 from\nImageNet-1K to CIFAR-10 and CIFAR-100, respectively. We report the standard test accuracy (%)\nvia standard linear finetuning (SLF) and standard full finetuning (SFF). The number after the dash\nline denotes subset fraction k ‚àà{0.05, 0.1, 0.2}.\nPre-training\nRuning time\n(hours)\nCIFAR-10\nCIFAR-100\nSLF\nSFF\nSLF\nSFF\nStandard training [48] on entire set\n-\n78.84\n97.41\n57.09\n84.21\nSAT [48] on entire set\n286.1\n93.53\n98.09\n77.29\n86.99\nFast-AT [24] on entire set\n10.4\n90.91\n97.54\n73.35\n83.33\nSAT with Random-0.05\n38.7\n85.72\n95.27\n69.29\n82.34\nSAT with RCS-0.05\n48.2\n92.68\n97.65\n75.35\n84.71\nSAT with Random-0.1\n45.8\n87.14\n95.60\n71.23\n83.62\nSAT with RCS-0.1\n55.4\n92.92\n97.82\n75.41\n85.22\nSAT with Random-0.2\n70.3\n87.69\n96.10\n72.05\n84.14\nSAT with RCS-0.2\n79.8\n93.48\n98.06\n76.39\n85.44\nwe finetuned all the parameters of the encoder. We set the initial learning rate of 0.001 for standard\nfull finetuning and 0.01 for standard partial finetuning. The learning rate is divided by 10 at Epoch\n50 and 100. Here, we use the same data augmentation as Salman et al. [48] to resize the images of\nCIFAR-10 and CIFAR-100 to 224 √ó 224. Note that we used the standardly pre-trained ResNet-50\non the entire set and adversarially pre-trained ResNet-50 on the entire set released in the GitHub\nof Salman et al. [48] to reproduce the results of baselines.\nTable 19 shows that RCS substantially accelerates SAT on ImageNet-1K while consistently achieving\nhigher standard test accuracy on downstream tasks than standard pre-training. Besides, SAT with\nRCS always obtains much higher standard test accuracy than SAT with Random. It validates that\nRCS is an effective method for efficient supervised AT.\nIn addition, we also provide a comparison between the standard transferability of Fast-AT [24]\nand SAT with RCS. We downloaded the pre-trained ResNet-50 vis Fast-AT released in the GitHub\nof Wong et al. [24]. Although SAT with RCS consumes more running time than Fast-AT, SAT with\nRCS obtains significantly higher standard test accuracy on downstream tasks, which validates the\neffectiveness of RCS in maintaining the transferability of supervised robust pre-training methods.\nTherefore, our proposed RCS can be a unified and effective framework for speeding up both supervised\nand unsupervised robust pre-training while maintaining the transferability of pre-trained models.\nC\nPossible Negative Societal Impacts\nOur paper aims to speed up robust pre-training by decreasing the number of training data, which\ndecreases the running time of pre-training and should be friendly to the environment. However, to\nimplement all the experiments in this project, we have conducted the pre-training procedure at least\n264 times and the finetuning procedure at least 2376 times, which definitely emitted a bunch of CO2\nand consumed a large amount of electricity. Therefore, we hope that our project can help improve the\nefficiency of robust pre-training and make pre-training more environmental-friendly in the future.\n28",
        "semantic_similarity": 0.33224761486053467,
        "keyword_overlap": 0.029551954242135366,
        "combined_score": 0.24143891667501485,
        "paragraph_id": 27,
        "source_title": "Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection"
      }
    ],
    "resolved_title": "Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection"
  },
  {
    "citation_marker": "[2]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.19, overlap: 0.00): **NO**  \nThe statement \"[2] Irina Calciu, Dave Dice, Yossi Lev, Victor Luchangco, Virendra J.\" **distorts authorship**. The cited paragraph references:  \n- **[3]**: Dice, Marathe, Shavit  \n- **[7]**: Dice, Marathe, Shavit  \n- **[19]**: Luchangco, Nussbaum, Shavit  \n**Irina Calciu and Yossi Lev are not listed as authors** in these references for the NUMA-aware RW locks discussed. While Lev appears in unrelated reference [9], Calciu is absent entirely. The statement erroneously includes them as co-authors of this specific work.  \n\n**Reason**: The citation misattributes authorship by adding names not credited in the cited sources for the NUMA RW locks, fabricating information.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.09, overlap: 0.00): **NO**  \n**Reason:** The statement attributes citation [2] to \"Irina Calciu, Dave Dice, Yossi Lev, Victor Luchangco, Virendra J.\" as authors, but the cited paragraph explicitly assigns [2] to \"Courtois et al.\" when discussing fairness properties (Section 2, paragraph 5: \"as recognized by Courtois et al. [2]\"). The paragraph does not mention the listed authors in connection with [2], indicating a misattribution.  \n\n**Key Discrepancy:**  \n- Cited text: \"Courtois et al. [2]\"  \n- Statement: Assigns [2] to unrelated authors.  \n\nThis constitutes distortion of the source material.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.06, overlap: 0.01): **NO**  \nThe statement distorts the author list from the cited document:  \n1. **Incomplete Name**: The statement lists \"Virendra J.\" (truncated surname), while the source explicitly cites \"Virendra J. Marathe\".  \n2. **Omitted Author**: The statement omits \"Nir Shavit\" (MIT), who is the sixth author in the source.  \n\nThe cited paragraph confirms the full, correct authorship:  \n*Irina Calciu, Dave Dice, Yossi Lev, Victor Luchangco, Virendra J. Marathe, Nir Shavit*.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "Locks\n1T\n2T\n3T\n4T\n6T\n8T\n12T\n16T\n24T\n32T\n48T\n64T\nC-RW-WP\n.510\n1.20\n1.78\n1.95\n3.08\n4.24\n6.99\n9.24\n14.5\n18.0\n26.7\n37.7\nC-RW-NP\n.521\n1.09\n1.64\n2.16\n3.09\n4.26\n6.58\n9.22\n14.3\n17.6\n25.4\n36.4\nC-RW-RP\n.550\n1.12\n1.77\n2.23\n3.31\n4.53\n7.09\n10.4\n13.3\n18.9\n34.5\n52.5\nC-RW-RP-opt\n.531\n1.15\n1.76\n2.19\n3.30\n4.54\n7.45\n10.5\n13.5\n17.9\n27.0\n41.2\nCohort\n.516\n1.25\n1.74\n2.35\n3.41\n4.36\n7.03\n8.55\n13.3\n18.2\n29.6\n44.3\nDR-MCS\n.531\n1.13\n1.58\n1.94\n3.17\n4.19\n6.79\n10.2\n17.6\n26.7\n49.6\n77.1\nDV\n.511\n1.18\n1.75\n2.14\n3.18\n4.12\n6.59\n9.88\n12.8\n21.3\n52.0\n56.8\nROLL\n.547\n1.25\n1.55\n1.99\n3.32\n4.50\n7.79\n11.3\n20.2\n29.0\n46.4\n63.3\nShirako\n.554\n1.16\n1.61\n2.07\n3.33\n4.53\n7.30\n10.7\n18.5\n27.0\n35.5\n55.2\nFigure 8. Scalability results of the Kyoto Cabinet kccachetest benchmark (with the command line arguments: wicked -th Thrds -capsiz\n2000000 100000). Each entry in the table reports wall clock time to completion in seconds.\nical sections, including short read-only and read-write ones, and\nlong and complex read-write ones. Overall, the workload is domi-\nnated by read-write critical sections, where the threads acquire the\nRW locks in write mode. As a result, Cohort performs compara-\nbly to our NUMA RW locks, and much better than all other locks\nthat contain NUMA-oblivious writers ‚Äì DR-MCS, DV, ROLL, and\nShirako. DR-MCS scales poorly because the underlying MCS lock\nacquired by writers forces the cache lines of the lock and the data it\nprotects to bounce between NUMA nodes more often than other\nlocks. Since Cohort signiÔ¨Åcantly curtails lock migration, it per-\nforms much better. Our NUMA-aware RW locks, except C-RW-RP,\nfurther extend the cohorting advantage because of NUMA-friendly\nreader-reader concurrency. C-RW-RP succumbs to the superÔ¨Çuous\nwriter ownership circulation performance problem described ear-\nlier, and, as a result, does not scale as well as our other locks. It\ndoes however scale better than all prior locks. Overall, C-RW-WP\nand C-RW-NP, which perform the best, outperform the best of the\nprior locks (DV and Shirako) by about 40%.\n5.\nConclusion\nThe rapid growth of multi-core multi-chip systems is making\nNUMA architectures commonplace, and fundamental data struc-\ntures and synchronization primitives must be redesigned to adapt\nto these environments. We introduced a new family of surprisingly\nsimple NUMA-aware reader-writer locks that outperform prior\nlock algorithms by a large margin. Writers use centralized lock\nmetadata and readers use decentralized metadata. Microbenchmark\nexperiments suggest that our best lock exceeds the performance of\nthe prior state-of-the-art by up to a factor of 10, and our exper-\niments on a real-world application, the Kyoto Cabinet database,\nshow our locks can boost the application‚Äôs performance by up to\n40%.\nAcknowledgments\nWe thank Doug Lea for useful discussions. Nir Shavit was sup-\nported in part by NSF grant 1217921.\nReferences\n[1] B. B. Brandenburg and J. H. Anderson. Spin-based Reader-Writer\nSynchronization for Multiprocessor Real-time Systems.\nReal-Time\nSyst., 46(1):25‚Äì87, 2010.\n[2] P. J. Courtois, F. Heymans, and D. L. Parnas. Concurrent control with\n‚Äùreaders‚Äù and ‚Äùwriters‚Äù. Communications of the ACM, 14(10):667‚Äì\n668, 1971.\n[3] D. Dice, V. J. Marathe, and N. Shavit.\nFlat Combining NUMA\nLocks. In Proceedings of the 23rd ACM Symposium on Parallelism\nin Algorithms and Architectures, 2011.\n[4] D. Dice. Solaris Scheduling: SPARC and CPUIDs. URL https://\nblogs.oracle.com/dave/entry/solaris_scheduling_and_\ncpuids.\n[5] D. Dice. A Partitioned Ticket Lock. In Proceedings of the 23rd ACM\nAymposium on Parallelism in Algorithms and Architectures, pages\n309‚Äì310, 2011.\n[6] D. Dice and N. Shavit. TLRW: Return of the Read-Write Lock. In Pro-\nceedings of the 22nd ACM Symposium on Parallelism in Algorithms\nand Architectures, pages 284‚Äì293, 2010.\n[7] D. Dice, V. J. Marathe, and N. Shavit. Lock Cohorting: A General\nTechnique for Designing NUMA Locks. In Proceedings of the 17th\nACM SIGPLAN symposium on Principles and Practice of Parallel\nProgramming, pages 247‚Äì256, 2012.\n[8] E. W. Dijkstra.\nThe origin of concurrent programming.\nchapter\nCooperating sequential processes, pages 65‚Äì138. 2002.\n[9] F. Ellen, Y. Lev, V. Luchangco, and M. Moir. SNZI: Scalable NonZero\nIndicators. In Proceedings of the 26th Annual ACM Symposium on\nPrinciples of Distributed Computing, pages 13‚Äì22, 2007.\n[10] E. Freudenthal and A. Gottlieb.\nProcess coordination with fetch-\nand-increment. In Proceedings of the 4th International Conference\non Architectural Support for Programming Languages and Operating\nSystems, pages 260‚Äì268, 1991.\n[11] W. C. Hsieh and W. E. Weihl.\nScalable Reader-Writer Locks for\nParallel Systems. In Proceedings of the Sixth International Parallel\nProcessing Symposium, 1991.\n[12] J. M. Mellor-Crummey and M. L. Scott. Algorithms for Scalable Syn-\nchronization on Shared-Memory Multiprocessors. ACM Transactions\non Computer Systems, 9(1):21‚Äì65, 1991.\n[13] J. M. Mellor-Crummey and M. L. Scott.\nSynchronization without\nContention. In Proceedings of the 4th International Conference on\nArchitectural Support for Programming Languages and Operating\nSystems, pages 269‚Äì278, 1991.\n[14] O. Krieger, M. Stumm, R. Unrau, and J. Hanna. A Fair Fast Scal-\nable Reader-Writer Lock. In Proceedings of the 1993 International\nConference on Parallel Processing, pages 201‚Äì204, 1993.\n[15] Y. Lev, V. Luchangco, and M. Olszewski.\nScalable Reader-Writer\nLocks. In Proceedings of the 21st Annual Symposium on Parallelism\nin Algorithms and Architectures, pages 101‚Äì110, 2009.\n[16] J. M. Mellor-Crummey and M. L. Scott.\nScalable Reader-Writer\nSynchronization for Shared-Memory Multiprocessors. In Proceedings\nof the 3rd ACM SIGPLAN Symposium on Principles and Practice of\nParallel Programming, pages 106‚Äì113, 1991.\n[17] Z. Radovi¬¥c and E. Hagersten. Hierarchical Backoff Locks for Nonuni-\nform Communication Architectures.\nIn HPCA-9, pages 241‚Äì252,\nAnaheim, California, USA, Feb. 2003.\n[18] J. Shirako, N. Vrvilo, E. G. Mercer, and V. Sarkar. Design, veriÔ¨Åcation\nand applications of a new read-write lock algorithm. In Proceedinbgs\nof the 24th ACM symposium on Parallelism in algorithms and archi-\ntectures, pages 48‚Äì57, 2012.\n[19] Victor Luchangco and Dan Nussbaum and Nir Shavit. A Hierarchical\nCLH Queue Lock. In Proceedings of the 12th International Euro-Par\nConference, pages 801‚Äì810, 2006.\n[20] D.\nVyukov.\nDistributed\nReader-Writer\nMu-\ntex.\nURL\nhttp://www.1024cores.net/home/\nlock-free-algorithms/reader-writer-problem/\ndistributed-reader-writer-mutex.",
        "semantic_similarity": 0.185624361038208,
        "keyword_overlap": 0.0022688598979013048,
        "combined_score": 0.130617710696116,
        "paragraph_id": 9,
        "source_title": "NUMA-Aware Reader-Writer Locks"
      },
      {
        "text_fragment": "Hsieh and Weihl [11] and Vyukov [20] independently suggested\na simple distributed2 approach to building scalable RW locks. Each\ndistributed RW lock contains N RW locks where N is the number\nof processors in the system. Each reader is mapped to a single\nRW lock, and must acquire that lock in read mode in order to\nexecute its critical section. A writer must acquire all the underlying\nRW locks in write mode to execute its critical section. Deadlocks\nbetween writers are avoided by forcing a speciÔ¨Åc locking order.\nThe approach can be made NUMA-aware by restricting N to the\nnumber of NUMA nodes in the system, and mapping each reader to\nthe lock dedicated to its node. This variant algorithm which we call\nDV (representing the initials of Vyukov), is partially NUMA-aware,\njust like the SNZI-based RW locks. Absent any writers, readers\non different nodes can obtain and release read permission without\ngenerating any inter-node write coherence trafÔ¨Åc. However, every\nwriter incurs the overhead of acquiring write permission for the RW\nlock of every node, potentially generating signiÔ¨Åcant coherence\ntrafÔ¨Åc. Thus, the performance of DV plummets with increased\nwriter activity. Also, because of the canonical locking order used\nto avoid deadlock, readers on nodes that appear late in the order\nmay enjoy an unfair performance advantage over readers running\non nodes that appear earlier.\nIn this paper we present a novel family of RW locks that are de-\nsigned to leverage NUMA features and deliver better performance\nand scalability than any prior RW lock algorithm. We take a three-\npronged approach in our lock designs. First, similar to DV, we\nmaintain a distributed structure for the readers metadata such that\nreaders denote their intent by updating only locations associated\nwith their node. By localizing updates to read indicators we reduce\ncoherence trafÔ¨Åc on the interconnect. Second, writers preferentially\nhand off access permission to blocked writers on the same node,\nenhancing reference locality in the node‚Äôs cache for both the lock\nmetadata and data accessed in the critical section it protects. Fi-\nnally, our algorithms maintain tight execution paths for both read-\ners and writers, reducing latency of the lock acquisition and release\noperations.\nOur RW lock algorithms build on the recently developed lock\ncohorting technique [7], which allows for the construction of\nNUMA-aware mutual exclusion locks. BrieÔ¨Çy, writers use a co-\nhort lock to synchronize with each other and to maintain writer-\nvs-writer exclusion. Using the cohort locking approach, a writer\nreleasing the lock generally prefers to transfer ownership to a pend-\ning local writer (if there is one), thus reducing lock migrations3\nbetween nodes.\nOur RW locks also contain distributed implementations of read\nindicators, a data structure that tracks the existence of readers [15].\nReaders ‚Äúarrive‚Äù at these read indicators during lock acquisition\nand ‚Äúdepart‚Äù from them during lock release. Writers query the\nread indicators to detect concurrently active readers. Because of\nthe distributed nature of our read indicators, the readers need to\naccess just the node-speciÔ¨Åc metadata of the lock. We additionally\nuse simple Ô¨Çags and checks for coordination between readers and\nwriters. The result is a family of surprisingly simple algorithms that\npush the performance envelope of RW locks on NUMA systems far\nbeyond the prior state-of-the-art algorithms.\nOur various RW locks can be differentiated on the basis of\nthe fairness properties they provide as recognized by Courtois et\n2 The term ‚Äúdistributed‚Äù was coined by Vyukov for his algorithm [20],\nbut this algorithm appears to be the same as Hsieh and Weihl‚Äôs ‚Äústatic\nalgorithm‚Äù [11]\n3 We say that lock migration occurs when the lock is consecutively acquired\nby threads residing on distinct NUMA nodes. On a cache-coherent NUMA\nsystem, lock migration leads to the transfer of cache lines‚Äìboth for lines\nunderlying the lock metadata as well as for lines underlying mutable data\naccessed in the critical section protected by the lock‚Äîfrom the cache\nassociated with the Ô¨Årst thread to that of the second thread.\nFigure 1. An example multi-core multi-chip NUMA system con-\ntaining 2 chips with 4 cores per chip. Each chip is a NUMA node.\nEach core can have multiple hardware thread contexts (not shown\nin the Ô¨Ågure). Each core has its individual L1 cache, and all cores\non a chip share an L2 cache. Inter-thread communication via local\ncaches (L1 and L2) is signiÔ¨Åcantly faster than via remote caches\nbecause the latter involve coherence messages across the intercon-\nnect. In the Ô¨Ågure, threads r1..r6 intend to acquire a RW lock in\nread mode, and threads w1..w6 intend to acquire the same lock in\nwrite mode.\nal. [2]. In particular, we present locks exhibiting different ‚Äúprefer-\nence‚Äù policies: reader-preference, writer-preference, and neutral-\npreference. The reader-preference policy dictates that readers\nshould acquire (be granted) the lock as early as possible, regard-\nless of arrival order, whereas the writer-preference policy has a\nsymmetric bias towards writers. More concretely, these preference\npolicies allow readers or writers to ‚Äúbypass‚Äù prior pending writ-\ners or readers (respectively) in the race to acquire the lock. The\npreference policies‚Äîexcept for the neutral policy‚Äîmay lead to\nstarvation of threads engaged in the non-preferred lock acquisition\noperation. We avoid such situations by allowing the lock mecha-\nnism to temporarily override the preference policy so as to allow\nforward progress of starving threads. Starving threads become ‚Äúim-\npatient‚Äù and transiently change the preference policy.\nWe present an empirical evaluation of our RW locks, compar-\ning them with each other and with prior RW lock implementations.\nOur evaluation, conducted on a 256-way 4-node Oracle SPARC\nT5440T M server, shows that our locks signiÔ¨Åcantly outperform all\nprior RW locks on a diverse set of workloads. In our microbench-\nmark experiments, our locks outperform the prior best RW lock\n(the SNZI-based ROLL lock [15]) by up to a factor of 10.\nWe discuss our RW lock design approach in Section 2. In Sec-\ntion 3, we present our lock algorithms in detail. We present our\nempirical evaluation in Section 4, and conclude in Section 5.\n2.\nLock Design Rationale\nNUMA-aware mutex locks have been explored in depth [3, 7, 17,\n19]. However, to the best of our knowledge, there has been no prior\neffort toward constructing NUMA-aware RW locks. NUMA-aware\nmutex lock designs pursue only one goal ‚Äì reduction of the lock",
        "semantic_similarity": 0.08977174758911133,
        "keyword_overlap": 0.00030674846625766873,
        "combined_score": 0.06293224785225524,
        "paragraph_id": 1,
        "source_title": "NUMA-Aware Reader-Writer Locks"
      },
      {
        "text_fragment": "NUMA-Aware Reader-Writer Locks\nIrina Calciu\nBrown University\nirina@cs.brown.edu\nDave Dice\nOracle Labs\ndave.dice@oracle.com\nYossi Lev\nOracle Labs\nyossi.lev@oracle.com\nVictor Luchangco\nOracle Labs\nvictor.luchangco@oracle.com\nVirendra J. Marathe\nOracle Labs\nvirendra.marathe@oracle.com\nNir Shavit\nMIT\nshanir@csail.mit.edu\nAbstract\nNon-Uniform Memory Access (NUMA) architectures are gain-\ning importance in mainstream computing systems due to the rapid\ngrowth of multi-core multi-chip machines. Extracting the best pos-\nsible performance from these new machines will require us to re-\nvisit the design of the concurrent algorithms and synchronization\nprimitives which form the building blocks of many of today‚Äôs appli-\ncations. This paper revisits one such critical synchronization prim-\nitive ‚Äì the reader-writer lock.\nWe present what is, to the best of our knowledge, the Ô¨Årst fam-\nily of reader-writer lock algorithms tailored to NUMA architec-\ntures. We present several variations which trade fairness between\nreaders and writers for higher concurrency among readers and bet-\nter back-to-back batching of writers from the same NUMA node.\nOur algorithms leverage the lock cohorting technique to manage\nsynchronization between writers in a NUMA-friendly fashion, bi-\nnary Ô¨Çags to coordinate readers and writers, and simple distributed\nreader counter implementations to enable NUMA-friendly concur-\nrency among readers. The end result is a collection of surprisingly\nsimple NUMA-aware algorithms that outperform the state-of-the-\nart reader-writer locks by up to a factor of 10 in our microbench-\nmark experiments. To evaluate our algorithms in a realistic setting\nwe also present performance results of the kccachetest benchmark\nof the Kyoto-Cabinet distribution, an open-source database which\nmakes heavy use of pthread reader-writer locks. Our locks boost\nthe performance of kccachetest by up to 40% over the best prior\nalternatives.\nCategories and Subject Descriptors\nD.1.3 [Programming Tech-\nniques]: Concurrent Programming\nGeneral Terms\nAlgorithms, Design, Performance\nKeywords\nNUMA, hierarchical locks, mutual exclusion, reader-\nwriter locks\n1.\nIntroduction\nAs microprocessor vendors aggressively pursue the production of\nbigger multi-core multi-chip systems (Intel‚Äôs Nehalem-based and\nOracle‚Äôs Niagara-based systems are typical examples), the com-\nputing industry is witnessing a shift toward distributed and cache-\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation\non the Ô¨Årst page. To copy otherwise, to republish, to post on servers or to redistribute\nto lists, requires prior speciÔ¨Åc permission and/or a fee.\nPPoPP‚Äô13,\nFebruary 23‚Äì27,2013,Shenzhen, China.\nCopyright c‚Éù2013 ACM 978-1-4503-1922/13/02. . . $15.00\ncoherent Non-Uniform Memory Access (NUMA) architectures. 1\nThese systems contain multiple nodes where each node has locally\nattached memory, a local cache and multiple processing cores. Such\nsystems present a uniform programming model where all memory\nis globally visible and cache-coherent. The set of cache-coherent\ncommunications channels between nodes is referred to collectively\nas the interconnect. These inter-node links normally suffer from\nhigher latency and lower bandwidth compared to the intra-node\nchannels. To decrease latency and to conserve interconnect band-\nwidth, NUMA-aware policies encourage intra-node communica-\ntion over inter-node communication.\nCreating efÔ¨Åcient software for NUMA systems is challenging\nbecause such systems present a naive uniform ‚ÄúÔ¨Çat‚Äù model of the\nrelationship between processors and memory, hiding the actual un-\nderlying topology from the programmer. The programmer must\nstudy architecture manuals and use special system-dependent li-\nbrary functions to exploit the system topology. NUMA-oblivious\nmultithreaded programs may suffer performance problems arising\nfrom long access latencies caused by inter-node coherence trafÔ¨Åc\nand from interconnect bandwidth limits. Furthermore, inter-node\ninterconnect bandwidth is a shared resource so coherence trafÔ¨Åc\ngenerated by one thread can impede the performance of other un-\nrelated threads because of queueing delays and channel contention.\nConcurrent data structures and synchronization constructs at the\ncore of modern multithreaded applications must be carefully de-\nsigned to adapt to the underlying NUMA architectures. One key\nsynchronization construct is the reader-writer (RW) lock.\nA RW lock relaxes the central property of traditional mutual ex-\nclusion (mutex) locks by allowing multiple threads to hold the lock\nsimultaneously in read mode. A thread may also acquire the lock in\nwrite mode for exclusive access. RW locks are used in a wide range\nof settings including operating system kernels, databases, high-end\nscientiÔ¨Åc computing applications and software transactional mem-\nory implementations [6].\nRW locks have been studied extensively for several decades [1,\n2, 11, 13‚Äì16], with proposals ranging from simple counter- or\nsemaphore-based solutions [2], to solutions leveraging centralized\nwait-queues [14, 16], to solutions that use more sophisticated data\nstructures such as Scalable Non-Zero Indicators (SNZI) [15]. Of\nthese, all but the SNZI-based solutions rely on centralized struc-\ntures to coordinate threads, and thus encounter scalability im-\npediments [15]. The SNZI-based algorithms keep track of read-\ners ‚Äì threads acquiring the RW lock in read mode ‚Äì with each\nreader arriving at a leaf in the ‚ÄúSNZI tree‚Äù. Readers can be made\nNUMA-aware by partitioning the leaves of the SNZI-tree among\nthe NUMA nodes, with threads arriving at SNZI leaves associated\nwith their node. Writers, however, remain NUMA-oblivious, which\ncan impair scalability.\n1 We use the term NUMA broadly to include Non-Uniform Communication\nArchitecture (NUCA) [17] machines as well.",
        "semantic_similarity": 0.05920892953872681,
        "keyword_overlap": 0.005724508050089445,
        "combined_score": 0.04316360309213559,
        "paragraph_id": 0,
        "source_title": "NUMA-Aware Reader-Writer Locks"
      }
    ],
    "resolved_title": "NUMA-Aware Reader-Writer Locks"
  },
  {
    "citation_marker": "[3]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: Based on the provided texts:\n\n**Answer:** YES  \n\n**Reason:**  \nThe article's statement accurately reflects the cited paragraph's description of CLoF:  \n1. The statement describes CLoF as \"a framework for building multi-level locks with level-heterogeneity\" ‚Äì directly matching ¬ß4's title (\"The Compositional Lock Framework\") and its assertion that CLoF generates \"multi-level heterogeneous NUMA-aware locks\" using syntactic recursion for heterogeneity.  \n2. The claim that locks are \"correct by construction\" is explicitly stated in the paragraph (¬ß4.2: \"CLoF locks are correct by construction\").  \n3. The \"tailored to the target platform\" claim aligns with CLoF's workflow (¬ß4), which requires platform-specific hierarchy configurations and benchmarks to generate optimized locks.  \n\nNo misrepresentation, distortion, or fabrication is present. The article concisely summarizes the core contributions of CLoF as presented in the source."
    ],
    "evidence": [
      {
        "text_fragment": "14 8\n16 24 32\n48\n64\n95\n127\nNumber of threads\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nMedian throughput (iter./¬µs)\nCLoF‚ü®4‚ü©-Arm\nHMCS‚ü®4‚ü©\nMCS\nCNA\nShÔ¨ÇLock\nFigure 4. LevelDB with increasing contention, comparing\ndifferent state-of-art locks and CLoF on Armv8.\n3.4\nIn Search for a Composable Approach\nGiven that modern NUMA architectures have deep hierar-\nchies (Section 3.1) and the best lock differs among archi-\ntectures and levels (Section 3.2), we propose CLoF in Sec-\ntion 4, a framework for building multi-level locks with level-\nheterogeneity. CLoF generates NUMA-aware locks that are\ncorrect by construction and tailored to the target platform\n(and underlying architecture).\nWe now demonstrate the potential of CLoF. In Figure 4,\nwe compare the throughput of LevelDB with the best CLoF-\nlock on Armv8, several state-of-the-art NUMA-aware locks,\nand MCS lock. For fewer than 32 threads, CNA lock and\nShflLock suffer from a shuffling overhead, degrading their\nperformance with respect to MCS lock. Once the NUMA\nlevel is crossed (> 32 threads), CNA lock and ShflLock match\nand later (> 64 threads) improve over MCS lock because they\nsupport NUMA-node level. Note that CLoF and HMCS do\nnot introduce that overhead. The support of the full hier-\narchy allows HMCS‚ü®4‚ü©to greatly outperform these locks.\nIntroducing heterogeneity gives CLoF additional 10% to 15%\nhigher performance from 8 to 128 threads. Similarly, the\nheterogeneity aspect is also beneficial for x86. In Figure 2,\nCLoF‚ü®4‚ü©outperforms HMCS‚ü®4‚ü©for most contention levels,\ne.g., by 5% with 8 threads and 33% with 96 threads.\nAs we are going to see in Section 5, the best CLoF-lock on\nx86 is not composed of the same NUMA-oblivious locks as\nthe best CLoF-lock on Armv8.\n4\nThe Compositional Lock Framework\nFigure 5 describes the CLoF user‚Äôs perspective. First, CLoF\nrequires a hierarchy configuration, i.e., a file describing the\nmemory hierarchy levels of target platform. As in Section 3.1,\nCLoF produces a heatmap of the target platform, from which\nthe user can identify these levels by grouping tiles colored\nwith similar intensity. The resulting configuration can be\ntuned to select only those levels most relevant to the user (see\nexample in Section 5.2). Note that, although not currently\nimplemented, identifying levels in a heatmap can be easily\nautomated. Next, the user selects a set of NUMA-oblivious\nNUMA-oblivious\nspinlocks ¬ß2:\n‚Äì Ticketlock\n‚Äì CLH lock\n‚Äì MCS lock\n‚Äì Hemlock\ntarget platform\ne.g., 128-core\nArmv8 server\nBest CLoF-lock(s)\nfor target platform,\ne.g., tkt-mcs-tkt\nVerify correctness,\ne.g., with GenMC\nand VSync [32]\nExperimentally\nfind memory\nhierarchy (see ¬ß3.1)\nTuning point:\nchoose selection\npolicy (e.g., ¬ß5)\nApply CLoF Lock\nGenerator\n(see ¬ß4.1)\nTuning point:\nchoose hierarchy\nlevels (e.g., ¬ß5)\nRun scripted bench-\nmark (see ¬ß4.3)\n0 4\n32\n64\n127\ncore 1\n127\n64\n32\n40\ncore 2\n148 162432\n48\n64\n95\n127\nNumber of threads\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nMedian throughput (iter./¬µs)\nOnly locks correct on\nthe target architecture\nhierarchy\nconfiguration\n100s of multi-level\nheterogeneous locks\nFigure 5. Workflow of CLoF: input is a target platform\nand a set of NUMA-oblivious locks; output is a correct best\nperforming multi-level heterogeneous NUMA-aware lock.\nGreen boxes are the contributions of this work; blue boxes\nare optional tuning points for the user; and gray boxes are\noutside the scope of this work.\nspinlocks, e.g., those described in Section 2. We call these\nthe basic locks. We verify and optimize these locks using the\nVSync framework [32].\nThe core of CLoF is the lock generator described in Sec-\ntion 4.1. Based on the set of correct basic locks and the hier-\narchy configuration, the CLoF lock generator outputs hun-\ndreds of multi-level heterogeneous NUMA-aware locks. In\nSection 4.2, we use an induction argument with model check-\ning to show that CLoF locks are correct by construction.\nFinally, the scripted benchmark described in Section 4.3\nselects the best CLoF lock for the target platform. The user\ncan change the default selection policy to prioritize low con-\ntention performance over high contention (we evaluate both\npolicies in Section 5).\nThe CLoF workflow (Figure 5) is fully automated with the\nexception of the tuning points: (1) creating the hierarchy\nconfiguration, and (2) choosing the selection policy.\n4.1\nThe Lock Generator\nWe now present our lock generator in two steps: first, we\naddress the issue of supporting multiple levels of heteroge-\nneous locks; second, we show how to abstract the spinlocks\nto isolate the framework from the lock implementations.\n4.1.1\nSyntactic Recursion. CLoF employs syntactic re-\ncursion to support different locks on each level of the hierar-\nchy. Since syntactic recursion unfolds at compile-time (e.g.,\nvia C++ templates or C macros), it does not have the over-\nhead of virtual function pointers. We describe the recursion\nin the lock generator with a simple domain specific language\nshown in Figure 6.\nLock sets. Let ùëôbe a lock in BasicLocks, i.e., the set of all\nNUMA-oblivious locks. Let ùêøbe a lock in ClofLocks, the set\nof all CLoF-generated locks. By construction, the composed\n856",
        "semantic_similarity": 0.6760393977165222,
        "keyword_overlap": 0.010812696198116497,
        "combined_score": 0.47647138726100047,
        "paragraph_id": 5,
        "source_title": "CLoF: A Compositional Lock Framework for Multi-level NUMA Systems"
      }
    ],
    "resolved_title": "CLoF: A Compositional Lock Framework for Multi-level NUMA Systems"
  },
  {
    "citation_marker": "[4]",
    "is_valid": false,
    "issues": [
      "‚ùå Citation marker '[4]' could not be resolved to a known paper title in the knowledge base."
    ],
    "evidence": [],
    "resolved_title": null
  },
  {
    "citation_marker": "[5]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: **Answer:** YES  \n**Reason:** The article's statement accurately reflects the cited document. Both correctly identify:  \n- Authors: Dave Dice, Virendra J. Marathe, Nir Shavit  \n- Venue: ACM Symposium on Parallelism in Algorithms and Architectures (SPAA)  \n- Core contribution: A scalable hierarchical NUMA lock algorithm based on flat combining, designed to reduce interconnect traffic and improve performance on NUMA architectures.  \n\nThe cited paragraph explicitly confirms these elements, including the algorithm's novelty, efficiency goals, and empirical superiority over existing locks (e.g., HCLH). No distortion or fabrication is present."
    ],
    "evidence": [
      {
        "text_fragment": "Flat-Combining NUMA Locks\nDave Dice\nOracle Labs\ndave.dice@oracle.com\nVirendra J. Marathe\nOracle Labs\nvirendra.marathe@oracle.com\nNir Shavit\nOracle Labs\nnir.shavit@oracle.com\nABSTRACT\nMulticore machines are growing in size, and accordingly\nshifting from simple bus-based designs to NUMA and CC-\nNUMA architectures. With this shift, the need for scalable\nhierarchical locking algorithms is becoming crucial to per-\nformance.\nThis paper presents a novel scalable hierarchi-\ncal queue-lock algorithm based on the Ô¨Çat combining syn-\nchronization paradigm. At the core of the new algorithm is\na scheme for building local queues of waiting threads in a\nhighly eÔ¨Écient manner, and then merging them globally, all\nwith little interconnect traÔ¨Éc and virtually no costly syn-\nchronization operations in the common case. In empirical\ntesting on an Oracle SPARC Enterprise T5440 Server, a\n256-way CC-NUMA machine, our new Ô¨Çat-combining hier-\narchical lock signiÔ¨Åcantly outperforms all classic locking al-\ngorithms, and at high concurrency levels, provides up to a\nfactor of two improvement over HCLH, the most eÔ¨Écient\nknown hierarchical locking algorithm.\nCategories and Subject Descriptors\nD.1.3 [Programming Techniques]: Concurrent Program-\nming\nGeneral Terms\nAlgorithms, Design, Experimentation, Performance\nKeywords\nhierarchical locks, queue locks, Ô¨Çat combining\n1.\nINTRODUCTION\nQueue locks [1, 2, 3, 4], and in particular the CLH [2,\n3, 4] and MCS [3] locks, have long been the algorithms of\nchoice for locking in many high performance systems. They\nare known to reduce the overall cache coherence traÔ¨Éc by\nforming queues of threads, each spinning on a separate mem-\nory location as they await their turn to access the critical\nsection.\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proÔ¨Åt or commercial advantage and that copies\nbear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc\npermission and/or a fee.\nSPAA‚Äô11, June 4‚Äì6, 2011, San Jose, California, USA.\nCopyright 2011 ACM 978-1-4503-0743-7/11/06 ...$10.00.\nCurrent trends in multicore architecture design imply that\nin coming years, there will be an accelerated shift towards\ndistributed nonuniform memory-access (NUMA) and cache-\ncoherent NUMA (CC-NUMA) architectures.\nSuch archi-\ntectures, examples of which include Intel‚Äôs 4 chip/32 way\nNehalem-based systems and Oracle‚Äôs 4 chip/256 way Niagara-\nbased systems, consist of collections of computing cores with\nfast local memory (as found on a single multicore chip), com-\nmunicating with each other via a slower (inter-chip) commu-\nnication medium. Access by a core to the local memory, and\nin particular to a shared local cache, can be several times\nfaster than access to the remote memory located on another\nchip [5].\nRadovi¬¥c and Hagersten [5] were the Ô¨Årst to show the ben-\neÔ¨Åts of designing locks that improve locality of reference on\nCC-NUMA architectures by developing hierarchical locks:\ngeneral-purpose mutual-exclusion locks that encourage thr-\neads with high mutual memory locality to acquire the lock\nconsecutively, thus reducing the overall level of cache misses\nwhen executing instructions in the critical section.\nRadovi¬¥c and Hagersten introduced the hierarchical back-\noÔ¨Älock (HBO): a test-and-test-and-set lock augmented with\na new backoÔ¨Äscheme to reduce contention on the lock vari-\nable. Their hierarchical backoÔ¨Ämechanism allows the back-\noÔ¨Ädelay to be tuned dynamically, so that when a thread\nnotices that another thread from its own local cluster owns\nthe lock, it can reduce its delay and increase its chances\nof acquiring the lock consecutively. However, because the\nlocks are test-and-test-and-set locks, they incur invalidation\ntraÔ¨Éc on every modiÔ¨Åcation of the shared global lock vari-\nable, which is especially costly on NUMA machines. In their\nwork [5], Radovi¬¥c and Hagersten did introduce a heuristic\ntechnique to throttle inter-chip coherence traÔ¨Éc. However,\nas we show in our evalution (Section 3), it does not nec-\nessarily translate to better scalability.\nMoreover, the dy-\nnamic adjustment of backoÔ¨Ädelay time in the lock intro-\nduces signiÔ¨Åcant starvation and fairness issues: it becomes\nlikely that two or more threads from the same cluster will\nrepeatedly acquire a lock while threads from other clusters\nstarve.\nRadovi¬¥c and Hagersten also introduced a heuris-\ntic to improve fairness, but this requires Ô¨Åne tuning of the\nbackoÔ¨Äparameters, which can change with the underlying\napplication‚Äôs characteristics.\nLuchangco et al. [6] overcome these drawbacks by intro-\nducing a hierarchical version of the CLH queue-locking al-\ngorithm (HCLH). Their HCLH algorithm collects requests\non each chip into a local CLH style queue, and then has\nthe thread at the head of the queue integrate each chip‚Äôs\n65",
        "semantic_similarity": 0.7032304108142853,
        "keyword_overlap": 0.020352250489236792,
        "combined_score": 0.49836696271677067,
        "paragraph_id": 0,
        "source_title": "Flat-combining NUMA locks"
      }
    ],
    "resolved_title": "Flat-combining NUMA locks"
  },
  {
    "citation_marker": "[6]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.73, overlap: 0.04): **NO**  \nThe statement from the article is **not supported** by the cited paragraph.  \n\n**Reason:**  \nThe cited paragraph discusses **transactional memory** (a hardware-based approach for atomic operations) and its advantages over locks (e.g., avoiding priority inversion, deadlock). It does **not mention CFL, Syncord, or throughput comparisons** (e.g., \"1.32√ó to 1.45√ó\"). Instead, it generically claims transactional memory outperforms locking techniques due to fewer memory accesses. The article‚Äôs specific performance metrics and system names (CFL/Syncord) are absent from the source, making the statement unsupported by this citation.  \n\n---\n\n**Key Discrepancies:**  \n1. **Subject Mismatch:**  \n   - Article focuses on lock schedulers (CFL/Syncord).  \n   - Cited source discusses transactional memory hardware.  \n2. **Absence of Specifics:**  \n   - No mention of CFL, Syncord, throughput ratios, or fairness in lock occupation in the cited text.  \n3. **Contextual Difference:**  \n   - The source addresses lock-free programming via hardware transactions, not software lock fairness or throughput benchmarks between named systems.  \n\nThe article‚Äôs claim appears to misattribute unrelated content to this citation.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.70, overlap: 0.04): **NO**  \nThe statement from the article claims that \"CFL outperforms Syncord in all cases by from 1.32√ó to 1.45√ó in terms of throughput, while both achieve fairness in lock occupation.\" However, the cited paragraph from Herlihy and Moss (1993) discusses:  \n1. **Transactional memory** as an architectural solution to avoid priority inversion, convoying, and deadlock in lock-free data structures.  \n2. Its performance being competitive with locking techniques *in the absence* of these issues.  \n\n**Key discrepancies**:  \n- The cited paragraph **never mentions CFL, Syncord, throughput metrics (1.32√ó‚Äì1.45√ó), or fairness in lock occupation**.  \n- It focuses on transactional memory vs. generic locking, not specific systems like CFL/Syncord.  \n- The article's performance claims are unsupported by this source, as the cited text lacks any reference to these systems or metrics.  \n\nThus, the article distorts the source by attributing unrelated performance data to it.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.69, overlap: 0.05): **NO**  \nThe cited paragraph from reference [6] (Herlihy and Moss, 1993) is a list of references (entries [1]‚Äì[11]) from the paper. It contains **no discussion** of:  \n- CFL or Syncord systems,  \n- throughput comparisons (1.32√ó‚Äì1.45√ó),  \n- fairness in lock occupation,  \n- or any experimental results related to the article's claims.  \n\nThe cited content exclusively lists technical reports and prior publications on topics like multiprocessor architectures, synchronization methods, and memory systems, unrelated to the article's statement. The article's performance claims about CFL/Syncord are **not supported** by this excerpt.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "Figure 5: Producer/Consumer Benchmark: Bus and Network\natomic operations on multiple locations. For example, the\nMotorola 68000 provides a COMPARE&SWAP2 that operates\non two independent locations. Massalin and Pu [25] use\nthis instruction for lock-free list manipulation in an oper-\nating system kernel. Transactional memory provides more\npowerfulsupportfor this‚Äúlock-free‚Äù style of programming.\nOther work that uses after-the-fact conÔ¨Çict detection to\nrecognize violations of desired correctness conditions in-\nclude Gharachorloo and Gibbons [11], who propose an\nimplementation of release consistency that exploits an un-\nderlying invalidation-based cache protocol to detect viola-\ntions of sequential consistency, and Franklin and Sohi [10],\nwho propose a hardware architecture that optimistically\nparallelizes sequential code at runtime.\nOther researchers who have investigated architectural\nsupport for multi-word synchronization include Knight\n[23], who suggests using cache coherence protocols to add\nparallelism to ‚Äúmostly functional‚Äù LISP programs, and the\nIBM 801 [7], which provides support for database-style\nlocking in hardware. Note that despite superÔ¨Åcial similar-\nities in terminology, the synchronization mechanisms pro-\nvided by transactional memory and by the 801 are intended\nfor entirely different purposes, and use entirely different\ntechniques.\nOur approach to performance issues has been heavily\ninÔ¨Çuenced by recent work on locking in multiprocessors,\nincluding work of Anderson [3], Bershad [4], Graunke and\nThakkar [17], and Mellor-Crummey and Scott [27].\n7\nConclusions\nThe primary goal of transactional memory is to make it\neasier to perform general atomic updates of multiple in-\ndependent memory words, avoiding the problems of locks\n(priorityinversion,convoying,anddeadlock). We sketched\nhow it can be implemented by adding new instructions\nto the processor, adding a small auxiliary, transactional\ncache (without disturbing the regular cache), and making\nstraightforward changes to the cache coherence protocol.\nWe investigated transactional memory for its added func-\ntionality, but our simulations showed that it outperforms\nother techniques for atomic updates. This is primarily be-\ncause transactional memory uses no explicit locks and thus\nperforms fewer shared memory accesses. Since transac-\ntional memory offers both improved functionality and bet-\nter performance, it shouldbe considered in future processor\narchitectures.\nPage 10",
        "semantic_similarity": 0.7296445965766907,
        "keyword_overlap": 0.039045553145336226,
        "combined_score": 0.5224648835472844,
        "paragraph_id": 11,
        "source_title": "Transactional Memory: Architectural Support for Lock-Free Data Structures"
      },
      {
        "text_fragment": "Transactional Memory:\nArchitectural Support for Lock-Free Data Structures\nMaurice Herlihy\nDigital Equipment Corporation\nCambridge Research Laboratory\nCambridge MA 02139\nherlihy@crl.dec.com\nJ. Eliot B. Moss\nDept. of Computer Science\nUniversity of Massachusetts\nAmherst, MA 01003\nmoss@cs.umass.edu\nAbstract\nA shared data structure is lock-free if its operations do not\nrequire mutual exclusion. If one process is interrupted in\nthe middle of an operation, other processes will not be\nprevented from operating on that object. In highly con-\ncurrent systems, lock-free data structures avoid common\nproblems associated with conventional locking techniques,\nincluding priority inversion, convoying, and difÔ¨Åculty of\navoiding deadlock.\nThis paper introduces transactional\nmemory, a new multiprocessor architecture intended to\nmake lock-free synchronization as efÔ¨Åcient (and easy to\nuse) as conventional techniques based on mutual exclu-\nsion. Transactional memory allows programmers to de-\nÔ¨Åne customized read-modify-write operations that apply\nto multiple, independently-chosen words of memory.\nIt\nis implemented by straightforward extensions to any mul-\ntiprocessor cache-coherence protocol. Simulation results\nshow that transactional memory matches or outperforms\nthe best known locking techniques for simple benchmarks,\neven in the absence of priority inversion, convoying, and\ndeadlock.\n1\nIntroduction\nA shared data structure is lock-free if its operations do not\nrequire mutual exclusion. If one process is interrupted in\nthe middle of an operation, other processes will not be\nprevented from operating on that object. Lock-free data\nstructures avoid common problems associated with con-\nventional locking techniques in highly concurrent systems:\n\u0000Priority inversion occurs when a lower-priority pro-\ncess is preempted while holding a lock needed by\nhigher-priority processes.\n\u0000Convoyingoccurs when a process holdinga lock is de-\nscheduled, perhaps by exhausting its schedulingquan-\ntum, by a page fault, or bysome other kindof interrupt.\nWhen such an interruptionoccurs, other processes ca-\npable of running may be unable to progress.\n\u0000Deadlock can occur if processes attempt to lock the\nsame set of objects in different orders.\nDeadlock\navoidance can be awkwardif processes mustlock mul-\ntiple data objects, particularly if the set of objects is\nnot known in advance.\nA number of researchers have investigated techniques for\nimplementing lock-free concurrent data structures using\nsoftware techniques [2, 4, 19, 25, 26, 32]. Experimental\nevidence suggests that in the absence of inversion, con-\nvoying, or deadlock, software implementations of lock-\nfree data structures often do not perform as well as their\nlocking-based counterparts.\nThispaper introducestransactionalmemory, a new mul-\ntiprocessor architecture intended to make lock-free syn-\nchronization as efÔ¨Åcient (and easy to use) as conventional\ntechniques based on mutual exclusion. Transactional mem-\noryallows programmers todeÔ¨Åne customized read-modify-\nwrite operations that apply to multiple, independently-\nchosen words of memory. It is implemented by straightfor-\nward extensions to multiprocessor cache-coherence proto-\ncols. Simulation results show that transactional memory is\ncompetitive with the best known lock-based techniques for\nsimple benchmarks, even in the absence of priority inver-\nsion, convoys, and deadlock.\nIn Section 2, we describe transactional memory and how\nto use it. In Section 3 we describe one way to implement\ntransactional memory, and in Section 4 we discuss some\nPage 1",
        "semantic_similarity": 0.7033697664737701,
        "keyword_overlap": 0.04271222637479979,
        "combined_score": 0.505172504444079,
        "paragraph_id": 0,
        "source_title": "Transactional Memory: Architectural Support for Lock-Free Data Structures"
      },
      {
        "text_fragment": "Figure 6: Doubly-Linked List Benchmark: Bus and Network\nReferences\n[1] A. Agarwal et al. The MIT Alewife machine: A\nlarge-scale distributed-memory multiprocessor.\nTechnical Report TM-454, MIT Lab for Computer\nScience, 545 Technology Square, Cambridge MA\n02139, March 1991. Extended version submitted for\npublication.\n[2] J. Allemany and E.W. Felton. Performance issues in\nnon-blocking synchronization on shared-memory\nmultiprocessors. In Proceedings of the 11th Annual\nACM Symposium on Principles of Distributed\nComputing, pages 125‚Äì134. ACM, August 1992.\n[3] T.E. Anderson. The performance of spin lock\nalternatives for shared-memory multiprocessors.\nIEEE Transactions on Parallel and Distributed\nSystems, 1(1):6‚Äì16, January 1990.\n[4] B.N. Bershad. Practical considerations for lock-free\nconcurrent objects. Technical Report\nCMU-CS-91-183, Carnegie Mellon University,\nPittsburgh, PA, September 1991.\n[5] E. A. Brewer, C. N. Dellarocas, A. Colbrook, and\nW. E. Weihl. PROTEUS: A high-performance parallel\narchitecture simulator. Technical Report\nMIT/LCS/TR-516, Massachusetts Institute of\nTechnology, September 1991.\n[6] D. Chaiken, J. Kubiatowicz, and A. Agarwal.\nLimitLESS directories: a scalable cache coherence\nscheme. In Proceedings of the 4th International\nConference on Architectural Support for\nProgramming Langauges and Operating Systems,\npages 224‚Äì234. ACM, April 1991.\n[7] A. Chang and M.F. Mergen. 801 storage:\nArchitecture and programming. ACM Transactions\non Computer Systems, 6(1):28‚Äì50, February 1988.\n[8] Michel Dubois and Christoph Scheurich. Memory\naccess dependencies in shared-memory\nmultiprocessors. IEEE Transactions on Software\nEngineering, 16(6):660‚Äì673, June 1990.\n[9] Michel Dubois, Christoph Scheurich, and Fay¬¥e\nBriggs. Memory access buffering in\nmultiprocessors. In Proceedings of the 13th Annual\nInternational Symposium on Computer Architecture,\npages 434‚Äì442, June 1986.\n[10] M. Franklin and G. S. Sohi. The expandable split\nwindow paradigm for exploiting Ô¨Åne-grain\nparallelism. In Proceedings of the 19th Annual\nInternational Symposium on Computer Architecture,\npages 58‚Äì67. IEEE, May 1992.\n[11] K. Gharachorloo and P. Gibbons. Detecting\nviolations of sequential consistency. In Proceedings\nPage 11",
        "semantic_similarity": 0.6908668279647827,
        "keyword_overlap": 0.04576659038901602,
        "combined_score": 0.4973367566920527,
        "paragraph_id": 13,
        "source_title": "Transactional Memory: Architectural Support for Lock-Free Data Structures"
      }
    ],
    "resolved_title": "Transactional Memory: Architectural Support for Lock-Free Data Structures"
  },
  {
    "citation_marker": "[7]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.05, overlap: 0.00): **NO**  \nThe statement cites reference [7] for \"Benjamin Hindman, Andy Konwinski, Matei Zaharia, Ali Ghodsi, Anthony D.\" as authors. However, the cited paragraph uses [7] solely to reference **\"libprocess\"**‚Äîa C++ library providing an actor-based programming model‚Äîwith **no mention of these authors**. The authors listed are likely the paper's own authors (not citation [7]), making the statement a misrepresentation of the source.  \n\n**Reason:** Citation [7] in the paragraph explicitly refers to a technical tool (libprocess), not authorship attribution.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.03, overlap: 0.00): **NO**  \nThe cited paragraph does not contain the names \"Benjamin Hindman, Andy Konwinski, Matei Zaharia, Ali Ghodsi, Anthony D.\" or any author list. It exclusively discusses technical content (e.g., Spark's implementation, Mesos's API benefits, and evaluation metrics). The statement appears to be a citation reference, but the provided paragraph offers no support for these names, suggesting the names are sourced from elsewhere (e.g., the paper's title page), not this specific section.  \n\n**Reason:** The paragraph is purely technical and lacks any authorship attribution.",
      "DeepSeek LLM confirms support for this match: **Answer:** YES  \n**Reason:** The statement \"[7] Benjamin Hindman, Andy Konwinski, Matei Zaharia, Ali Ghodsi, Anthony D.\" is a standard citation attribution for the source document (\"Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center\"). It accurately lists the authors of the paper and correctly references the citation marker \"[7]\". The cited paragraph is excerpted from this paper and does not contradict or distort the authorship attribution. The statement merely credits the authors and does not make claims about technical content, thus it is accurately supported."
    ],
    "evidence": [
      {
        "text_fragment": "isting frameworks are online algorithms, because frame-\nworks cannot predict task times and must be able to han-\ndle failures and stragglers [18, 40, 38]. These policies\nare easy to implement over resource offers.\n5\nImplementation\nWe have implemented Mesos in about 10,000 lines of\nC++. The system runs on Linux, Solaris and OS X, and\nsupports frameworks written in C++, Java, and Python.\nTo reduce the complexity of our implementation, we\nuse a C++ library called libprocess [7] that provides\nan actor-based programming model using efÔ¨Åcient asyn-\nchronous I/O mechanisms (epoll, kqueue, etc). We\nalso use ZooKeeper [4] to perform leader election.\nMesos can use Linux containers [9] or Solaris projects\n[13] to isolate tasks. We currently isolate CPU cores and\nmemory. We plan to leverage recently added support for\nnetwork and I/O isolation in Linux [8] in the future.\nWe have implemented four frameworks on top of\nMesos. First, we have ported three existing cluster com-\nputing systems: Hadoop [2], the Torque resource sched-\nuler [33], and the MPICH2 implementation of MPI [16].\nNone of these ports required changing these frameworks‚Äô\nAPIs, so all of them can run unmodiÔ¨Åed user programs.\nIn addition, we built a specialized framework for iterative\njobs called Spark, which we discuss in Section 5.3.\n5.1\nHadoop Port\nPorting Hadoop to run on Mesos required relatively few\nmodiÔ¨Åcations, because Hadoop‚Äôs Ô¨Åne-grained map and\nreduce tasks map cleanly to Mesos tasks. In addition, the\nHadoop master, known as the JobTracker, and Hadoop\nslaves, known as TaskTrackers, Ô¨Åt naturally into the\nMesos model as a framework scheduler and executor.\nTo add support for running Hadoop on Mesos, we took\nadvantage of the fact that Hadoop already has a plug-\ngable API for writing job schedulers. We wrote a Hadoop\nscheduler that connects to Mesos, launches TaskTrackers\nas its executors, and maps each Hadoop task to a Mesos\ntask. When there are unlaunched tasks in Hadoop, our\nscheduler Ô¨Årst starts Mesos tasks on the nodes of the\ncluster that it wants to use, and then sends the Hadoop\ntasks to them using Hadoop‚Äôs existing internal interfaces.\nWhen tasks Ô¨Ånish, our executor notiÔ¨Åes Mesos by listen-\ning for task Ô¨Ånish events using an API in the TaskTracker.\nWe used delay scheduling [38] to achieve data locality\nby waiting for slots on the nodes that contain task in-\nput data. In addition, our approach allowed us to reuse\nHadoop‚Äôs existing logic for re-scheduling of failed tasks\nand for speculative execution (straggler mitigation).\nWe also needed to change how map output data is\nserved to reduce tasks.\nHadoop normally writes map\noutput Ô¨Åles to the local Ô¨Ålesystem, then serves these to\nreduce tasks using an HTTP server included in the Task-\nTracker. However, the TaskTracker within Mesos runs\nas an executor, which may be terminated if it is not run-\nning tasks. This would make map output Ô¨Åles unavailable\nto reduce tasks. We solved this problem by providing a\nshared Ô¨Åle server on each node in the cluster to serve\nlocal Ô¨Åles. Such a service is useful beyond Hadoop, to\nother frameworks that write data locally on each node.\nIn total, our Hadoop port is 1500 lines of code.\n5.2\nTorque and MPI Ports\nWe have ported the Torque cluster resource manager to\nrun as a framework on Mesos. The framework consists\nof a Mesos scheduler and executor, written in 360 lines\nof Python code, that launch and manage different com-\nponents of Torque. In addition, we modiÔ¨Åed 3 lines of\nTorque source code to allow it to elastically scale up and\ndown on Mesos depending on the jobs in its queue.\nAfter registering with the Mesos master, the frame-\nwork scheduler conÔ¨Ågures and launches a Torque server\nand then periodically monitors the server‚Äôs job queue.\nWhile the queue is empty, the scheduler releases all tasks\n(down to an optional minimum, which we set to 0) and\nrefuses all resource offers it receives from Mesos. Once\na job gets added to Torque‚Äôs queue (using the standard\nqsub command), the scheduler begins accepting new\nresource offers. As long as there are jobs in Torque‚Äôs\nqueue, the scheduler accepts offers as necessary to sat-\nisfy the constraints of as many jobs in the queue as pos-\nsible. On each node where offers are accepted, Mesos\nlaunches our executor, which in turn starts a Torque\nbackend daemon and registers it with the Torque server.\nWhen enough Torque backend daemons have registered,\nthe torque server will launch the next job in its queue.\nBecause jobs that run on Torque (e.g. MPI) may not be\nfault tolerant, Torque avoids having its tasks revoked by\nnot accepting resources beyond its guaranteed allocation.\nIn addition to the Torque framework, we also created\na Mesos MPI ‚Äúwrapper‚Äù framework, written in 200 lines\nof Python code, for running MPI jobs directly on Mesos.\n5.3\nSpark Framework\nMesos enables the creation of specialized frameworks\noptimized for workloads for which more general exe-\ncution layers may not be optimal. To test the hypoth-\nesis that simple specialized frameworks provide value,\nwe identiÔ¨Åed one class of jobs that were found to per-\nform poorly on Hadoop by machine learning researchers\nat our lab: iterative jobs, where a dataset is reused across\na number of iterations. We built a specialized framework\ncalled Spark [39] optimized for these workloads.\nOne example of an iterative algorithm used in ma-\nchine learning is logistic regression [22]. This algorithm\nseeks to Ô¨Ånd a line that separates two sets of labeled data\npoints. The algorithm starts with a random line w. Then,\n8",
        "semantic_similarity": 0.04597640037536621,
        "keyword_overlap": 0.0003506311360448808,
        "combined_score": 0.03228866960356981,
        "paragraph_id": 9,
        "source_title": "Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center"
      },
      {
        "text_fragment": ". . . \nw \nf(x,w) \nw \nf(x,w) \nx \nx \na) Dryad \nb) Spark \nw \nf(x,w) \nx \nFigure 4: Data Ô¨Çow of a logistic regression job in Dryad\nvs. Spark. Solid lines show data Ô¨Çow within the framework.\nDashed lines show reads from a distributed Ô¨Åle system. Spark\nreuses in-memory data across iterations to improve efÔ¨Åciency.\non each iteration, it computes the gradient of an objective\nfunction that measures how well the line separates the\npoints, and shifts w along this gradient. This gradient\ncomputation amounts to evaluating a function f(x, w)\nover each data point x and summing the results.\nAn\nimplementation of logistic regression in Hadoop must\nrun each iteration as a separate MapReduce job, because\neach iteration depends on the w computed at the previous\none. This imposes overhead because every iteration must\nre-read the input Ô¨Åle into memory. In Dryad, the whole\njob can be expressed as a data Ô¨Çow DAG as shown in Fig-\nure 4a, but the data must still must be reloaded from disk\nat each iteration. Reusing the data in memory between\niterations in Dryad would require cyclic data Ô¨Çow.\nSpark‚Äôs execution is shown in Figure 4b. Spark uses\nthe long-lived nature of Mesos executors to cache a slice\nof the dataset in memory at each executor, and then run\nmultiple iterations on this cached data. This caching is\nachieved in a fault-tolerant manner: if a node is lost,\nSpark remembers how to recompute its slice of the data.\nBy building Spark on top of Mesos, we were able to\nkeep its implementation small (about 1300 lines of code),\nyet still capable of outperforming Hadoop by 10√ó for\niterative jobs. In particular, using Mesos‚Äôs API saved us\nthe time to write a master daemon, slave daemon, and\ncommunication protocols between them for Spark. The\nmain pieces we had to write were a framework scheduler\n(which uses delay scheduling for locality) and user APIs.\n6\nEvaluation\nWe evaluated Mesos through a series of experiments on\nthe Amazon Elastic Compute Cloud (EC2). We begin\nwith a macrobenchmark that evaluates how the system\nshares resources between four workloads, and go on to\npresent a series of smaller experiments designed to eval-\nuate overhead, decentralized scheduling, our specialized\nframework (Spark), scalability, and failure recovery.\nBin\nJob Type\nMap Tasks\nReduce Tasks\n# Jobs Run\n1\nselection\n1\nNA\n38\n2\ntext search\n2\nNA\n18\n3\naggregation\n10\n2\n14\n4\nselection\n50\nNA\n12\n5\naggregation\n100\n10\n6\n6\nselection\n200\nNA\n6\n7\ntext search\n400\nNA\n4\n8\njoin\n400\n30\n2\nTable 3: Job types for each bin in our Facebook Hadoop mix.\n6.1\nMacrobenchmark\nTo evaluate the primary goal of Mesos, which is enabling\ndiverse frameworks to efÔ¨Åciently share a cluster, we ran a\nmacrobenchmark consisting of a mix of four workloads:\n‚Ä¢ A Hadoop instance running a mix of small and large\njobs based on the workload at Facebook.\n‚Ä¢ A Hadoop instance running a set of large batch jobs.\n‚Ä¢ Spark running a series of machine learning jobs.\n‚Ä¢ Torque running a series of MPI jobs.\nWe compared a scenario where the workloads ran as\nfour frameworks on a 96-node Mesos cluster using fair\nsharing to a scenario where they were each given a static\npartition of the cluster (24 nodes), and measured job re-\nsponse times and resource utilization in both cases. We\nused EC2 nodes with 4 CPU cores and 15 GB of RAM.\nWe begin by describing the four workloads in more\ndetail, and then present our results.\n6.1.1\nMacrobenchmark Workloads\nFacebook Hadoop Mix\nOur Hadoop job mix was\nbased on the distribution of job sizes and inter-arrival\ntimes at Facebook, reported in [38]. The workload con-\nsists of 100 jobs submitted at Ô¨Åxed times over a 25-\nminute period, with a mean inter-arrival time of 14s.\nMost of the jobs are small (1-12 tasks), but there are also\nlarge jobs of up to 400 tasks.4 The jobs themselves were\nfrom the Hive benchmark [6], which contains four types\nof queries: text search, a simple selection, an aggrega-\ntion, and a join that gets translated into multiple MapRe-\nduce steps. We grouped the jobs into eight bins of job\ntype and size (listed in Table 3) so that we could com-\npare performance in each bin. We also set the framework\nscheduler to perform fair sharing between its jobs, as this\npolicy is used at Facebook.\nLarge Hadoop Mix\nTo emulate batch workloads that\nneed to run continuously, such as web crawling, we had\na second instance of Hadoop run a series of IO-intensive\n2400-task text search jobs. A script launched ten of these\njobs, submitting each one after the previous one Ô¨Ånished.\n4We scaled down the largest jobs in [38] to have the workload Ô¨Åt a\nquarter of our cluster size.\n9",
        "semantic_similarity": 0.0288238525390625,
        "keyword_overlap": 0.0004012841091492777,
        "combined_score": 0.02029708201008853,
        "paragraph_id": 10,
        "source_title": "Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center"
      },
      {
        "text_fragment": "Scheduler Actions \nreplyToOffer(offerId, tasks) \nsetNeedsOffers(bool) \nsetFilters(filters) \ngetGuaranteedShare() \nkillTask(taskId) \nTable 1: Mesos API functions for schedulers and executors.\n3.6\nFault Tolerance\nSince all the frameworks depend on the Mesos master, it\nis critical to make the master fault-tolerant. To achieve\nthis, we have designed the master to be soft state, so that\na new master can completely reconstruct its internal state\nfrom information held by the slaves and the framework\nschedulers. In particular, the master‚Äôs only state is the list\nof active slaves, active frameworks, and running tasks.\nThis information is sufÔ¨Åcient to compute how many re-\nsources each framework is using and run the allocation\npolicy. We run multiple masters in a hot-standby conÔ¨Åg-\nuration using ZooKeeper [4] for leader election. When\nthe active master fails, the slaves and schedulers connect\nto the next elected master and repopulate its state.\nAside from handling master failures, Mesos reports\nnode failures and executor crashes to frameworks‚Äô sched-\nulers. Frameworks can then react to these failures using\nthe policies of their choice.\nFinally, to deal with scheduler failures, Mesos allows a\nframework to register multiple schedulers such that when\none fails, another one is notiÔ¨Åed by the Mesos master to\ntake over. Frameworks must use their own mechanisms\nto share state between their schedulers.\n3.7\nAPI Summary\nTable 1 summarizes the Mesos API. The ‚Äúcallback‚Äù\ncolumns list functions that frameworks must implement,\nwhile ‚Äúactions‚Äù are operations that they can invoke.\n4\nMesos Behavior\nIn this section, we study Mesos‚Äôs behavior for different\nworkloads. Our goal is not to develop an exact model of\nthe system, but to provide a coarse understanding of its\nbehavior, in order to characterize the environments that\nMesos‚Äôs distributed scheduling model works well in.\nIn short, we Ô¨Ånd that Mesos performs very well when\nframeworks can scale up and down elastically, tasks\ndurations are homogeneous, and frameworks prefer all\nnodes equally (¬ß4.2). When different frameworks pre-\nfer different nodes, we show that Mesos can emulate a\ncentralized scheduler that performs fair sharing across\nframeworks (¬ß4.3). In addition, we show that Mesos can\nhandle heterogeneous task durations without impacting\nthe performance of frameworks with short tasks (¬ß4.4).\nWe also discuss how frameworks are incentivized to im-\nprove their performance under Mesos, and argue that\nthese incentives also improve overall cluster utilization\n(¬ß4.5). We conclude this section with some limitations\nof Mesos‚Äôs distributed scheduling model (¬ß4.6).\n4.1\nDeÔ¨Ånitions, Metrics and Assumptions\nIn our discussion, we consider three metrics:\n‚Ä¢ Framework ramp-up time:\ntime it takes a new\nframework to achieve its allocation (e.g., fair share);\n‚Ä¢ Job completion time: time it takes a job to complete,\nassuming one job per framework;\n‚Ä¢ System utilization: total cluster utilization.\nWe characterize workloads along two dimensions: elas-\nticity and task duration distribution. An elastic frame-\nwork, such as Hadoop and Dryad, can scale its resources\nup and down, i.e., it can start using nodes as soon as it\nacquires them and release them as soon its task Ô¨Ånish. In\ncontrast, a rigid framework, such as MPI, can start run-\nning its jobs only after it has acquired a Ô¨Åxed quantity of\nresources, and cannot scale up dynamically to take ad-\nvantage of new resources or scale down without a large\nimpact on performance. For task durations, we consider\nboth homogeneous and heterogeneous distributions.\nWe also differentiate between two types of resources:\nmandatory and preferred. A resource is mandatory if a\nframework must acquire it in order to run. For example, a\ngraphical processing unit (GPU) is mandatory if a frame-\nwork cannot run without access to GPU. In contrast, a re-\nsource is preferred if a framework performs ‚Äúbetter‚Äù us-\ning it, but can also run using another equivalent resource.\nFor example, a framework may prefer running on a node\nthat locally stores its data, but may also be able to read\nthe data remotely if it must.\nWe assume the amount of mandatory resources re-\nquested by a framework never exceeds its guaranteed\nshare. This ensures that frameworks will not deadlock\nwaiting for the mandatory resources to become free.2 For\nsimplicity, we also assume that all tasks have the same re-\nsource demands and run on identical slices of machines\ncalled slots, and that each framework runs a single job.\n4.2\nHomogeneous Tasks\nWe consider a cluster with n slots and a framework, f,\nthat is entitled to k slots. For the purpose of this analy-\nsis, we consider two distributions of the task durations:\nconstant (i.e., all tasks have the same length) and expo-\nnential. Let the mean task duration be T, and assume that\n2In workloads where the mandatory resource demands of the ac-\ntive frameworks can exceed the capacity of the cluster, the allocation\nmodule needs to implement admission control.\n5",
        "semantic_similarity": 0.01531982421875,
        "keyword_overlap": 0.00040617384240454913,
        "combined_score": 0.010845729105846365,
        "paragraph_id": 6,
        "source_title": "Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center"
      }
    ],
    "resolved_title": "Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center"
  },
  {
    "citation_marker": "[8]",
    "is_valid": false,
    "issues": [
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.45, overlap: 0.04): **NO**  \n\n**Reason:** The article's statement discusses lock scheduling issues (e.g., spinlocks/mutexes causing performance degradation) and references a cross-directory rename benchmark from [8]. However, the cited paragraph from the [8] source (\"Pages to Accelerate Address Translation\") exclusively focuses on **memory page table optimization** (HugeGPT's impact on latency/throughput using huge pages in virtualized environments). It contains:  \n- Zero mention of locks, spinlocks, mutexes, or scheduling.  \n- No reference to directory operations, rename benchmarks, or thread-group experiments.  \n- Only evaluates workload colocation, page sizes, and KVM overhead.  \n\nThe article misattributes lock-related claims and benchmarks to a source discussing unrelated memory management techniques, constituting a distortion.",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.43, overlap: 0.02): **NO**  \n\n**Reason:**  \nThe cited paragraph ([8] in the references) describes \"Smartmd: A high performance deduplication engine with mixed pages,\" which focuses on **memory deduplication**, not lock scheduling, spinlocks, mutexes, or performance interference from locks. The article's statement attributes claims about lock scheduling deficiencies and associated performance issues to [8], but the cited source discusses unrelated topics (deduplication, page tables, and memory management). This misrepresents [8]'s content.  \n\n**Key Discrepancies:**  \n- The article cites [8] for:  \n  - Lack of lock scheduling causing performance degradation.  \n  - Examples of Linux kernel spinlocks/mutexes.  \n  - Cross-directory rename benchmarks.  \n- The actual [8] (Guo et al., 2017) is about **storage deduplication**, with no mention of locks, scheduling, or kernel synchronization mechanisms.  \n\nThe article distorts the source by incorrectly linking its claims to [8].",
      "DeepSeek LLM suggests potential distortion or lack of support for this RAG match (sem_sim: 0.42, overlap: 0.04): **NO**  \nThe statement **misrepresents** the cited source.  \n\n**Reason:**  \nThe cited paragraph from [8] (\"HugeGPT...\") exclusively discusses **virtual memory optimizations** (e.g., reducing page walk latency via huge pages for nested page tables). It measures performance impacts on workloads like Redis/Memcached, focusing on TLB misses, address translation, and memory access locality.  \n\nThe article‚Äôs statement claims [8] demonstrates \"performance interference/degradation\" due to **lock scheduling issues** (e.g., spinlocks/mutexes for inode caches). However, the cited text contains **no mention of locks, scheduling, concurrency, or file-system operations** (e.g., inodes, directories, or rename benchmarks).  \n\nThus, the article **distorts** [8] by attributing lock-related conclusions to a source discussing unrelated memory-management optimizations.",
      "While potential evidence was found (see 'evidence' for scores), the LLM did not confirm the statement's support for any match."
    ],
    "evidence": [
      {
        "text_fragment": "0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\nSphinx\nSpecjbb\nSpecjbb\nShore\nSphinx\nShore\nNormalized\nTail Latency (95th)\nHugeGPT\nFig. 15: 95th percentile tail latencies of latency sensitive workloads when\nthey are colocated on the same server. Tail latencies are normalized to\nvanilla Linux/KVM.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\nSphinx\nSpecjbb\nSpecjbb\nShore\nSphinx\nShore\nNormalized\nTail Latency (99th)\nHugeGPT\nFig. 16: 99th percentile tail latencies of latency sensitive workloads when\nthey are colocated on the same server. Tail latencies are normalized to\nvanilla Linux/KVM.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\nCanneal\nGUPS\nBTree\nNormalized Throughput\nHugeGPT\nFig. 17: Throughputs of six throughput oriented workloads colocated on\nthe same server. We run two copies of each workload. Throughputs are\nnormalized to vanilla Linux/KVM.\nin comparison to vanilla Linux/KVM. This shows HUGEGPT\ncan reduce average and tail latencies when latency sensitive\nworkloads are colocated on the same server.\nFigure 17 shows HUGEGPT‚Äôs throughput when six work-\nloads are colocated on the same server. We run two copies\nof each workload (Canneal, GUPS, and BTree). Since copies\nof the same workload have similar throughput, we plot the\naverage throughput for the copies of each workload. The VM\nrunning each workload has 12 vCPUs and 40GB memory. The\nworking set size of each workload is kept around 35GB. This\nprevents the total workload working set size from exceeding\nthe server‚Äôs memory capacity. On average, HUGEGPT outper-\nforms vanilla Linux/KVM by 13%. This is consistent with the\ntest results when two workloads are consolidated on the same\nserver, as shown in Figure 13.\nFigure 18 shows HUGEGPT‚Äôs throughput for different page\nsizes. We run XSBench to test HUGEGPT‚Äôs throughput. We\nchoose 4KB, 2MB, and 1GB memory page sizes because\ncurrent x86 CPU only supports those page sizes. As the page\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\n4KB\n2MB\n1GB\nNormalized Throughput\nHugeGPT\nFig. 18: HUGEGPT‚Äôs throughputs with different memory page sizes.\nThroughputs are normalized to vanilla Linux/KVM.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\nSwaptions\nRaytrace\nNormalized Throughput\nHugeGPT\nFig. 19: HUGEGPT‚Äôs overhead. Swaptions and Raytrace are page\nwalk non-intensive workloads. Throughputs are normalized to vanilla\nLinux/KVM.\nsize increases from 4KB to 1GB, HUGEGPT‚Äôs throughput\nimprovement relative to vanilla Linux/KVM degrades from\n15% to 4%. This is because huge pages (e.g., 1GB) can shorten\npage table walk. For instance, the page table for 1GB huge\npages does not need the last two levels that are present in page\ntables for 4KB pages. As a result, HUGEGPT cannot obtain\nmore benefits when the page size becomes very large. On the\nother hand, 1GB huge pages are not widely used as they incur\nlarge overhead such as memory fragmentation and CPU waste\nfor defragmentation [6].\nTo evaluate HUGEGPT‚Äôs overhead, we test the performance\nof HUGEGPT and vanilla Linux/KVM with two page walk\nnon-intensive workloads, i.e., Swaptions and Raytrace. We\nshow the performance results in Figure 19. When the workload\nis page walk non-intensive, there is almost no space for\nHUGEGPT to improve application performance compared to\nvanilla Linux/KVM, and the performance difference between\nHUGEGPT and vanilla Linux/KVM shows HUGEGPT‚Äôs over-\nhead. Figure 19 shows that HUGEGPT does not introduce\nmuch performance overhead (3% on average). HUGEGPT may\nintroduce overhead as it needs to identify guest page table\npages and allocate huge pages in the host OS.\nVII. Discussion\nLive Migration. HUGEGPT can support live migration and\nrestore from a snapshot. It needs the destination host OS to\nconduct system initialization as described in ¬ßV-A.\nMemory Consumption. HUGEGPT consumes negligible ex-\ntra memory space to store page table data compared to vanilla\nLinux/KVM. In our evaluation, for 100GB of application data,\nvanilla Linux/KVM needs around 217MB of memory space to\nstore page table data, and HUGEGPT needs around 221MB.",
        "semantic_similarity": 0.4523983597755432,
        "keyword_overlap": 0.044290657439446365,
        "combined_score": 0.32996604907471416,
        "paragraph_id": 8,
        "source_title": "Pages to Accelerate Address Translation"
      },
      {
        "text_fragment": "References\n[1] A. Margaritov, D. Ustiugov, E. Bugnion, and B. Grot, ‚ÄúPrefetched\naddress translation,‚Äù in Proceedings of the 52nd Annual IEEE/ACM\nInternational Symposium on Microarchitecture, 2019, pp. 1023‚Äì1036.\n[2] M. Ferdman, A. Adileh, O. Kocberber, S. Volos, M. Alisafaee, D. Jevd-\njic, C. Kaynak, A. D. Popescu, A. Ailamaki, and B. Falsafi, ‚ÄúClearing the\nclouds: a study of emerging scale-out workloads on modern hardware,‚Äù\nAcm sigplan notices, vol. 47, no. 4, pp. 37‚Äì48, 2012.\n[3] J. Navarro, S. Iyer, P. Druschel, and A. Cox, ‚ÄúPractical, transparent\noperating system support for superpages,‚Äù ACM SIGOPS Operating\nSystems Review, vol. 36, no. SI, pp. 89‚Äì104, 2002.\n[4] R. Achermann, A. Panwar, A. Bhattacharjee, T. Roscoe, and J. Gandhi,\n‚ÄúMitosis: Transparently self-replicating page-tables for large-memory\nmachines,‚Äù in Proceedings of the Twenty-Fifth International Conference\non Architectural Support for Programming Languages and Operating\nSystems, 2020, pp. 283‚Äì300.\n[5] A. Panwar, R. Achermann, A. Basu, A. Bhattacharjee, K. Gopinath,\nand J. Gandhi, ‚ÄúFast local page-tables for virtualized numa servers with\nvmitosis,‚Äù in Proceedings of the Twenty-Sixth International Conference\non Architectural Support for Programming Languages and Operating\nSystems, 2021.\n[6] A. Panwar, S. Bansal, and K. Gopinath, ‚ÄúHawkeye: Efficient fine-\ngrained os support for huge pages,‚Äù in Proceedings of the Twenty-Fourth\nInternational Conference on Architectural Support for Programming\nLanguages and Operating Systems, 2019, pp. 347‚Äì360.\n[7] F. Guo, S. Kim, Y. Baskakov, and I. Banerjee, ‚ÄúProactively break-\ning large pages to improve memory overcommitment performance in\nvmware esxi,‚Äù in Proceedings of the 11th ACM SIGPLAN/SIGOPS\nInternational Conference on Virtual Execution Environments, 2015, pp.\n39‚Äì51.\n[8] F. Guo, Y. Li, Y. Xu, S. Jiang, and J. C. Lui, ‚ÄúSmartmd: A high per-\nformance deduplication engine with mixed pages,‚Äù in 2017 {USENIX}\nAnnual Technical Conference ({USENIX}{ATC} 17), 2017, pp. 733‚Äì\n744.\n[9] A. Margaritov, D. Ustiugov, A. Shahab, and B. Grot, ‚ÄúPtemagnet: Fine-\ngrained physical memory reservation for faster page walks in public\nclouds,‚Äù in The 26th International Conference on Architectural Support\nfor Programming Languages and Operating Systems, ASPLOS 2021,\n2021.\n[10] T. Merrifield and H. R. Taheri, ‚ÄúPerformance implications of extended\npage tables on virtualized x86 processors,‚Äù in Proceedings of the12th\nACM SIGPLAN/SIGOPS International Conference on Virtual Execution\nEnvironments, 2016, pp. 25‚Äì35.\n[11] B. Pham, J. Vesel`y, G. H. Loh, and A. Bhattacharjee, ‚ÄúLarge pages and\nlightweight memory management in virtualized environments: Can you\nhave it both ways?‚Äù in Proceedings of the 48th International Symposium\non Microarchitecture, 2015, pp. 1‚Äì12.\n[12] B. Pham, J. Vesely, G. H. Loh, and A. Bhattacharjee, ‚ÄúUsing tlb\nspeculation to overcome page splintering in virtual machines,‚Äù 2015.\n[13] A. Panwar, A. Prasad, and K. Gopinath, ‚ÄúMaking huge pages actually\nuseful,‚Äù in Proceedings of the Twenty-Third International Conference\non Architectural Support for Programming Languages and Operating\nSystems, 2018, pp. 679‚Äì692.\n[14] W. Zhu, A. L. Cox, and S. Rixner, ‚ÄúA comprehensive analysis of\nsuperpage management mechanisms and policies,‚Äù in 2020 {USENIX}\nAnnual Technical Conference ({USENIX}{ATC} 20), 2020, pp. 829‚Äì\n842.\n[15] R. Kadekodi, S. Kadekodi, S. Ponnapalli, H. Shirwadkar, G. R. Ganger,\nA. Kolli, and V. Chidambaram, ‚ÄúWinefs: a hugepage-aware file system\nfor persistent memory that ages gracefully,‚Äù in Proceedings of the ACM\nSIGOPS 28th Symposium on Operating Systems Principles CD-ROM,\n2021, pp. 804‚Äì818.\n[16] A. Hunter, C. Kennelly, P. Turner, D. Gove, T. Moseley, and P. Ran-\nganathan, ‚ÄúBeyond malloc efficiency to fleet efficiency: a hugepage-\naware memory allocator,‚Äù in 15th {USENIX} Symposium on Operating\nSystems Design and Implementation ({OSDI} 21), 2021, pp. 257‚Äì273.\n[17] M. Maas, C. Kennelly, K. Nguyen, D. Gove, K. S. McKinley, and\nP. Turner, ‚ÄúAdaptive huge-page subrelease for non-moving memory\nallocators in warehouse-scale computers,‚Äù in Proceedings of the 2021\nACM SIGPLAN International Symposium on Memory Management,\n2021, pp. 28‚Äì38.\n[18] J. Gandhi, M. D. Hill, and M. M. Swift, ‚ÄúAgile paging: Exceeding the\nbest of nested and shadow paging,‚Äù in 2016 ACM/IEEE 43rd Annual\nInternational Symposium on Computer Architecture (ISCA).\nIEEE,\n2016, pp. 707‚Äì718.\n[19] J. Stojkovic, D. Skarlatos, A. Kokolis, T. Xu, and J. Torrellas, ‚ÄúParallel\nvirtualized memory translation with nested elastic cuckoo page tables,‚Äù\nin Proceedings of the 27th ACM International Conference on Archi-\ntectural Support for Programming Languages and Operating Systems,\n2022, pp. 84‚Äì97.\n[20] ‚ÄúIntel\n64\nand\nia-32\narchitectures\ndeveloper‚Äôs\nmanual,‚Äù\nhttps://www.intel.com/content/www/us/en/architecture-and-technology/\n64-ia-32-architectures-software-developer-manual-325462.html.\n[21] ‚ÄúAmd64 architecture programmer‚Äôs manual,‚Äù https://developer.amd.com/\nresources/developer-guides-manuals/.\n[22] ‚ÄúIntel five level paging,‚Äù https://en.wikipedia.org/wiki/Intel 5-level\npaging.\n[23] ‚ÄúFive\nlevel\npaging\nand\nfive\nlevel\nEPT\nwhite\npaper,‚Äù\nhttps://software.intel.com/content/www/us/en/develop/download/\n5-level-paging-and-5-level-ept-white-paper.html.\n[24] J. H. Ryoo, N. Gulur, S. Song, and L. K. John, ‚ÄúRethinking tlb designs\nin virtualized environments: A very large part-of-memory tlb,‚Äù ACM\nSIGARCH Computer Architecture News, vol. 45, no. 2, pp. 469‚Äì480,\n2017.\n[25] V. Karakostas, J. Gandhi, F. Ayar, A. Cristal, M. D. Hill, K. S. McKinley,\nM. Nemirovsky, M. M. Swift, and O. ¬®Unsal, ‚ÄúRedundant memory\nmappings for fast access to large memories,‚Äù ACM SIGARCH Computer\nArchitecture News, vol. 43, no. 3S, pp. 66‚Äì78, 2015.\n[26] S. Gupta, A. Bhattacharyya, Y. Oh, A. Bhattacharjee, B. Falsafi,\nand M. Payer, ‚ÄúRebooting virtual memory with midgard,‚Äù in 2021\nACM/IEEE 48th Annual International Symposium on Computer Archi-\ntecture (ISCA).\nIEEE, 2021, pp. 512‚Äì525.\n[27] B. Pham, V. Vaidyanathan, A. Jaleel, and A. Bhattacharjee, ‚ÄúColt: Coa-\nlesced large-reach tlbs,‚Äù in 2012 45th Annual IEEE/ACM International\nSymposium on Microarchitecture.\nIEEE, 2012, pp. 258‚Äì269.\n[28] B. Pham, A. Bhattacharjee, Y. Eckert, and G. H. Loh, ‚ÄúIncreasing tlb\nreach by exploiting clustering in page translations,‚Äù in 2014 IEEE 20th\nInternational Symposium on High Performance Computer Architecture\n(HPCA).\nIEEE, 2014, pp. 558‚Äì567.\n[29] C. H. Park, T. Heo, J. Jeong, and J. Huh, ‚ÄúHybrid tlb coalescing:\nImproving tlb translation coverage under diverse fragmented memory\nallocations,‚Äù in Proceedings of the 44th Annual International Symposium\non Computer Architecture, 2017, pp. 444‚Äì456.\n[30] T. W. Barr, A. L. Cox, and S. Rixner, ‚ÄúTranslation caching: skip, don‚Äôt\nwalk (the page table),‚Äù ACM SIGARCH Computer Architecture News,\nvol. 38, no. 3, pp. 48‚Äì59, 2010.\n[31] I. Yaniv and D. Tsafrir, ‚ÄúHash, don‚Äôt cache (the page table),‚Äù ACM\nSIGMETRICS Performance Evaluation Review, vol. 44, no. 1, pp. 337‚Äì\n350, 2016.\n[32] Y. Kwon, H. Yu, S. Peter, C. J. Rossbach, and E. Witchel, ‚ÄúCoordinated\nand efficient huge page management with ingens,‚Äù in 12th {USENIX}\nSymposium on Operating Systems Design and Implementation ({OSDI}\n16), 2016, pp. 705‚Äì721.\n[33] I. Subramanian, C. Mather, K. Peterson, and B. Raghunath, ‚ÄúImple-\nmentation of multiple pagesize support in hp-ux.‚Äù in USENIX Annual\nTechnical Conference, 1998, pp. 105‚Äì119.\n[34] N. Ganapathy and C. Schimmel, ‚ÄúGeneral purpose operating system sup-\nport for multiple page sizes.‚Äù in USENIX Annual Technical Conference,\nno. 98, 1998, pp. 91‚Äì104.\n[35] M. Talluri, S. Kong, M. D. Hill, and D. A. Patterson, ‚ÄúTradeoffs\nin supporting two page sizes,‚Äù in Proceedings of the 19th annual\ninternational symposium on Computer architecture, 1992, pp. 415‚Äì424.\n[36] M. Dashti, A. Fedorova, J. Funston, F. Gaud, R. Lachaize, B. Lepers,\nV. Quema, and M. Roth, ‚ÄúTraffic management: a holistic approach to\nmemory placement on numa systems,‚Äù ACM SIGPLAN Notices, vol. 48,\nno. 4, pp. 381‚Äì394, 2013.\n[37] F. Gaud, B. Lepers, J. Decouchant, J. Funston, A. Fedorova, and\nV. Qu¬¥ema, ‚ÄúLarge pages may be harmful on {NUMA} systems,‚Äù in\n2014 {USENIX} Annual Technical Conference ({USENIX}{ATC} 14),\n2014, pp. 231‚Äì242.\n[38] C. H. Park, I. Vougioukas, A. Sandberg, and D. Black-Schaffer, ‚ÄúEvery\nwalk‚Äôs a hit: making page walks single-access cache hits,‚Äù in Proceed-\nings of the 27th ACM International Conference on Architectural Support\nfor Programming Languages and Operating Systems, 2022, pp. 128‚Äì141.\n[39] F. Gaud, B. Lepers, J. Decouchant, J. Funston, A. Fedorova, and\nV.\nQuema,\n‚ÄúLarge\npages\nmay\nbe\nharmful\non\nnuma\nsystems,‚Äù\nin\n2014\nUSENIX\nAnnual\nTechnical\nConference\n(USENIX\nATC",
        "semantic_similarity": 0.43028974533081055,
        "keyword_overlap": 0.024213075060532687,
        "combined_score": 0.3084667442497272,
        "paragraph_id": 10,
        "source_title": "Pages to Accelerate Address Translation"
      },
      {
        "text_fragment": "0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\nRedis\nMemcached\nSpecjbb\nMasstree\nMoses\nSphinx\nShore\nNormalized\nAverage Latency\nNative\nHugeGPT\nFig. 7: Average latencies of latency sensitive workloads. Average latencies\nare normalized to vanilla Linux/KVM.\nsystem on average. This confirms HUGEGPT‚Äôs effectiveness\non improving application throughput by reducing the overhead\nof two dimensional page walks in vanilla Linux/KVM.\nFigure 5 also shows that HUGEGPT increases the through-\nput by the largest percentage (16%) for the Memcached\nworkload and the smallest percentage (5%) for the BTree and\nGUPS workloads. For the Memcached workload, it strides the\nmemory with weak memory access locality so more page table\nentries may be cached by TLB and page walk caches compared\nto random memory accesses. Therefore, reducing the leaf page\ntable entries of the nested page table in HUGEGPT shows\nmore performance improvement. This is consistent with the\nperformance observation in ¬ßII-B. Since GUPS and BTree\nworkloads conduct randomly memory accesses, HUGEGPT‚Äôs\nperformance improvement on these workloads is less. For\ninstance, GUPS is calculated by identifying the number of\nmemory locations that can be randomly updated in one second,\nso it shows almost no memory access locality such that it may\nbe hard to cache lower level page table entries.\nC. Experiments with Latency Sensitive Workloads\nFigure 7 shows the average latencies of different systems\nwhen they are tested with latency sensitive workloads. On\naverage, native system shows the lowest average latency as\nmost page table entries can be cached while walking the one\nlevel page table. In the worst case, native system only incurs\nfour memory references. Relative to native system, HUGEGPT\nincreases the average latency by 16% on average. Compared to\nvanilla Linux/KVM, HUGEGPT reduces the average latency\nby 8% on average. This is because HUGEGPT reduces the\naverage page walk latency of the two dimensional page walks\nby up to about 50% as explained in ¬ßII-B. HUGEGPT reduces\npage walk cache misses and the number of memory references\nin two dimensional page walks from 24 to 20 in the worst case.\nTo further pinpoint why HUGEGPT increases the average\nlatency compared to native system and reduces the average la-\ntency compared to vanilla Linux/KVM, we profile the average\npage walk latency when the latency sensitive workloads are\ntested with the three systems. We show the profiling results in\nFigure 10. On average, HUGEGPT increases the average page\nwalk latency by 62% compared to native system and decreases\nthe average page walk latency by 8% compared to vanilla\nLinux/KVM. This is consistent with the average latency results\nand also shows HUGEGPT‚Äôs effectiveness on reducing the\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\nRedis\nMemcached\nSpecjbb\nMasstree\nMoses\nSphinx\nShore\nNormalized\nTail Latency (95th)\nNative\nHugeGPT\nFig. 8: 95th percentile tail latencies of latency sensitive workloads. Tail\nlatencies are normalized to vanilla Linux/KVM.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\nRedis\nMemcached\nSpecjbb\nMasstree\nMoses\nSphinx\nShore\nNormalized\nTail Latency (99th)\nNative\nHugeGPT\nFig. 9: 99th percentile tail latencies of latency sensitive workloads. Tail\nlatencies are normalized to vanilla Linux/KVM.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1.2\n 1.4\nRedis\nMemcached\nSpecjbb\nMasstree\nMoses\nSphinx\nShore\nNormalized Average\nPage Walk Latency\nNative\nHugeGPT\nFig. 10: Average page walk latencies of latency sensitive workloads.\nAverage page walk latencies are normalized to vanilla Linux/KVM.\noverhead of two dimensional page walks for latency sensitive\nworkloads in comparison to vanilla Linux/KVM.\nFigure 8 and Figure 9 show the 95th percentile tail latencies\nand the 99th percentile tail latencies, respectively, when the\nlatency sensitive workloads are tested with the three systems.\nOn average, HUGEGPT provides 8% lower 95th percentile\ntail latency 8% lower 99th percentile tail latency compared\nto vanilla Linux/KVM, and 32% higher 95th percentile tail\nlatency and 30% higher 99th percentile tail latency relative to\nnative system. The tail latency test results are consistent with\nthe average page walk latency of the three systems as shown\nin Figure 10.\nFigure 7, Figure 8, and Figure 9 also show that HUGEGPT\nshows small performance advantage for some workloads (e.g.,\nMoses and Masstree) and large performance advantage for\nsome other workloads (e.g., Specjbb and Sphinx). This is\nbecause Specjbb and Sphinx show weak memory access\nlocality. HUGEGPT performs better on these workloads as\nexplained in ¬ßII-B. Memory access patterns in Moses and\nMasstree workloads are more random than Specjbb and\nSphinx. HUGEGPT does not show good performance with",
        "semantic_similarity": 0.41610074043273926,
        "keyword_overlap": 0.035725576983876066,
        "combined_score": 0.30198819139808025,
        "paragraph_id": 6,
        "source_title": "Pages to Accelerate Address Translation"
      }
    ],
    "resolved_title": "Pages to Accelerate Address Translation"
  },
  {
    "citation_marker": "[9]",
    "is_valid": true,
    "issues": [
      "DeepSeek LLM confirms support for this match: **Answer:** YES  \n**Reason:** The cited paragraph explicitly states that Shuffling-based locks (including Shfllock) achieve \"NUMA-awareness\" by reordering the waiting thread queue according to policies like prioritizing threads on the same socket. This directly supports the article's claim that Shfllock \"trie[s] to hand over the lock to the threads on the same socket.\" The mechanism (queue reordering) and goal (NUMA affinity) align accurately."
    ],
    "evidence": [
      {
        "text_fragment": "Scalable and Practical Locking with Shuffling\nSanidhya Kashyap Irina Calciu‚àó\nXiaohe Cheng‚Ä°\nChangwoo Min‚Ä†\nTaesoo Kim\nGeorgia Institute of Technology\n‚àóVMware Research\n‚Ä°HKUST\n‚Ä†Virginia Tech\nAbstract\nLocks are an essential building block for high-performance\nmulticore system software. To meet performance goals, lock\nalgorithms have evolved towards specialized solutions for ar-\nchitectural characteristics (e.g., NUMA). However, in practice,\napplications run on different server platforms and exhibit\nwidely diverse behaviors that evolve with time (e.g., num-\nber of threads, number of locks). This creates performance\nand scalability problems for locks optimized for a single sce-\nnario and platform. For example, popular spinlocks suffer\nfrom excessive cache-line bouncing in NUMA systems, while\nscalable, NUMA-aware locks exhibit sub-par single-thread\nperformance.\nIn this paper, we identify four dominating factors that im-\npact the performance of lock algorithms. We then propose a\nnew technique, shuffling, that can dynamically accommodate\nall these factors, without slowing down the critical path of\nthe lock. The key idea of shuffling is to re-order the queue\nof threads waiting to acquire the lock in accordance with\nsome pre-established policy. For best performance, this work\nis done off the critical path, by the waiter threads. Using\nshuffling, we demonstrate how to achieve NUMA-awareness\nand implement an efficient parking/wake-up strategy, with-\nout any auxiliary data structure, mostly off the critical path.\nThe evaluation shows that our family of locks based on shuf-\nfling improves the throughput of real-world applications\nup to 12.5√ó, with impressive memory footprint reduction\ncompared with the recent lock algorithms.\nCCS Concepts\n‚Ä¢ Software and its engineering ‚ÜíMu-\ntual exclusion.\nKeywords\nmutual exclusion, memory footprint, Linux.\nACM Reference Format:\nSanidhya Kashyap Irina Calciu Xiaohe Cheng Changwoo Min\nTaesoo Kim. 2019. Scalable and Practical Locking with Shuffling. In\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies\nare not made or distributed for profit or commercial advantage and that\ncopies bear this notice and the full citation on the first page. Copyrights\nfor components of this work owned by others than the author(s) must\nbe honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee. Request permissions from permissions@acm.org.\nSOSP‚Äô19, October 27‚Äì30, 2019, Huntsville, ON, Canada\n¬© 2019 Copyright held by the owner/author(s). Publication rights licensed\nto ACM.\nACM ISBN 978-1-4503-6873-5/19/10...$15.00\nhttps://doi.org/10.1145/3341301.3359629\n0k\n100k\n200k\n300k\n400k\n500k\n24 48 72 96 120144168192\n1\n2\n4\n8\n16\n32\n64\n128\n256\n512\n1024\n0\n24 48 72 96 120144168192\nOps/sec\n#threads\n(a) File creation in a shared directory\nStock\nCST\nCohort\nShflLock\nMemory (MB)\n#threads\n(b) Memory used by lock instances\nFigure 1. Impact of locks on a file-system micro-benchmark that\nspawns threads to create new files in a shared directory (MWCM [39]).\nA process stresses the writer side of the readers-writer lock. We eval-\nuate the Linux baseline version (Stock), CST [27], Cohort lock [18],\nand our proposed ShflLock. (a) File creation throughput on an\n8-socket 192-core machine. (b) Total memory consumed by locks\nthat are part of the inode structure.\nACM SIGOPS 27th Symposium on Operating Systems and Principles\n(SOSP‚Äô19). ACM, New York, NY, USA. 14 pages. https://doi.org/10.\n1145/3341301.3359629\n1\nIntroduction\nThe introduction of multicore machines marked the end of\nthe ‚Äúfree lunch‚Äù[47], making concurrent programming, es-\npecially lock-based mutual exclusion, a critical approach to\nimprove the performance of applications. Lock algorithms\ndetermine the scalability of applications in multicore ma-\nchines [3, 5, 21].\nSince the invention of concurrent programming, lock de-\nsign has been influenced by hardware evolution. For instance,\nMCS [37] was proposed to address excessive cache-line traffic\nresulting from an increasing number of threads trying to ac-\nquire the lock at the same time, while Cohort locks [18] were\nproposed in response to the emergence of the non-uniform\nmemory access (NUMA) architecture. NUMA machines con-\nsist of multiple nodes (or sockets), each with multiple cores,\nlocally attached memories, and fast caches. In such machines,\nthe access from a socket to its local memory is faster than\nremote access to memory on a different socket [44] and each\nsocket has a shared last-level-cache. Cohort locks exploit\nthis characteristic to improve application throughput.\nUnfortunately, the influence of hardware evolution on\nlock design has resulted in a tight coupling between hard-\nware characteristics and lock algorithms. Meanwhile, other\nfactors have been neglected, such as memory footprint [10],\nlow thread counts, and core over-subscription. For exam-\nple, Cohort locks can achieve high throughput at high core\ncounts, but also require memory proportional to the num-\nber of sockets. The extra memory is unacceptable for some\napplications, such as databases and OSes, which can have\n586",
        "semantic_similarity": 0.7842199206352234,
        "keyword_overlap": 0.0580991474581623,
        "combined_score": 0.566383688682105,
        "paragraph_id": 0,
        "source_title": "Scalable and Practical Locking with Shuffling"
      }
    ],
    "resolved_title": "Scalable and Practical Locking with Shuffling"
  }
]