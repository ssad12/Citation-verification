import os
import json
import argparse
import re
from paper_citation_verifier import PaperCitationVerifier, call_llm_api # Import call_llm_api
from citation_utils import extract_citations_from_text
import fitz 


def read_pdf_text_fitz(pdf_path):
    try:
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text() + "\n"
        doc.close()
        return text
    except Exception as e:
        print(f"PDFè¯»å–å¤±è´¥: {e}")
        return ""

def read_text_file(file_path):
    try:
        with open(file_path, encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        print(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        return ""

def verify_article(verifier, article_path, output_path, sim_thresh=0.6, keyword_overlap_thresh=0.3):
    if article_path.lower().endswith(".pdf"):
        text = read_pdf_text_fitz(article_path)
        # For PDF, we also need to extract its own references section
        _, _, _, article_references = verifier.extract_title_and_abstract_authors_references(text, article_path)
        print(f"Extracted {len(article_references)} references from article bibliography.")
    else:
        text = read_text_file(article_path)
        article_references = [] # No bibliography for text files, or need to parse it if available

    if not text:
        print(f"æ— æ³•è¯»å–æ–‡ç« å†…å®¹: {article_path}")
        return

    citations = extract_citations_from_text(text)
    print(f"æ£€æµ‹åˆ°æ–‡ä¸­å¼•ç”¨ {len(citations)} æ¡")

    results = []
    for idx, cit in enumerate(citations):
        print(f"éªŒè¯å¼•ç”¨ {idx+1}/{len(citations)}: {cit}")
        context_fragments = []
        for m in re.finditer(re.escape(cit), text):
            # Attempt to get the whole sentence containing the citation for better context
            sentence_start = text.rfind('.', 0, m.start()) + 1
            if sentence_start == 0: # If no dot before, go to start of text
                sentence_start = text.rfind('\n\n', 0, m.start()) + 2 # Or start of paragraph
            
            sentence_end = text.find('.', m.end()) + 1
            if sentence_end == 0: # If no dot after, go to end of text
                sentence_end = text.find('\n\n', m.end()) # Or end of paragraph
                if sentence_end == -1: sentence_end = len(text)


            fragment = text[sentence_start:sentence_end].strip()
            # If the sentence is too short or clearly broken, take a wider window
            if len(fragment) < 30 or '\n' in fragment:
                start_idx = max(0, m.start() - 200) 
                end_idx = min(len(text), m.end() + 200) 
                fragment = text[start_idx:end_idx].strip()
            
            fragment = re.sub(r'\n\s*\n+', '\n\n', fragment).strip() 
            context_fragments.append(fragment)

        if not context_fragments:
            results.append({"citation_marker": cit, "status": "skipped", "reason": "æœªåœ¨æ–‡ä¸­æ‰¾åˆ°å¼•ç”¨æ ‡è®°æˆ–æœ‰æ•ˆä¸Šä¸‹æ–‡"})
            continue
        
        merged_context = "\n\n".join(context_fragments)
        
        # Pass article_references to resolve_citation_marker_to_title
        result = verifier.verify_citation(
            citation_marker=cit, 
            cited_context=merged_context, 
            article_references=article_references, # NEW: pass article's bibliography
            sim_thresh=sim_thresh, 
            keyword_overlap_thresh=keyword_overlap_thresh
        )
        result["citation_marker"] = cit 
        results.append(result)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"å¼•ç”¨éªŒè¯å®Œæˆï¼Œç»“æœå†™å…¥ï¼š{output_path}")

def query_knowledge_base_mode(verifier, query_text, output_path, n_results=5, sim_thresh=0.6):
    print(f"ğŸ” æ­£åœ¨æŸ¥è¯¢çŸ¥è¯†åº“: '{query_text}'")
    results = verifier.query_knowledge_base(query_text, n_results=n_results, sim_thresh=sim_thresh)
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œç»“æœå†™å…¥ï¼š{output_path}")


def add_papers_from_dir(verifier, pdf_dir, recursive=False):
    count_added, count_failed = 0, 0
    if not os.path.exists(pdf_dir):
        print(f"é”™è¯¯: æ–‡ä»¶å¤¹ '{pdf_dir}' ä¸å­˜åœ¨ã€‚")
        return

    for root, _, files in os.walk(pdf_dir):
        for file in files:
            if file.lower().endswith(".pdf"):
                full_path = os.path.join(root, file)
                print(f"--- å°è¯•æ·»åŠ : {full_path} ---")
                if verifier.add_paper(full_path):
                    count_added += 1
                else:
                    count_failed += 1
        if not recursive:
            break 
    print(f"æ·»åŠ å®Œæˆï¼šæˆåŠŸ {count_added} ç¯‡ï¼Œå¤±è´¥ {count_failed} ç¯‡")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ğŸ“˜ æ–‡çŒ®å¼•ç”¨éªŒè¯ä¸çŸ¥è¯†åº“æŸ¥è¯¢ç³»ç»Ÿ")
    parser.add_argument("mode", choices=["add", "verify_citations", "query_kb", "update"], help="é€‰æ‹©è¿è¡Œæ¨¡å¼")
    parser.add_argument("--pdf_dir", help="PDF æ–‡ä»¶å¤¹è·¯å¾„ (ç”¨äº add æ¨¡å¼)")
    parser.add_argument("--article", help="å¾…éªŒè¯çš„æ–‡ç« è·¯å¾„ (PDF æˆ– TXTï¼Œç”¨äº verify_citations æ¨¡å¼)")
    parser.add_argument("--query_text", help="æŸ¥è¯¢çŸ¥è¯†åº“çš„æ–‡æœ¬ (ç”¨äº query_kb æ¨¡å¼)")
    parser.add_argument("--output", default="results.json", help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ (éªŒè¯æˆ–æŸ¥è¯¢ç»“æœ)")
    parser.add_argument("--chroma_dir", default="./chroma_db", help="RAG å­˜å‚¨è·¯å¾„")
    parser.add_argument("--embedding_model", default="./local_models/all-MiniLM-L6-v2", help="æœ¬åœ°åµŒå…¥æ¨¡å‹è·¯å¾„")
    parser.add_argument("--recursive", action="store_true", help="æ˜¯å¦é€’å½’éå† pdf_dir (ç”¨äº add æ¨¡å¼)")
    parser.add_argument("--sim_thresh", type=float, default=0.05, help="è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ (ç”¨äº verify_citations å’Œ query_kb æ¨¡å¼)")
    parser.add_argument("--keyword_overlap_thresh", type=float, default=0.3, help="å…³é”®è¯é‡åˆåº¦é˜ˆå€¼ (ç”¨äº verify_citations æ¨¡å¼)") 
    args = parser.parse_args()

    verifier = PaperCitationVerifier(chroma_dir=args.chroma_dir, embedding_model=args.embedding_model)

    if args.mode == "add":
        if not args.pdf_dir:
            print("â— 'add' æ¨¡å¼éœ€è¦æä¾› --pdf_dir å‚æ•°ã€‚")
        else:
            add_papers_from_dir(verifier, args.pdf_dir, recursive=args.recursive)
            verifier.update_rag_collection()

    elif args.mode == "verify_citations":
        if not args.article:
            print("â— 'verify_citations' æ¨¡å¼éœ€è¦æä¾› --article å‚æ•°ã€‚")
        else:
            verify_article(verifier, args.article, args.output, sim_thresh=args.sim_thresh, keyword_overlap_thresh=args.keyword_overlap_thresh)

    elif args.mode == "query_kb":
        if not args.query_text:
            print("â— 'query_kb' æ¨¡å¼éœ€è¦æä¾› --query_text å‚æ•°ã€‚")
        else:
            query_knowledge_base_mode(verifier, args.query_text, args.output, sim_thresh=args.sim_thresh)

    elif args.mode == "update":
        verifier.update_rag_collection()
        print("ğŸ“š RAG çŸ¥è¯†åº“å·²æ‰‹åŠ¨æ›´æ–°ã€‚")